{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f824fe",
   "metadata": {},
   "source": [
    "###  데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6264d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "# MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'trained')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960cee4",
   "metadata": {},
   "source": [
    "#### json 파싱하기\n",
    "- `train.json`, `validation.json` : 이미지에 담겨 있는 사람들의 pose keypoint(Pose Estimation을 위한 label) 정보들을 가지고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c628c196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e04fa3",
   "metadata": {},
   "source": [
    "- `joints` 가 우리가 label 로 사용할 keypoint 의 label\n",
    "  - 0 - 오른쪽 발목\n",
    "    1 - 오른쪽 무릎\n",
    "    2 - 오른쪽 엉덩이\n",
    "    3 - 왼쪽 엉덩이\n",
    "    4 - 왼쪽 무릎\n",
    "    5 - 왼쪽 발목\n",
    "    6 - 골반\n",
    "    7 - 가슴(흉부)\n",
    "    8 - 목\n",
    "    9 - 머리 위\n",
    "    10 - 오른쪽 손목\n",
    "    11 - 오른쪽 팔꿈치\n",
    "    12 - 오른쪽 어깨\n",
    "    13 - 왼쪽 어깨\n",
    "    14 - 왼쪽 팔꿈치\n",
    "    15 - 왼쪽 손목\n",
    "- `scale`과 `center`는 사람 몸의 크기와 중심점\n",
    "- 이미지 형상과 사람의 포즈에 따라 모든 label 이 이미지에 나타나지 않기 때문에, `joints_vis` 를 이용해서 실제로 사용할 수 있는 keypoint 인지 나타냄\n",
    "- MPII 의 경우 1 (visible) / 0(non) 으로만 나누어지기 때문에 조금 더 쉽게 사용\n",
    "- coco 의 경우 2 / 1 / 0 으로 표현해서 occlusion 상황까지 label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ef04d",
   "metadata": {},
   "source": [
    "- json annotation 을 파싱하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52145a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bcf1903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc445faa",
   "metadata": {},
   "source": [
    "### TFRecord 파일 만들기\n",
    "- 실제 프로젝트에서는 튜토리얼 데이터셋보다 훨씬 큰 크기의 데이터를 다룸\n",
    "- unless you are using tf.data and reading data is still the bottleneck to training.\n",
    "- gpu 의 연산 속도보다 HDD I/O 가 느리기 때문에 병목 현상이 발생\n",
    "- 대단위 프로젝트 실험에서 효율성이 떨어짐\n",
    "\n",
    "**[tf.data API로 성능 향상하기](https://www.tensorflow.org/guide/data_performance?hl=ko)**  \n",
    "- data read(또는 prefetch) 또는 데이터 변환 단계에서 gpu 학습과 병렬적으로 수행되도록 prefetch를 적용해야 함\n",
    "- 수행방법은 tf.data의 map 함수를 이용하고 cache 에 저장해두는 방법 -> TFRecord를 사용하여 해결\n",
    "- TFRecord : binary record sequence 를 저장하기 위한 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099ec5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2eb3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(l, n):\n",
    "    # 전체 데이터 l을 n그룹으로 나눔\n",
    "    # -> n개의 TFRecord 파일을 만듦 (n개로 shard 함)\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7f7e5",
   "metadata": {},
   "source": [
    "- 너무 작은 파일로 많이 나누면 학습 중간에 너무 잦은 입출력이 요구\n",
    "- 너무 큰 파일로 적게 나누면 입출력마다 걸리는 시간이 길어짐\n",
    "- 입출력에 걸리는 시간이 GPU의 계산 시간보다 길어지면 그만큼 손해가 됨\n",
    "\n",
    "**annotation 을 shard 로 나눠야 하는 이유**  \n",
    "- I/O 병목을 피하기 위해 입력 파일을 여러개로 나눈 뒤, 병렬적으로 prefetch 하는 것이 학습 속도를 빠르게 함\n",
    "- 튜토리얼에서는 경험상 데이터를 읽는 호스트보다 최소 10 배 많은 파일을 보유하는 것이 좋음\n",
    "- 동시에 각 파일은 I / O 프리 페치의 이점을 누릴 수 있도록 충분히 커야함\n",
    "- 최소 10MB 이상, 이상적으로는 100MB 이상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d840489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3530243",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707347c7",
   "metadata": {},
   "source": [
    "[`@ray.remote`](https://www.ray.io/) : 병렬 처리용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8db3aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286ab4b",
   "metadata": {},
   "source": [
    "### Ray\n",
    "[What is Ray?](https://docs.ray.io/en/latest/)  \n",
    "\n",
    "```\n",
    "import ray\n",
    "ray.init()\n",
    "\n",
    "@ray.remote\n",
    "def f(x):\n",
    "    return x * x\n",
    "\n",
    "futures = [f.remote(i) for i in range(4)]\n",
    "print(ray.get(futures)) # [0, 1, 4, 9]\n",
    "\n",
    "@ray.remote\n",
    "class Counter(object):\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "\n",
    "    def increment(self):\n",
    "        self.n += 1\n",
    "\n",
    "    def read(self):\n",
    "        return self.n\n",
    "\n",
    "counters = [Counter.remote() for i in range(4)]\n",
    "[c.increment.remote() for c in counters]\n",
    "futures = [c.read.remote() for c in counters]\n",
    "print(ray.get(futures)) # [1, 1, 1, 1]\n",
    "\n",
    "```\n",
    "- 함수나 클래스에 `@ray.remote` 데코레이터를 붙이고 `some_function.remote()`형식으로 함수를 만듦\n",
    "- 클래스의 경우에는 메서드를 호출할 때 `remote()`를 이용\n",
    "- 함수나 메서드는 이 시점에 실행되는 것이 아니라 생성만 됨\n",
    "- `ray.get()`을 통해 실행이 되는 구조\n",
    "\n",
    "**[multiprocessing 과 ray 의 사용상 차이점](https://modulabs.co.kr/blog/ray-multiprocessing-python)**  \n",
    "- MP 는 병렬화를 위해 추상적 구조를 새로 설계해야 하지만 ray 는 쓰던 코드에서 거의 수정 없이 병렬화 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b722ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 12:05:39,124\tWARNING services.py:1729 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.89gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': '/aiffel/aiffel/mpii/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    }
   ],
   "source": [
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb5c3c",
   "metadata": {},
   "source": [
    "### data label 로 만들기\n",
    "- TFRecord로 저장된 데이터를 모델 학습에 필요한 데이터로 바꿔줄 함수가 필요\n",
    "- tensorflow에서 이미 제공해주는 함수를 사용\n",
    "- 주의할 점은 TFRecord가 직렬화된 데이터이기 때문에 만들 때 데이터 순서와 읽어올 때 데이터 순서가 같아야 함\n",
    "- 데이터의 형식도 동일해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09becb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a57dd610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n"
     ]
    }
   ],
   "source": [
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd0c502",
   "metadata": {},
   "source": [
    "![heatmap](https://d3s0tskafalll9.cloudfront.net/media/images/GC-10-P-2.max-800x600.png)  \n",
    "- (x, y) 좌표로 되어있는 keypoint 를 heatmap 으로 변경\n",
    "- 하나의 점에만 표시 되어있는 정보를 좌표 근처 여러 지점에 확률 분포 형태로 학습시키면 결과가 더 좋음\n",
    "- 확률 분포로는 2차원 가우시안 분포를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0d65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6678efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6129f97",
   "metadata": {},
   "source": [
    "### 모델 학습\n",
    "#### Hourglass 모델\n",
    "![model](https://d3s0tskafalll9.cloudfront.net/media/images/GC-10-P-4.max-800x600.png)  \n",
    "- residual block 의 2가지 타입\n",
    "  - 3x3-3x3 basic block, 1x1-3x3-1x1 bottleneck block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb05df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a90a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        # 재귀 함수를 이용\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2551c33",
   "metadata": {},
   "source": [
    "- intermediate output을 위한 linear layer\n",
    "![https://d3s0tskafalll9.cloudfront.net/media/original_images/GC-10-P-5.png](https://d3s0tskafalll9.cloudfront.net/media/original_images/GC-10-P-5.png)  \n",
    "- 여러 모듈을 쌓을수록 모델이 깊어지는 만큼 학습이 어려워, 저자들은 Intermediate supervision을 적용\n",
    "- 모듈 사이의 네트워크의 파란 박스는 모델 중간에 계산되는 히트맵 결과를 출력하는 convolution layer\n",
    "- 이 히트맵과 ground truth의 차이를 intermediate loss (auxilary loss) 로 계산\n",
    "- [stacked hourglass module은 보다 정교한 결과를 도출](https://modulabs.co.kr/blog/stacked-hourglass-networks-pose-estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "207973e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001ecd6",
   "metadata": {},
   "source": [
    "![https://d3s0tskafalll9.cloudfront.net/media/images/GC-10-P-6.max-800x600.png](https://d3s0tskafalll9.cloudfront.net/media/images/GC-10-P-6.max-800x600.png)  \n",
    "- stacked 되는 hourglass 층 사이사이에 LinearLayer 를 삽입하고 중간 loss 를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "369acab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9a4df",
   "metadata": {},
   "source": [
    "### 학습 엔진\n",
    "#### [GPU가 여러개인 환경](https://www.tensorflow.org/tutorials/distribute/keras)\n",
    "- `tf.distribute.MirroredStrategy`\n",
    "  - 한 컴퓨터에 GPU가 여러 개인 경우 사용할 수 있는 방법\n",
    "  - 여러 GPU가 모델을 학습한 후 각각의 Loss를 계산하면 CPU가 전체 Loss를 종합\n",
    "  - 그런 후 모델의 가중치를 업데이트\n",
    "- `strategy.reduce`\n",
    "  - 각 GPU에서 계산한 Loss를 토대로 전체 Loss를 종합해주는 역할\n",
    "- 그 외\n",
    "  - MirroredStrategy : TensorFlow에서 다중 GPU 환경에서 모델을 훈련하기 위한 분산 전략\n",
    "  - TPUStrategy : 구글의 Tensor Processing Units(TPU)를 사용하여 모델을 훈련하기 위한 분산 전략\n",
    "  - MultiWorkerMirroredStrategy : 다중 워커(worker) 환경에서 모델을 훈련하기 위한 분산 전략\n",
    "  - ParameterServerStrategy : 다중 서버(server) 환경에서 모델을 훈련하기 위한 분산 전략\n",
    "  - CentralStorageStrategy : 중앙 저장소를 사용하여 모델을 훈련하기 위한 분산 전략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a53fc8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "356e4cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c7c02",
   "metadata": {},
   "source": [
    "- `with strategy.scope():`\n",
    "  - https://www.tensorflow.org/tutorials/distribute/custom_training\n",
    "- `experimental_distribute_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b892df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2b99d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.12009263 epoch total loss 2.12009263\n",
      "Trained batch 2 batch loss 2.12654424 epoch total loss 2.12331843\n",
      "Trained batch 3 batch loss 2.36624193 epoch total loss 2.20429301\n",
      "Trained batch 4 batch loss 2.42921 epoch total loss 2.26052213\n",
      "Trained batch 5 batch loss 2.38259649 epoch total loss 2.2849369\n",
      "Trained batch 6 batch loss 2.23971128 epoch total loss 2.2773993\n",
      "Trained batch 7 batch loss 2.17191243 epoch total loss 2.26232982\n",
      "Trained batch 8 batch loss 1.99617171 epoch total loss 2.22906\n",
      "Trained batch 9 batch loss 2.28736401 epoch total loss 2.23553801\n",
      "Trained batch 10 batch loss 2.27136898 epoch total loss 2.2391212\n",
      "Trained batch 11 batch loss 2.20829 epoch total loss 2.23631835\n",
      "Trained batch 12 batch loss 2.11149955 epoch total loss 2.22591686\n",
      "Trained batch 13 batch loss 2.01417232 epoch total loss 2.20962882\n",
      "Trained batch 14 batch loss 2.03427458 epoch total loss 2.1971035\n",
      "Trained batch 15 batch loss 2.03839493 epoch total loss 2.18652296\n",
      "Trained batch 16 batch loss 2.04831576 epoch total loss 2.17788506\n",
      "Trained batch 17 batch loss 2.02324891 epoch total loss 2.16878891\n",
      "Trained batch 18 batch loss 2.05940914 epoch total loss 2.16271234\n",
      "Trained batch 19 batch loss 2.011024 epoch total loss 2.15472865\n",
      "Trained batch 20 batch loss 1.99270964 epoch total loss 2.1466279\n",
      "Trained batch 21 batch loss 1.94852781 epoch total loss 2.1371944\n",
      "Trained batch 22 batch loss 1.99808276 epoch total loss 2.13087106\n",
      "Trained batch 23 batch loss 1.87401235 epoch total loss 2.11970329\n",
      "Trained batch 24 batch loss 1.86546457 epoch total loss 2.10911\n",
      "Trained batch 25 batch loss 1.93560171 epoch total loss 2.10216975\n",
      "Trained batch 26 batch loss 1.81557429 epoch total loss 2.09114671\n",
      "Trained batch 27 batch loss 1.7245903 epoch total loss 2.07757068\n",
      "Trained batch 28 batch loss 1.88981986 epoch total loss 2.07086515\n",
      "Trained batch 29 batch loss 1.93203473 epoch total loss 2.06607795\n",
      "Trained batch 30 batch loss 1.93942702 epoch total loss 2.06185627\n",
      "Trained batch 31 batch loss 1.9164561 epoch total loss 2.05716586\n",
      "Trained batch 32 batch loss 1.85907888 epoch total loss 2.05097556\n",
      "Trained batch 33 batch loss 1.87557983 epoch total loss 2.0456605\n",
      "Trained batch 34 batch loss 1.86802161 epoch total loss 2.04043579\n",
      "Trained batch 35 batch loss 1.71341264 epoch total loss 2.03109217\n",
      "Trained batch 36 batch loss 1.76154387 epoch total loss 2.02360463\n",
      "Trained batch 37 batch loss 1.81334615 epoch total loss 2.01792216\n",
      "Trained batch 38 batch loss 1.84653437 epoch total loss 2.01341176\n",
      "Trained batch 39 batch loss 1.89472604 epoch total loss 2.01036882\n",
      "Trained batch 40 batch loss 1.92110133 epoch total loss 2.00813723\n",
      "Trained batch 41 batch loss 1.95192361 epoch total loss 2.00676608\n",
      "Trained batch 42 batch loss 1.84328341 epoch total loss 2.00287366\n",
      "Trained batch 43 batch loss 1.75334442 epoch total loss 1.99707067\n",
      "Trained batch 44 batch loss 1.80829608 epoch total loss 1.99278033\n",
      "Trained batch 45 batch loss 1.64353204 epoch total loss 1.98501921\n",
      "Trained batch 46 batch loss 1.80608606 epoch total loss 1.98112941\n",
      "Trained batch 47 batch loss 1.77337742 epoch total loss 1.97670913\n",
      "Trained batch 48 batch loss 1.75612116 epoch total loss 1.97211349\n",
      "Trained batch 49 batch loss 1.51924586 epoch total loss 1.96287131\n",
      "Trained batch 50 batch loss 1.83452404 epoch total loss 1.96030438\n",
      "Trained batch 51 batch loss 1.84955752 epoch total loss 1.95813286\n",
      "Trained batch 52 batch loss 1.75444436 epoch total loss 1.95421588\n",
      "Trained batch 53 batch loss 1.79093838 epoch total loss 1.95113516\n",
      "Trained batch 54 batch loss 1.77101088 epoch total loss 1.94779956\n",
      "Trained batch 55 batch loss 1.82645106 epoch total loss 1.94559324\n",
      "Trained batch 56 batch loss 1.78927207 epoch total loss 1.94280171\n",
      "Trained batch 57 batch loss 1.79328394 epoch total loss 1.94017863\n",
      "Trained batch 58 batch loss 1.62825298 epoch total loss 1.93480051\n",
      "Trained batch 59 batch loss 1.77235866 epoch total loss 1.93204737\n",
      "Trained batch 60 batch loss 1.76176095 epoch total loss 1.92920923\n",
      "Trained batch 61 batch loss 1.74806881 epoch total loss 1.92623973\n",
      "Trained batch 62 batch loss 1.74635172 epoch total loss 1.92333841\n",
      "Trained batch 63 batch loss 1.73740578 epoch total loss 1.92038703\n",
      "Trained batch 64 batch loss 1.65953219 epoch total loss 1.91631114\n",
      "Trained batch 65 batch loss 1.72602773 epoch total loss 1.91338372\n",
      "Trained batch 66 batch loss 1.73457694 epoch total loss 1.91067445\n",
      "Trained batch 67 batch loss 1.72425842 epoch total loss 1.90789211\n",
      "Trained batch 68 batch loss 1.73617172 epoch total loss 1.90536678\n",
      "Trained batch 69 batch loss 1.68967509 epoch total loss 1.90224087\n",
      "Trained batch 70 batch loss 1.70099378 epoch total loss 1.8993659\n",
      "Trained batch 71 batch loss 1.70714521 epoch total loss 1.89665842\n",
      "Trained batch 72 batch loss 1.70954323 epoch total loss 1.89405966\n",
      "Trained batch 73 batch loss 1.69322562 epoch total loss 1.89130855\n",
      "Trained batch 74 batch loss 1.7042861 epoch total loss 1.88878119\n",
      "Trained batch 75 batch loss 1.77286434 epoch total loss 1.88723552\n",
      "Trained batch 76 batch loss 1.67916441 epoch total loss 1.88449776\n",
      "Trained batch 77 batch loss 1.70110631 epoch total loss 1.8821162\n",
      "Trained batch 78 batch loss 1.68446946 epoch total loss 1.87958217\n",
      "Trained batch 79 batch loss 1.75416481 epoch total loss 1.87799454\n",
      "Trained batch 80 batch loss 1.80516255 epoch total loss 1.87708414\n",
      "Trained batch 81 batch loss 1.72151816 epoch total loss 1.87516356\n",
      "Trained batch 82 batch loss 1.79666829 epoch total loss 1.87420619\n",
      "Trained batch 83 batch loss 1.73645937 epoch total loss 1.87254667\n",
      "Trained batch 84 batch loss 1.66968822 epoch total loss 1.87013173\n",
      "Trained batch 85 batch loss 1.67438054 epoch total loss 1.86782873\n",
      "Trained batch 86 batch loss 1.71441555 epoch total loss 1.86604488\n",
      "Trained batch 87 batch loss 1.65551519 epoch total loss 1.86362505\n",
      "Trained batch 88 batch loss 1.75421214 epoch total loss 1.8623817\n",
      "Trained batch 89 batch loss 1.78986979 epoch total loss 1.8615669\n",
      "Trained batch 90 batch loss 1.77588618 epoch total loss 1.8606149\n",
      "Trained batch 91 batch loss 1.74629486 epoch total loss 1.85935855\n",
      "Trained batch 92 batch loss 1.74447513 epoch total loss 1.85810983\n",
      "Trained batch 93 batch loss 1.68423557 epoch total loss 1.85624027\n",
      "Trained batch 94 batch loss 1.7762959 epoch total loss 1.85538971\n",
      "Trained batch 95 batch loss 1.78047872 epoch total loss 1.85460114\n",
      "Trained batch 96 batch loss 1.81195652 epoch total loss 1.85415685\n",
      "Trained batch 97 batch loss 1.81467283 epoch total loss 1.85374975\n",
      "Trained batch 98 batch loss 1.78796494 epoch total loss 1.85307837\n",
      "Trained batch 99 batch loss 1.75821257 epoch total loss 1.85212016\n",
      "Trained batch 100 batch loss 1.6505363 epoch total loss 1.85010433\n",
      "Trained batch 101 batch loss 1.66869509 epoch total loss 1.84830832\n",
      "Trained batch 102 batch loss 1.65597785 epoch total loss 1.84642267\n",
      "Trained batch 103 batch loss 1.79452145 epoch total loss 1.84591877\n",
      "Trained batch 104 batch loss 1.76446331 epoch total loss 1.84513557\n",
      "Trained batch 105 batch loss 1.8221972 epoch total loss 1.84491718\n",
      "Trained batch 106 batch loss 1.75421834 epoch total loss 1.84406149\n",
      "Trained batch 107 batch loss 1.687958 epoch total loss 1.84260261\n",
      "Trained batch 108 batch loss 1.69834816 epoch total loss 1.84126687\n",
      "Trained batch 109 batch loss 1.51266038 epoch total loss 1.83825219\n",
      "Trained batch 110 batch loss 1.7164917 epoch total loss 1.83714533\n",
      "Trained batch 111 batch loss 1.78568566 epoch total loss 1.83668172\n",
      "Trained batch 112 batch loss 1.83706975 epoch total loss 1.83668518\n",
      "Trained batch 113 batch loss 1.75030816 epoch total loss 1.83592081\n",
      "Trained batch 114 batch loss 1.78414571 epoch total loss 1.83546662\n",
      "Trained batch 115 batch loss 1.80227053 epoch total loss 1.83517802\n",
      "Trained batch 116 batch loss 1.67993593 epoch total loss 1.83383965\n",
      "Trained batch 117 batch loss 1.59853053 epoch total loss 1.83182847\n",
      "Trained batch 118 batch loss 1.70476604 epoch total loss 1.83075166\n",
      "Trained batch 119 batch loss 1.75582552 epoch total loss 1.83012211\n",
      "Trained batch 120 batch loss 1.64507425 epoch total loss 1.82858014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 121 batch loss 1.62215519 epoch total loss 1.82687414\n",
      "Trained batch 122 batch loss 1.76209593 epoch total loss 1.82634318\n",
      "Trained batch 123 batch loss 1.63841009 epoch total loss 1.82481527\n",
      "Trained batch 124 batch loss 1.69271815 epoch total loss 1.82375\n",
      "Trained batch 125 batch loss 1.68077469 epoch total loss 1.82260621\n",
      "Trained batch 126 batch loss 1.81749129 epoch total loss 1.82256556\n",
      "Trained batch 127 batch loss 1.7841152 epoch total loss 1.82226288\n",
      "Trained batch 128 batch loss 1.76659191 epoch total loss 1.82182789\n",
      "Trained batch 129 batch loss 1.7843082 epoch total loss 1.82153702\n",
      "Trained batch 130 batch loss 1.68618703 epoch total loss 1.82049584\n",
      "Trained batch 131 batch loss 1.50643408 epoch total loss 1.81809843\n",
      "Trained batch 132 batch loss 1.66201437 epoch total loss 1.81691599\n",
      "Trained batch 133 batch loss 1.65650296 epoch total loss 1.81571\n",
      "Trained batch 134 batch loss 1.66567826 epoch total loss 1.81459033\n",
      "Trained batch 135 batch loss 1.69259572 epoch total loss 1.81368673\n",
      "Trained batch 136 batch loss 1.68860972 epoch total loss 1.81276703\n",
      "Trained batch 137 batch loss 1.71624935 epoch total loss 1.8120625\n",
      "Trained batch 138 batch loss 1.76248538 epoch total loss 1.81170321\n",
      "Trained batch 139 batch loss 1.77948833 epoch total loss 1.81147146\n",
      "Trained batch 140 batch loss 1.69129324 epoch total loss 1.81061316\n",
      "Trained batch 141 batch loss 1.75986242 epoch total loss 1.81025314\n",
      "Trained batch 142 batch loss 1.70769048 epoch total loss 1.80953097\n",
      "Trained batch 143 batch loss 1.69712603 epoch total loss 1.80874479\n",
      "Trained batch 144 batch loss 1.55733204 epoch total loss 1.80699897\n",
      "Trained batch 145 batch loss 1.74316168 epoch total loss 1.80655873\n",
      "Trained batch 146 batch loss 1.69435084 epoch total loss 1.80579007\n",
      "Trained batch 147 batch loss 1.74665976 epoch total loss 1.80538797\n",
      "Trained batch 148 batch loss 1.64924693 epoch total loss 1.80433309\n",
      "Trained batch 149 batch loss 1.5842737 epoch total loss 1.80285609\n",
      "Trained batch 150 batch loss 1.53723204 epoch total loss 1.80108523\n",
      "Trained batch 151 batch loss 1.58895421 epoch total loss 1.79968035\n",
      "Trained batch 152 batch loss 1.73380494 epoch total loss 1.79924691\n",
      "Trained batch 153 batch loss 1.64606833 epoch total loss 1.79824567\n",
      "Trained batch 154 batch loss 1.66164207 epoch total loss 1.79735875\n",
      "Trained batch 155 batch loss 1.66774035 epoch total loss 1.79652262\n",
      "Trained batch 156 batch loss 1.56821716 epoch total loss 1.79505897\n",
      "Trained batch 157 batch loss 1.55443907 epoch total loss 1.79352641\n",
      "Trained batch 158 batch loss 1.68161941 epoch total loss 1.79281807\n",
      "Trained batch 159 batch loss 1.64017344 epoch total loss 1.79185808\n",
      "Trained batch 160 batch loss 1.56642365 epoch total loss 1.79044914\n",
      "Trained batch 161 batch loss 1.6535269 epoch total loss 1.7895987\n",
      "Trained batch 162 batch loss 1.6252687 epoch total loss 1.78858435\n",
      "Trained batch 163 batch loss 1.66502774 epoch total loss 1.78782642\n",
      "Trained batch 164 batch loss 1.59895074 epoch total loss 1.78667474\n",
      "Trained batch 165 batch loss 1.63637066 epoch total loss 1.78576386\n",
      "Trained batch 166 batch loss 1.45502985 epoch total loss 1.7837714\n",
      "Trained batch 167 batch loss 1.61389089 epoch total loss 1.78275418\n",
      "Trained batch 168 batch loss 1.64504576 epoch total loss 1.7819345\n",
      "Trained batch 169 batch loss 1.64945102 epoch total loss 1.78115046\n",
      "Trained batch 170 batch loss 1.63609242 epoch total loss 1.78029716\n",
      "Trained batch 171 batch loss 1.63327408 epoch total loss 1.7794373\n",
      "Trained batch 172 batch loss 1.6414448 epoch total loss 1.77863503\n",
      "Trained batch 173 batch loss 1.50916982 epoch total loss 1.77707744\n",
      "Trained batch 174 batch loss 1.5645287 epoch total loss 1.77585578\n",
      "Trained batch 175 batch loss 1.53458846 epoch total loss 1.77447701\n",
      "Trained batch 176 batch loss 1.65152824 epoch total loss 1.77377844\n",
      "Trained batch 177 batch loss 1.63189375 epoch total loss 1.77297676\n",
      "Trained batch 178 batch loss 1.50857759 epoch total loss 1.77149141\n",
      "Trained batch 179 batch loss 1.63491023 epoch total loss 1.77072847\n",
      "Trained batch 180 batch loss 1.55680883 epoch total loss 1.76954007\n",
      "Trained batch 181 batch loss 1.62397218 epoch total loss 1.76873577\n",
      "Trained batch 182 batch loss 1.64851475 epoch total loss 1.76807535\n",
      "Trained batch 183 batch loss 1.44256008 epoch total loss 1.76629651\n",
      "Trained batch 184 batch loss 1.4013443 epoch total loss 1.7643131\n",
      "Trained batch 185 batch loss 1.50338316 epoch total loss 1.76290262\n",
      "Trained batch 186 batch loss 1.58065295 epoch total loss 1.76192284\n",
      "Trained batch 187 batch loss 1.617208 epoch total loss 1.76114905\n",
      "Trained batch 188 batch loss 1.53797138 epoch total loss 1.75996184\n",
      "Trained batch 189 batch loss 1.62354302 epoch total loss 1.75924\n",
      "Trained batch 190 batch loss 1.61186504 epoch total loss 1.75846446\n",
      "Trained batch 191 batch loss 1.69328725 epoch total loss 1.75812328\n",
      "Trained batch 192 batch loss 1.61955404 epoch total loss 1.75740159\n",
      "Trained batch 193 batch loss 1.67330396 epoch total loss 1.75696588\n",
      "Trained batch 194 batch loss 1.76398706 epoch total loss 1.757002\n",
      "Trained batch 195 batch loss 1.70494986 epoch total loss 1.75673521\n",
      "Trained batch 196 batch loss 1.38604987 epoch total loss 1.75484383\n",
      "Trained batch 197 batch loss 1.31673145 epoch total loss 1.75262\n",
      "Trained batch 198 batch loss 1.54782987 epoch total loss 1.75158572\n",
      "Trained batch 199 batch loss 1.57220769 epoch total loss 1.75068426\n",
      "Trained batch 200 batch loss 1.69458127 epoch total loss 1.75040376\n",
      "Trained batch 201 batch loss 1.7316277 epoch total loss 1.7503103\n",
      "Trained batch 202 batch loss 1.58721721 epoch total loss 1.7495029\n",
      "Trained batch 203 batch loss 1.51943445 epoch total loss 1.74836969\n",
      "Trained batch 204 batch loss 1.68973243 epoch total loss 1.74808216\n",
      "Trained batch 205 batch loss 1.71310699 epoch total loss 1.74791157\n",
      "Trained batch 206 batch loss 1.74320579 epoch total loss 1.74788868\n",
      "Trained batch 207 batch loss 1.71681166 epoch total loss 1.74773848\n",
      "Trained batch 208 batch loss 1.7112602 epoch total loss 1.74756312\n",
      "Trained batch 209 batch loss 1.73538589 epoch total loss 1.74750483\n",
      "Trained batch 210 batch loss 1.5835011 epoch total loss 1.74672389\n",
      "Trained batch 211 batch loss 1.69759071 epoch total loss 1.74649107\n",
      "Trained batch 212 batch loss 1.61139691 epoch total loss 1.74585378\n",
      "Trained batch 213 batch loss 1.61815596 epoch total loss 1.74525428\n",
      "Trained batch 214 batch loss 1.68962908 epoch total loss 1.7449944\n",
      "Trained batch 215 batch loss 1.66480863 epoch total loss 1.7446214\n",
      "Trained batch 216 batch loss 1.43343616 epoch total loss 1.74318075\n",
      "Trained batch 217 batch loss 1.62292981 epoch total loss 1.74262655\n",
      "Trained batch 218 batch loss 1.61655712 epoch total loss 1.74204826\n",
      "Trained batch 219 batch loss 1.6323843 epoch total loss 1.74154747\n",
      "Trained batch 220 batch loss 1.65860915 epoch total loss 1.74117041\n",
      "Trained batch 221 batch loss 1.66717684 epoch total loss 1.74083567\n",
      "Trained batch 222 batch loss 1.69775641 epoch total loss 1.74064159\n",
      "Trained batch 223 batch loss 1.62986851 epoch total loss 1.74014485\n",
      "Trained batch 224 batch loss 1.64940965 epoch total loss 1.73973978\n",
      "Trained batch 225 batch loss 1.69281197 epoch total loss 1.73953128\n",
      "Trained batch 226 batch loss 1.64766717 epoch total loss 1.73912477\n",
      "Trained batch 227 batch loss 1.67577505 epoch total loss 1.73884571\n",
      "Trained batch 228 batch loss 1.69796586 epoch total loss 1.73866642\n",
      "Trained batch 229 batch loss 1.71568835 epoch total loss 1.73856616\n",
      "Trained batch 230 batch loss 1.75598264 epoch total loss 1.73864186\n",
      "Trained batch 231 batch loss 1.69588888 epoch total loss 1.73845685\n",
      "Trained batch 232 batch loss 1.73768497 epoch total loss 1.73845339\n",
      "Trained batch 233 batch loss 1.72703981 epoch total loss 1.73840451\n",
      "Trained batch 234 batch loss 1.65756214 epoch total loss 1.73805904\n",
      "Trained batch 235 batch loss 1.65571606 epoch total loss 1.73770869\n",
      "Trained batch 236 batch loss 1.68884969 epoch total loss 1.73750162\n",
      "Trained batch 237 batch loss 1.66394567 epoch total loss 1.7371912\n",
      "Trained batch 238 batch loss 1.61332071 epoch total loss 1.73667073\n",
      "Trained batch 239 batch loss 1.67756188 epoch total loss 1.73642337\n",
      "Trained batch 240 batch loss 1.68512106 epoch total loss 1.73620963\n",
      "Trained batch 241 batch loss 1.64950383 epoch total loss 1.73584986\n",
      "Trained batch 242 batch loss 1.67213237 epoch total loss 1.73558652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 243 batch loss 1.6841588 epoch total loss 1.73537493\n",
      "Trained batch 244 batch loss 1.65491962 epoch total loss 1.73504519\n",
      "Trained batch 245 batch loss 1.72650731 epoch total loss 1.73501027\n",
      "Trained batch 246 batch loss 1.68937695 epoch total loss 1.73482478\n",
      "Trained batch 247 batch loss 1.70281053 epoch total loss 1.7346952\n",
      "Trained batch 248 batch loss 1.67475545 epoch total loss 1.73445356\n",
      "Trained batch 249 batch loss 1.71316862 epoch total loss 1.73436797\n",
      "Trained batch 250 batch loss 1.66583598 epoch total loss 1.7340939\n",
      "Trained batch 251 batch loss 1.63443232 epoch total loss 1.73369682\n",
      "Trained batch 252 batch loss 1.63103139 epoch total loss 1.73328948\n",
      "Trained batch 253 batch loss 1.66587436 epoch total loss 1.73302293\n",
      "Trained batch 254 batch loss 1.61070311 epoch total loss 1.73254144\n",
      "Trained batch 255 batch loss 1.62153053 epoch total loss 1.73210609\n",
      "Trained batch 256 batch loss 1.69889879 epoch total loss 1.73197639\n",
      "Trained batch 257 batch loss 1.71713424 epoch total loss 1.73191857\n",
      "Trained batch 258 batch loss 1.6167171 epoch total loss 1.73147213\n",
      "Trained batch 259 batch loss 1.66729307 epoch total loss 1.73122442\n",
      "Trained batch 260 batch loss 1.64526129 epoch total loss 1.73089373\n",
      "Trained batch 261 batch loss 1.53524423 epoch total loss 1.73014414\n",
      "Trained batch 262 batch loss 1.66293764 epoch total loss 1.7298876\n",
      "Trained batch 263 batch loss 1.62407863 epoch total loss 1.72948539\n",
      "Trained batch 264 batch loss 1.66330254 epoch total loss 1.7292347\n",
      "Trained batch 265 batch loss 1.62878263 epoch total loss 1.72885561\n",
      "Trained batch 266 batch loss 1.63269901 epoch total loss 1.72849405\n",
      "Trained batch 267 batch loss 1.68021834 epoch total loss 1.72831321\n",
      "Trained batch 268 batch loss 1.63935614 epoch total loss 1.72798121\n",
      "Trained batch 269 batch loss 1.66218328 epoch total loss 1.72773659\n",
      "Trained batch 270 batch loss 1.65071785 epoch total loss 1.72745132\n",
      "Trained batch 271 batch loss 1.65899932 epoch total loss 1.72719872\n",
      "Trained batch 272 batch loss 1.51233494 epoch total loss 1.72640884\n",
      "Trained batch 273 batch loss 1.5049417 epoch total loss 1.72559762\n",
      "Trained batch 274 batch loss 1.41652274 epoch total loss 1.72446954\n",
      "Trained batch 275 batch loss 1.49242 epoch total loss 1.72362578\n",
      "Trained batch 276 batch loss 1.56954157 epoch total loss 1.72306752\n",
      "Trained batch 277 batch loss 1.51784801 epoch total loss 1.72232676\n",
      "Trained batch 278 batch loss 1.55585384 epoch total loss 1.72172785\n",
      "Trained batch 279 batch loss 1.45500922 epoch total loss 1.72077191\n",
      "Trained batch 280 batch loss 1.6005621 epoch total loss 1.72034264\n",
      "Trained batch 281 batch loss 1.47637582 epoch total loss 1.71947443\n",
      "Trained batch 282 batch loss 1.52710938 epoch total loss 1.7187922\n",
      "Trained batch 283 batch loss 1.65352702 epoch total loss 1.71856165\n",
      "Trained batch 284 batch loss 1.64931095 epoch total loss 1.71831787\n",
      "Trained batch 285 batch loss 1.58528852 epoch total loss 1.71785104\n",
      "Trained batch 286 batch loss 1.5541321 epoch total loss 1.7172786\n",
      "Trained batch 287 batch loss 1.65184951 epoch total loss 1.71705067\n",
      "Trained batch 288 batch loss 1.66975391 epoch total loss 1.7168864\n",
      "Trained batch 289 batch loss 1.60202634 epoch total loss 1.71648896\n",
      "Trained batch 290 batch loss 1.59379768 epoch total loss 1.71606588\n",
      "Trained batch 291 batch loss 1.58995247 epoch total loss 1.71563256\n",
      "Trained batch 292 batch loss 1.60993743 epoch total loss 1.71527052\n",
      "Trained batch 293 batch loss 1.63438678 epoch total loss 1.71499455\n",
      "Trained batch 294 batch loss 1.59096515 epoch total loss 1.71457267\n",
      "Trained batch 295 batch loss 1.61306787 epoch total loss 1.71422863\n",
      "Trained batch 296 batch loss 1.63709688 epoch total loss 1.71396804\n",
      "Trained batch 297 batch loss 1.65953588 epoch total loss 1.71378481\n",
      "Trained batch 298 batch loss 1.7294383 epoch total loss 1.71383727\n",
      "Trained batch 299 batch loss 1.65908647 epoch total loss 1.71365404\n",
      "Trained batch 300 batch loss 1.63357174 epoch total loss 1.71338701\n",
      "Trained batch 301 batch loss 1.64942026 epoch total loss 1.71317446\n",
      "Trained batch 302 batch loss 1.66069198 epoch total loss 1.71300077\n",
      "Trained batch 303 batch loss 1.60089862 epoch total loss 1.71263075\n",
      "Trained batch 304 batch loss 1.58419919 epoch total loss 1.71220839\n",
      "Trained batch 305 batch loss 1.5884428 epoch total loss 1.7118026\n",
      "Trained batch 306 batch loss 1.64898276 epoch total loss 1.71159732\n",
      "Trained batch 307 batch loss 1.68052673 epoch total loss 1.71149611\n",
      "Trained batch 308 batch loss 1.6516794 epoch total loss 1.71130192\n",
      "Trained batch 309 batch loss 1.60306764 epoch total loss 1.71095169\n",
      "Trained batch 310 batch loss 1.69132042 epoch total loss 1.7108885\n",
      "Trained batch 311 batch loss 1.57888699 epoch total loss 1.71046388\n",
      "Trained batch 312 batch loss 1.66036832 epoch total loss 1.71030331\n",
      "Trained batch 313 batch loss 1.6413871 epoch total loss 1.71008301\n",
      "Trained batch 314 batch loss 1.50054324 epoch total loss 1.70941567\n",
      "Trained batch 315 batch loss 1.52151394 epoch total loss 1.70881915\n",
      "Trained batch 316 batch loss 1.6268208 epoch total loss 1.70855963\n",
      "Trained batch 317 batch loss 1.63767207 epoch total loss 1.70833611\n",
      "Trained batch 318 batch loss 1.57077026 epoch total loss 1.70790339\n",
      "Trained batch 319 batch loss 1.64908528 epoch total loss 1.70771909\n",
      "Trained batch 320 batch loss 1.58008063 epoch total loss 1.70732021\n",
      "Trained batch 321 batch loss 1.65721214 epoch total loss 1.70716417\n",
      "Trained batch 322 batch loss 1.56048286 epoch total loss 1.70670867\n",
      "Trained batch 323 batch loss 1.64043844 epoch total loss 1.70650351\n",
      "Trained batch 324 batch loss 1.67725039 epoch total loss 1.70641315\n",
      "Trained batch 325 batch loss 1.61639655 epoch total loss 1.70613623\n",
      "Trained batch 326 batch loss 1.53319705 epoch total loss 1.70560575\n",
      "Trained batch 327 batch loss 1.64109969 epoch total loss 1.70540845\n",
      "Trained batch 328 batch loss 1.54879653 epoch total loss 1.7049309\n",
      "Trained batch 329 batch loss 1.59385896 epoch total loss 1.70459342\n",
      "Trained batch 330 batch loss 1.65509081 epoch total loss 1.70444334\n",
      "Trained batch 331 batch loss 1.63789666 epoch total loss 1.70424223\n",
      "Trained batch 332 batch loss 1.63306284 epoch total loss 1.70402789\n",
      "Trained batch 333 batch loss 1.61223042 epoch total loss 1.70375216\n",
      "Trained batch 334 batch loss 1.5399251 epoch total loss 1.70326173\n",
      "Trained batch 335 batch loss 1.50678432 epoch total loss 1.7026751\n",
      "Trained batch 336 batch loss 1.58304071 epoch total loss 1.70231915\n",
      "Trained batch 337 batch loss 1.64677286 epoch total loss 1.7021544\n",
      "Trained batch 338 batch loss 1.55266857 epoch total loss 1.70171213\n",
      "Trained batch 339 batch loss 1.5395416 epoch total loss 1.70123386\n",
      "Trained batch 340 batch loss 1.68168902 epoch total loss 1.7011764\n",
      "Trained batch 341 batch loss 1.68297148 epoch total loss 1.701123\n",
      "Trained batch 342 batch loss 1.55387282 epoch total loss 1.70069253\n",
      "Trained batch 343 batch loss 1.42102885 epoch total loss 1.69987714\n",
      "Trained batch 344 batch loss 1.56816554 epoch total loss 1.69949424\n",
      "Trained batch 345 batch loss 1.67542541 epoch total loss 1.69942451\n",
      "Trained batch 346 batch loss 1.64609718 epoch total loss 1.69927049\n",
      "Trained batch 347 batch loss 1.66467905 epoch total loss 1.69917071\n",
      "Trained batch 348 batch loss 1.71552181 epoch total loss 1.69921768\n",
      "Trained batch 349 batch loss 1.72679234 epoch total loss 1.69929671\n",
      "Trained batch 350 batch loss 1.67882729 epoch total loss 1.6992383\n",
      "Trained batch 351 batch loss 1.51006424 epoch total loss 1.69869936\n",
      "Trained batch 352 batch loss 1.59803629 epoch total loss 1.69841337\n",
      "Trained batch 353 batch loss 1.58468699 epoch total loss 1.69809127\n",
      "Trained batch 354 batch loss 1.57605457 epoch total loss 1.69774652\n",
      "Trained batch 355 batch loss 1.54778576 epoch total loss 1.69732404\n",
      "Trained batch 356 batch loss 1.67214596 epoch total loss 1.69725323\n",
      "Trained batch 357 batch loss 1.6427511 epoch total loss 1.69710064\n",
      "Trained batch 358 batch loss 1.57031965 epoch total loss 1.69674647\n",
      "Trained batch 359 batch loss 1.57553244 epoch total loss 1.69640887\n",
      "Trained batch 360 batch loss 1.62698328 epoch total loss 1.69621599\n",
      "Trained batch 361 batch loss 1.60117936 epoch total loss 1.69595277\n",
      "Trained batch 362 batch loss 1.66242731 epoch total loss 1.69586015\n",
      "Trained batch 363 batch loss 1.64205527 epoch total loss 1.69571185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 364 batch loss 1.62066627 epoch total loss 1.69550562\n",
      "Trained batch 365 batch loss 1.60866761 epoch total loss 1.69526768\n",
      "Trained batch 366 batch loss 1.61451674 epoch total loss 1.69504702\n",
      "Trained batch 367 batch loss 1.58026052 epoch total loss 1.69473422\n",
      "Trained batch 368 batch loss 1.61728907 epoch total loss 1.69452381\n",
      "Trained batch 369 batch loss 1.63184667 epoch total loss 1.69435394\n",
      "Trained batch 370 batch loss 1.67543817 epoch total loss 1.6943028\n",
      "Trained batch 371 batch loss 1.70584702 epoch total loss 1.69433391\n",
      "Trained batch 372 batch loss 1.64167595 epoch total loss 1.69419241\n",
      "Trained batch 373 batch loss 1.67562366 epoch total loss 1.69414258\n",
      "Trained batch 374 batch loss 1.75501442 epoch total loss 1.6943053\n",
      "Trained batch 375 batch loss 1.70833445 epoch total loss 1.69434261\n",
      "Trained batch 376 batch loss 1.51266718 epoch total loss 1.69385946\n",
      "Trained batch 377 batch loss 1.46029294 epoch total loss 1.69323993\n",
      "Trained batch 378 batch loss 1.41242599 epoch total loss 1.69249701\n",
      "Trained batch 379 batch loss 1.44674706 epoch total loss 1.69184864\n",
      "Trained batch 380 batch loss 1.51266348 epoch total loss 1.69137704\n",
      "Trained batch 381 batch loss 1.33306491 epoch total loss 1.6904366\n",
      "Trained batch 382 batch loss 1.32077897 epoch total loss 1.68946898\n",
      "Trained batch 383 batch loss 1.35674894 epoch total loss 1.68860018\n",
      "Trained batch 384 batch loss 1.39041543 epoch total loss 1.68782377\n",
      "Trained batch 385 batch loss 1.55804396 epoch total loss 1.68748665\n",
      "Trained batch 386 batch loss 1.56438112 epoch total loss 1.68716776\n",
      "Trained batch 387 batch loss 1.648718 epoch total loss 1.68706846\n",
      "Trained batch 388 batch loss 1.66427755 epoch total loss 1.68700981\n",
      "Trained batch 389 batch loss 1.69669127 epoch total loss 1.68703473\n",
      "Trained batch 390 batch loss 1.71510983 epoch total loss 1.68710673\n",
      "Trained batch 391 batch loss 1.72997653 epoch total loss 1.6872164\n",
      "Trained batch 392 batch loss 1.64809978 epoch total loss 1.6871165\n",
      "Trained batch 393 batch loss 1.6898824 epoch total loss 1.68712354\n",
      "Trained batch 394 batch loss 1.64733338 epoch total loss 1.68702257\n",
      "Trained batch 395 batch loss 1.65810466 epoch total loss 1.68694925\n",
      "Trained batch 396 batch loss 1.61818743 epoch total loss 1.68677557\n",
      "Trained batch 397 batch loss 1.491189 epoch total loss 1.68628299\n",
      "Trained batch 398 batch loss 1.62220752 epoch total loss 1.68612194\n",
      "Trained batch 399 batch loss 1.5903554 epoch total loss 1.68588185\n",
      "Trained batch 400 batch loss 1.58502352 epoch total loss 1.68562973\n",
      "Trained batch 401 batch loss 1.61802483 epoch total loss 1.68546116\n",
      "Trained batch 402 batch loss 1.58441567 epoch total loss 1.68520987\n",
      "Trained batch 403 batch loss 1.60563564 epoch total loss 1.68501246\n",
      "Trained batch 404 batch loss 1.50587845 epoch total loss 1.684569\n",
      "Trained batch 405 batch loss 1.54542327 epoch total loss 1.68422532\n",
      "Trained batch 406 batch loss 1.55110967 epoch total loss 1.68389738\n",
      "Trained batch 407 batch loss 1.52808368 epoch total loss 1.6835146\n",
      "Trained batch 408 batch loss 1.49053252 epoch total loss 1.68304157\n",
      "Trained batch 409 batch loss 1.56126857 epoch total loss 1.68274391\n",
      "Trained batch 410 batch loss 1.47469568 epoch total loss 1.68223643\n",
      "Trained batch 411 batch loss 1.46118402 epoch total loss 1.68169856\n",
      "Trained batch 412 batch loss 1.49501264 epoch total loss 1.68124533\n",
      "Trained batch 413 batch loss 1.57995427 epoch total loss 1.68100011\n",
      "Trained batch 414 batch loss 1.57635272 epoch total loss 1.68074739\n",
      "Trained batch 415 batch loss 1.58265531 epoch total loss 1.680511\n",
      "Trained batch 416 batch loss 1.56108642 epoch total loss 1.68022394\n",
      "Trained batch 417 batch loss 1.48238337 epoch total loss 1.67974937\n",
      "Trained batch 418 batch loss 1.55488753 epoch total loss 1.67945063\n",
      "Trained batch 419 batch loss 1.53585231 epoch total loss 1.6791079\n",
      "Trained batch 420 batch loss 1.53215504 epoch total loss 1.67875803\n",
      "Trained batch 421 batch loss 1.51270604 epoch total loss 1.67836356\n",
      "Trained batch 422 batch loss 1.48026943 epoch total loss 1.67789423\n",
      "Trained batch 423 batch loss 1.51615334 epoch total loss 1.67751193\n",
      "Trained batch 424 batch loss 1.66897523 epoch total loss 1.67749166\n",
      "Trained batch 425 batch loss 1.56416214 epoch total loss 1.67722499\n",
      "Trained batch 426 batch loss 1.59074438 epoch total loss 1.67702198\n",
      "Trained batch 427 batch loss 1.55919921 epoch total loss 1.67674613\n",
      "Trained batch 428 batch loss 1.49076796 epoch total loss 1.67631161\n",
      "Trained batch 429 batch loss 1.54705548 epoch total loss 1.67601025\n",
      "Trained batch 430 batch loss 1.54125237 epoch total loss 1.67569697\n",
      "Trained batch 431 batch loss 1.51726806 epoch total loss 1.67532933\n",
      "Trained batch 432 batch loss 1.63716877 epoch total loss 1.67524099\n",
      "Trained batch 433 batch loss 1.5478791 epoch total loss 1.67494678\n",
      "Trained batch 434 batch loss 1.56175852 epoch total loss 1.67468596\n",
      "Trained batch 435 batch loss 1.53435552 epoch total loss 1.67436337\n",
      "Trained batch 436 batch loss 1.60406244 epoch total loss 1.6742022\n",
      "Trained batch 437 batch loss 1.66168177 epoch total loss 1.67417347\n",
      "Trained batch 438 batch loss 1.76685607 epoch total loss 1.67438507\n",
      "Trained batch 439 batch loss 1.75012827 epoch total loss 1.67455769\n",
      "Trained batch 440 batch loss 1.67031574 epoch total loss 1.67454791\n",
      "Trained batch 441 batch loss 1.6937294 epoch total loss 1.67459142\n",
      "Trained batch 442 batch loss 1.60122609 epoch total loss 1.67442536\n",
      "Trained batch 443 batch loss 1.37748957 epoch total loss 1.67375505\n",
      "Trained batch 444 batch loss 1.54663932 epoch total loss 1.67346883\n",
      "Trained batch 445 batch loss 1.62693858 epoch total loss 1.67336428\n",
      "Trained batch 446 batch loss 1.64800262 epoch total loss 1.67330742\n",
      "Trained batch 447 batch loss 1.60400951 epoch total loss 1.67315233\n",
      "Trained batch 448 batch loss 1.49346566 epoch total loss 1.67275131\n",
      "Trained batch 449 batch loss 1.55300891 epoch total loss 1.67248452\n",
      "Trained batch 450 batch loss 1.52942133 epoch total loss 1.67216659\n",
      "Trained batch 451 batch loss 1.46411681 epoch total loss 1.67170525\n",
      "Trained batch 452 batch loss 1.61028481 epoch total loss 1.67156947\n",
      "Trained batch 453 batch loss 1.52538979 epoch total loss 1.67124677\n",
      "Trained batch 454 batch loss 1.56322217 epoch total loss 1.67100883\n",
      "Trained batch 455 batch loss 1.60381925 epoch total loss 1.67086112\n",
      "Trained batch 456 batch loss 1.64430583 epoch total loss 1.67080283\n",
      "Trained batch 457 batch loss 1.60229635 epoch total loss 1.67065299\n",
      "Trained batch 458 batch loss 1.42722142 epoch total loss 1.67012155\n",
      "Trained batch 459 batch loss 1.38597798 epoch total loss 1.6695025\n",
      "Trained batch 460 batch loss 1.46208537 epoch total loss 1.66905165\n",
      "Trained batch 461 batch loss 1.63207901 epoch total loss 1.66897142\n",
      "Trained batch 462 batch loss 1.59633744 epoch total loss 1.66881418\n",
      "Trained batch 463 batch loss 1.57284975 epoch total loss 1.66860688\n",
      "Trained batch 464 batch loss 1.57591844 epoch total loss 1.6684072\n",
      "Trained batch 465 batch loss 1.59476113 epoch total loss 1.66824889\n",
      "Trained batch 466 batch loss 1.6111306 epoch total loss 1.66812634\n",
      "Trained batch 467 batch loss 1.48977399 epoch total loss 1.6677444\n",
      "Trained batch 468 batch loss 1.45390499 epoch total loss 1.66728747\n",
      "Trained batch 469 batch loss 1.56411457 epoch total loss 1.66706741\n",
      "Trained batch 470 batch loss 1.55570972 epoch total loss 1.66683054\n",
      "Trained batch 471 batch loss 1.56431365 epoch total loss 1.66661286\n",
      "Trained batch 472 batch loss 1.60843217 epoch total loss 1.66648972\n",
      "Trained batch 473 batch loss 1.59232366 epoch total loss 1.66633296\n",
      "Trained batch 474 batch loss 1.61947227 epoch total loss 1.66623402\n",
      "Trained batch 475 batch loss 1.56836939 epoch total loss 1.6660279\n",
      "Trained batch 476 batch loss 1.6179167 epoch total loss 1.66592693\n",
      "Trained batch 477 batch loss 1.60510683 epoch total loss 1.66579938\n",
      "Trained batch 478 batch loss 1.47136164 epoch total loss 1.66539264\n",
      "Trained batch 479 batch loss 1.47889805 epoch total loss 1.6650033\n",
      "Trained batch 480 batch loss 1.63346887 epoch total loss 1.66493762\n",
      "Trained batch 481 batch loss 1.59899569 epoch total loss 1.66480052\n",
      "Trained batch 482 batch loss 1.57324815 epoch total loss 1.66461051\n",
      "Trained batch 483 batch loss 1.44596481 epoch total loss 1.66415787\n",
      "Trained batch 484 batch loss 1.42874742 epoch total loss 1.66367161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 485 batch loss 1.59630501 epoch total loss 1.66353273\n",
      "Trained batch 486 batch loss 1.52652144 epoch total loss 1.6632508\n",
      "Trained batch 487 batch loss 1.51476431 epoch total loss 1.66294599\n",
      "Trained batch 488 batch loss 1.48578072 epoch total loss 1.66258287\n",
      "Trained batch 489 batch loss 1.45638323 epoch total loss 1.66216123\n",
      "Trained batch 490 batch loss 1.40189934 epoch total loss 1.66163\n",
      "Trained batch 491 batch loss 1.41369784 epoch total loss 1.66112506\n",
      "Trained batch 492 batch loss 1.46998358 epoch total loss 1.66073656\n",
      "Trained batch 493 batch loss 1.54830575 epoch total loss 1.66050851\n",
      "Trained batch 494 batch loss 1.52540672 epoch total loss 1.66023493\n",
      "Trained batch 495 batch loss 1.4879899 epoch total loss 1.65988696\n",
      "Trained batch 496 batch loss 1.59272587 epoch total loss 1.65975153\n",
      "Trained batch 497 batch loss 1.602072 epoch total loss 1.65963542\n",
      "Trained batch 498 batch loss 1.57511711 epoch total loss 1.65946579\n",
      "Trained batch 499 batch loss 1.54070973 epoch total loss 1.65922773\n",
      "Trained batch 500 batch loss 1.57024729 epoch total loss 1.65904975\n",
      "Trained batch 501 batch loss 1.61503243 epoch total loss 1.65896201\n",
      "Trained batch 502 batch loss 1.56815195 epoch total loss 1.65878117\n",
      "Trained batch 503 batch loss 1.55612278 epoch total loss 1.65857708\n",
      "Trained batch 504 batch loss 1.50590694 epoch total loss 1.65827417\n",
      "Trained batch 505 batch loss 1.59192026 epoch total loss 1.65814281\n",
      "Trained batch 506 batch loss 1.56473649 epoch total loss 1.65795827\n",
      "Trained batch 507 batch loss 1.50603116 epoch total loss 1.65765858\n",
      "Trained batch 508 batch loss 1.55793023 epoch total loss 1.65746224\n",
      "Trained batch 509 batch loss 1.53220022 epoch total loss 1.65721619\n",
      "Trained batch 510 batch loss 1.65124166 epoch total loss 1.65720451\n",
      "Trained batch 511 batch loss 1.58086967 epoch total loss 1.65705514\n",
      "Trained batch 512 batch loss 1.59472 epoch total loss 1.65693343\n",
      "Trained batch 513 batch loss 1.67004108 epoch total loss 1.65695894\n",
      "Trained batch 514 batch loss 1.56153011 epoch total loss 1.65677333\n",
      "Trained batch 515 batch loss 1.57591283 epoch total loss 1.65661633\n",
      "Trained batch 516 batch loss 1.57013845 epoch total loss 1.65644872\n",
      "Trained batch 517 batch loss 1.52044988 epoch total loss 1.65618563\n",
      "Trained batch 518 batch loss 1.49639297 epoch total loss 1.65587723\n",
      "Trained batch 519 batch loss 1.53001821 epoch total loss 1.65563476\n",
      "Trained batch 520 batch loss 1.50825226 epoch total loss 1.65535128\n",
      "Trained batch 521 batch loss 1.50734901 epoch total loss 1.65506709\n",
      "Trained batch 522 batch loss 1.59445322 epoch total loss 1.6549511\n",
      "Trained batch 523 batch loss 1.62300646 epoch total loss 1.65489\n",
      "Trained batch 524 batch loss 1.60172141 epoch total loss 1.65478849\n",
      "Trained batch 525 batch loss 1.4687165 epoch total loss 1.65443408\n",
      "Trained batch 526 batch loss 1.3441329 epoch total loss 1.65384412\n",
      "Trained batch 527 batch loss 1.44830453 epoch total loss 1.65345407\n",
      "Trained batch 528 batch loss 1.61728883 epoch total loss 1.65338564\n",
      "Trained batch 529 batch loss 1.63122821 epoch total loss 1.65334368\n",
      "Trained batch 530 batch loss 1.6035881 epoch total loss 1.65324986\n",
      "Trained batch 531 batch loss 1.62794161 epoch total loss 1.65320218\n",
      "Trained batch 532 batch loss 1.73058033 epoch total loss 1.65334761\n",
      "Trained batch 533 batch loss 1.62780046 epoch total loss 1.65329969\n",
      "Trained batch 534 batch loss 1.61178899 epoch total loss 1.65322196\n",
      "Trained batch 535 batch loss 1.63748145 epoch total loss 1.65319252\n",
      "Trained batch 536 batch loss 1.56807041 epoch total loss 1.65303373\n",
      "Trained batch 537 batch loss 1.5329175 epoch total loss 1.65281\n",
      "Trained batch 538 batch loss 1.64133716 epoch total loss 1.65278864\n",
      "Trained batch 539 batch loss 1.59658957 epoch total loss 1.65268445\n",
      "Trained batch 540 batch loss 1.60044789 epoch total loss 1.65258777\n",
      "Trained batch 541 batch loss 1.55071187 epoch total loss 1.65239954\n",
      "Trained batch 542 batch loss 1.58942032 epoch total loss 1.65228331\n",
      "Trained batch 543 batch loss 1.57280803 epoch total loss 1.65213692\n",
      "Trained batch 544 batch loss 1.5400387 epoch total loss 1.65193081\n",
      "Trained batch 545 batch loss 1.53444552 epoch total loss 1.65171528\n",
      "Trained batch 546 batch loss 1.45217288 epoch total loss 1.65134978\n",
      "Trained batch 547 batch loss 1.58481944 epoch total loss 1.65122819\n",
      "Trained batch 548 batch loss 1.75983024 epoch total loss 1.65142632\n",
      "Trained batch 549 batch loss 1.68989205 epoch total loss 1.65149641\n",
      "Trained batch 550 batch loss 1.68048596 epoch total loss 1.6515491\n",
      "Trained batch 551 batch loss 1.67648172 epoch total loss 1.65159428\n",
      "Trained batch 552 batch loss 1.55640948 epoch total loss 1.65142179\n",
      "Trained batch 553 batch loss 1.61674881 epoch total loss 1.65135908\n",
      "Trained batch 554 batch loss 1.61161256 epoch total loss 1.65128744\n",
      "Trained batch 555 batch loss 1.46608186 epoch total loss 1.65095365\n",
      "Trained batch 556 batch loss 1.55914819 epoch total loss 1.65078855\n",
      "Trained batch 557 batch loss 1.57721734 epoch total loss 1.65065646\n",
      "Trained batch 558 batch loss 1.65623128 epoch total loss 1.65066648\n",
      "Trained batch 559 batch loss 1.66100478 epoch total loss 1.65068495\n",
      "Trained batch 560 batch loss 1.60545754 epoch total loss 1.65060425\n",
      "Trained batch 561 batch loss 1.54005897 epoch total loss 1.6504072\n",
      "Trained batch 562 batch loss 1.56003916 epoch total loss 1.65024638\n",
      "Trained batch 563 batch loss 1.55252421 epoch total loss 1.65007293\n",
      "Trained batch 564 batch loss 1.61450648 epoch total loss 1.65000975\n",
      "Trained batch 565 batch loss 1.73180008 epoch total loss 1.65015459\n",
      "Trained batch 566 batch loss 1.6468215 epoch total loss 1.65014875\n",
      "Trained batch 567 batch loss 1.49127388 epoch total loss 1.64986849\n",
      "Trained batch 568 batch loss 1.54515219 epoch total loss 1.64968419\n",
      "Trained batch 569 batch loss 1.47065949 epoch total loss 1.6493696\n",
      "Trained batch 570 batch loss 1.48343909 epoch total loss 1.64907849\n",
      "Trained batch 571 batch loss 1.44719434 epoch total loss 1.64872491\n",
      "Trained batch 572 batch loss 1.51391244 epoch total loss 1.64848924\n",
      "Trained batch 573 batch loss 1.54886317 epoch total loss 1.64831543\n",
      "Trained batch 574 batch loss 1.61760402 epoch total loss 1.6482619\n",
      "Trained batch 575 batch loss 1.64684904 epoch total loss 1.64825952\n",
      "Trained batch 576 batch loss 1.64070559 epoch total loss 1.64824629\n",
      "Trained batch 577 batch loss 1.55314636 epoch total loss 1.64808154\n",
      "Trained batch 578 batch loss 1.54852784 epoch total loss 1.64790928\n",
      "Trained batch 579 batch loss 1.5531795 epoch total loss 1.64774561\n",
      "Trained batch 580 batch loss 1.60302496 epoch total loss 1.6476686\n",
      "Trained batch 581 batch loss 1.5373733 epoch total loss 1.6474787\n",
      "Trained batch 582 batch loss 1.50085795 epoch total loss 1.64722681\n",
      "Trained batch 583 batch loss 1.51203346 epoch total loss 1.64699483\n",
      "Trained batch 584 batch loss 1.45640361 epoch total loss 1.64666855\n",
      "Trained batch 585 batch loss 1.48620069 epoch total loss 1.64639425\n",
      "Trained batch 586 batch loss 1.60980165 epoch total loss 1.64633179\n",
      "Trained batch 587 batch loss 1.54100633 epoch total loss 1.64615238\n",
      "Trained batch 588 batch loss 1.45347464 epoch total loss 1.64582467\n",
      "Trained batch 589 batch loss 1.50860882 epoch total loss 1.64559174\n",
      "Trained batch 590 batch loss 1.57824612 epoch total loss 1.64547765\n",
      "Trained batch 591 batch loss 1.61702144 epoch total loss 1.64542937\n",
      "Trained batch 592 batch loss 1.54127824 epoch total loss 1.64525342\n",
      "Trained batch 593 batch loss 1.49519026 epoch total loss 1.64500034\n",
      "Trained batch 594 batch loss 1.56842566 epoch total loss 1.64487147\n",
      "Trained batch 595 batch loss 1.61515427 epoch total loss 1.64482152\n",
      "Trained batch 596 batch loss 1.65327084 epoch total loss 1.64483571\n",
      "Trained batch 597 batch loss 1.59134781 epoch total loss 1.64474618\n",
      "Trained batch 598 batch loss 1.50335073 epoch total loss 1.64450967\n",
      "Trained batch 599 batch loss 1.51615787 epoch total loss 1.64429545\n",
      "Trained batch 600 batch loss 1.50929952 epoch total loss 1.64407039\n",
      "Trained batch 601 batch loss 1.51768589 epoch total loss 1.64386022\n",
      "Trained batch 602 batch loss 1.58648992 epoch total loss 1.64376485\n",
      "Trained batch 603 batch loss 1.47218418 epoch total loss 1.6434803\n",
      "Trained batch 604 batch loss 1.55636168 epoch total loss 1.64333606\n",
      "Trained batch 605 batch loss 1.48905587 epoch total loss 1.64308107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 606 batch loss 1.5552063 epoch total loss 1.64293611\n",
      "Trained batch 607 batch loss 1.56420755 epoch total loss 1.64280641\n",
      "Trained batch 608 batch loss 1.57998586 epoch total loss 1.64270306\n",
      "Trained batch 609 batch loss 1.5313797 epoch total loss 1.64252019\n",
      "Trained batch 610 batch loss 1.50645947 epoch total loss 1.64229715\n",
      "Trained batch 611 batch loss 1.46862876 epoch total loss 1.64201295\n",
      "Trained batch 612 batch loss 1.53502893 epoch total loss 1.64183807\n",
      "Trained batch 613 batch loss 1.52036691 epoch total loss 1.64164\n",
      "Trained batch 614 batch loss 1.55177343 epoch total loss 1.64149356\n",
      "Trained batch 615 batch loss 1.48708653 epoch total loss 1.6412425\n",
      "Trained batch 616 batch loss 1.59176421 epoch total loss 1.64116216\n",
      "Trained batch 617 batch loss 1.59044981 epoch total loss 1.6410799\n",
      "Trained batch 618 batch loss 1.6211623 epoch total loss 1.64104772\n",
      "Trained batch 619 batch loss 1.61104345 epoch total loss 1.6409992\n",
      "Trained batch 620 batch loss 1.60921359 epoch total loss 1.64094794\n",
      "Trained batch 621 batch loss 1.62480175 epoch total loss 1.64092195\n",
      "Trained batch 622 batch loss 1.62926555 epoch total loss 1.64090323\n",
      "Trained batch 623 batch loss 1.55892944 epoch total loss 1.64077151\n",
      "Trained batch 624 batch loss 1.55385494 epoch total loss 1.64063227\n",
      "Trained batch 625 batch loss 1.66691756 epoch total loss 1.64067423\n",
      "Trained batch 626 batch loss 1.61691463 epoch total loss 1.64063632\n",
      "Trained batch 627 batch loss 1.68440104 epoch total loss 1.64070618\n",
      "Trained batch 628 batch loss 1.57087743 epoch total loss 1.64059508\n",
      "Trained batch 629 batch loss 1.52873468 epoch total loss 1.6404171\n",
      "Trained batch 630 batch loss 1.59206057 epoch total loss 1.64034033\n",
      "Trained batch 631 batch loss 1.51696801 epoch total loss 1.64014482\n",
      "Trained batch 632 batch loss 1.51951778 epoch total loss 1.63995397\n",
      "Trained batch 633 batch loss 1.59524107 epoch total loss 1.63988328\n",
      "Trained batch 634 batch loss 1.59820151 epoch total loss 1.63981748\n",
      "Trained batch 635 batch loss 1.47787356 epoch total loss 1.63956249\n",
      "Trained batch 636 batch loss 1.58409619 epoch total loss 1.63947535\n",
      "Trained batch 637 batch loss 1.42910433 epoch total loss 1.63914502\n",
      "Trained batch 638 batch loss 1.44840813 epoch total loss 1.63884604\n",
      "Trained batch 639 batch loss 1.53966165 epoch total loss 1.63869083\n",
      "Trained batch 640 batch loss 1.54138732 epoch total loss 1.63853872\n",
      "Trained batch 641 batch loss 1.53580165 epoch total loss 1.63837838\n",
      "Trained batch 642 batch loss 1.63035166 epoch total loss 1.63836598\n",
      "Trained batch 643 batch loss 1.66276026 epoch total loss 1.63840377\n",
      "Trained batch 644 batch loss 1.67746568 epoch total loss 1.63846445\n",
      "Trained batch 645 batch loss 1.66007864 epoch total loss 1.63849795\n",
      "Trained batch 646 batch loss 1.60209 epoch total loss 1.63844156\n",
      "Trained batch 647 batch loss 1.37623489 epoch total loss 1.63803625\n",
      "Trained batch 648 batch loss 1.38779175 epoch total loss 1.63765013\n",
      "Trained batch 649 batch loss 1.27853703 epoch total loss 1.63709676\n",
      "Trained batch 650 batch loss 1.2975719 epoch total loss 1.63657451\n",
      "Trained batch 651 batch loss 1.54010868 epoch total loss 1.63642645\n",
      "Trained batch 652 batch loss 1.53922033 epoch total loss 1.63627732\n",
      "Trained batch 653 batch loss 1.55100608 epoch total loss 1.63614666\n",
      "Trained batch 654 batch loss 1.60854423 epoch total loss 1.63610446\n",
      "Trained batch 655 batch loss 1.571383 epoch total loss 1.63600576\n",
      "Trained batch 656 batch loss 1.51136065 epoch total loss 1.63581574\n",
      "Trained batch 657 batch loss 1.58883262 epoch total loss 1.63574421\n",
      "Trained batch 658 batch loss 1.5342207 epoch total loss 1.63558984\n",
      "Trained batch 659 batch loss 1.58875597 epoch total loss 1.63551879\n",
      "Trained batch 660 batch loss 1.58809412 epoch total loss 1.63544703\n",
      "Trained batch 661 batch loss 1.59910357 epoch total loss 1.63539207\n",
      "Trained batch 662 batch loss 1.5825572 epoch total loss 1.6353122\n",
      "Trained batch 663 batch loss 1.56628692 epoch total loss 1.63520801\n",
      "Trained batch 664 batch loss 1.5761174 epoch total loss 1.6351192\n",
      "Trained batch 665 batch loss 1.59653139 epoch total loss 1.63506114\n",
      "Trained batch 666 batch loss 1.5347321 epoch total loss 1.63491058\n",
      "Trained batch 667 batch loss 1.50547719 epoch total loss 1.63471663\n",
      "Trained batch 668 batch loss 1.55453265 epoch total loss 1.63459659\n",
      "Trained batch 669 batch loss 1.522475 epoch total loss 1.63442898\n",
      "Trained batch 670 batch loss 1.512918 epoch total loss 1.63424766\n",
      "Trained batch 671 batch loss 1.43895829 epoch total loss 1.63395667\n",
      "Trained batch 672 batch loss 1.37601852 epoch total loss 1.6335727\n",
      "Trained batch 673 batch loss 1.44752979 epoch total loss 1.63329625\n",
      "Trained batch 674 batch loss 1.52736115 epoch total loss 1.63313901\n",
      "Trained batch 675 batch loss 1.57907 epoch total loss 1.63305902\n",
      "Trained batch 676 batch loss 1.57947469 epoch total loss 1.63297975\n",
      "Trained batch 677 batch loss 1.60389137 epoch total loss 1.63293672\n",
      "Trained batch 678 batch loss 1.53732038 epoch total loss 1.63279581\n",
      "Trained batch 679 batch loss 1.48041415 epoch total loss 1.63257146\n",
      "Trained batch 680 batch loss 1.54111338 epoch total loss 1.63243699\n",
      "Trained batch 681 batch loss 1.53003621 epoch total loss 1.63228655\n",
      "Trained batch 682 batch loss 1.59569752 epoch total loss 1.6322329\n",
      "Trained batch 683 batch loss 1.57521796 epoch total loss 1.63214946\n",
      "Trained batch 684 batch loss 1.44798183 epoch total loss 1.63188016\n",
      "Trained batch 685 batch loss 1.50115514 epoch total loss 1.63168931\n",
      "Trained batch 686 batch loss 1.52674031 epoch total loss 1.63153625\n",
      "Trained batch 687 batch loss 1.59905434 epoch total loss 1.63148892\n",
      "Trained batch 688 batch loss 1.53071332 epoch total loss 1.63134253\n",
      "Trained batch 689 batch loss 1.53076756 epoch total loss 1.6311965\n",
      "Trained batch 690 batch loss 1.61417747 epoch total loss 1.63117182\n",
      "Trained batch 691 batch loss 1.5915221 epoch total loss 1.63111448\n",
      "Trained batch 692 batch loss 1.6372143 epoch total loss 1.6311233\n",
      "Trained batch 693 batch loss 1.50919652 epoch total loss 1.63094723\n",
      "Trained batch 694 batch loss 1.57666349 epoch total loss 1.63086903\n",
      "Trained batch 695 batch loss 1.5047915 epoch total loss 1.63068759\n",
      "Trained batch 696 batch loss 1.52985835 epoch total loss 1.63054276\n",
      "Trained batch 697 batch loss 1.47133052 epoch total loss 1.63031435\n",
      "Trained batch 698 batch loss 1.47428644 epoch total loss 1.63009071\n",
      "Trained batch 699 batch loss 1.51191485 epoch total loss 1.62992179\n",
      "Trained batch 700 batch loss 1.51863158 epoch total loss 1.62976289\n",
      "Trained batch 701 batch loss 1.54470372 epoch total loss 1.62964141\n",
      "Trained batch 702 batch loss 1.41389573 epoch total loss 1.62933421\n",
      "Trained batch 703 batch loss 1.51881206 epoch total loss 1.62917697\n",
      "Trained batch 704 batch loss 1.47737253 epoch total loss 1.62896144\n",
      "Trained batch 705 batch loss 1.52625132 epoch total loss 1.62881565\n",
      "Trained batch 706 batch loss 1.46702 epoch total loss 1.62858653\n",
      "Trained batch 707 batch loss 1.50592613 epoch total loss 1.62841308\n",
      "Trained batch 708 batch loss 1.45805442 epoch total loss 1.6281724\n",
      "Trained batch 709 batch loss 1.43585229 epoch total loss 1.62790132\n",
      "Trained batch 710 batch loss 1.47210717 epoch total loss 1.62768197\n",
      "Trained batch 711 batch loss 1.60824788 epoch total loss 1.62765467\n",
      "Trained batch 712 batch loss 1.67547452 epoch total loss 1.62772167\n",
      "Trained batch 713 batch loss 1.55125499 epoch total loss 1.6276145\n",
      "Trained batch 714 batch loss 1.49054325 epoch total loss 1.62742257\n",
      "Trained batch 715 batch loss 1.39817286 epoch total loss 1.62710202\n",
      "Trained batch 716 batch loss 1.4621501 epoch total loss 1.62687159\n",
      "Trained batch 717 batch loss 1.54837132 epoch total loss 1.62676215\n",
      "Trained batch 718 batch loss 1.55237067 epoch total loss 1.62665844\n",
      "Trained batch 719 batch loss 1.57211208 epoch total loss 1.62658262\n",
      "Trained batch 720 batch loss 1.56695366 epoch total loss 1.62649977\n",
      "Trained batch 721 batch loss 1.49501038 epoch total loss 1.62631738\n",
      "Trained batch 722 batch loss 1.5441004 epoch total loss 1.62620342\n",
      "Trained batch 723 batch loss 1.58622551 epoch total loss 1.6261481\n",
      "Trained batch 724 batch loss 1.50909483 epoch total loss 1.62598646\n",
      "Trained batch 725 batch loss 1.55550396 epoch total loss 1.6258893\n",
      "Trained batch 726 batch loss 1.56299865 epoch total loss 1.62580276\n",
      "Trained batch 727 batch loss 1.63142908 epoch total loss 1.6258105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 728 batch loss 1.45210767 epoch total loss 1.62557197\n",
      "Trained batch 729 batch loss 1.56792843 epoch total loss 1.62549281\n",
      "Trained batch 730 batch loss 1.52475107 epoch total loss 1.62535489\n",
      "Trained batch 731 batch loss 1.51003015 epoch total loss 1.62519705\n",
      "Trained batch 732 batch loss 1.6898793 epoch total loss 1.62528539\n",
      "Trained batch 733 batch loss 1.57832289 epoch total loss 1.62522137\n",
      "Trained batch 734 batch loss 1.54979777 epoch total loss 1.62511861\n",
      "Trained batch 735 batch loss 1.51255417 epoch total loss 1.62496543\n",
      "Trained batch 736 batch loss 1.56668413 epoch total loss 1.62488627\n",
      "Trained batch 737 batch loss 1.5907656 epoch total loss 1.62484\n",
      "Trained batch 738 batch loss 1.54618502 epoch total loss 1.62473333\n",
      "Trained batch 739 batch loss 1.40800953 epoch total loss 1.62444007\n",
      "Trained batch 740 batch loss 1.51080644 epoch total loss 1.62428653\n",
      "Trained batch 741 batch loss 1.4386059 epoch total loss 1.62403595\n",
      "Trained batch 742 batch loss 1.56900597 epoch total loss 1.62396169\n",
      "Trained batch 743 batch loss 1.49217653 epoch total loss 1.62378442\n",
      "Trained batch 744 batch loss 1.54517448 epoch total loss 1.62367868\n",
      "Trained batch 745 batch loss 1.54832363 epoch total loss 1.62357759\n",
      "Trained batch 746 batch loss 1.63012922 epoch total loss 1.62358642\n",
      "Trained batch 747 batch loss 1.54249644 epoch total loss 1.62347782\n",
      "Trained batch 748 batch loss 1.51062393 epoch total loss 1.6233269\n",
      "Trained batch 749 batch loss 1.50457537 epoch total loss 1.62316835\n",
      "Trained batch 750 batch loss 1.4004705 epoch total loss 1.6228714\n",
      "Trained batch 751 batch loss 1.40019131 epoch total loss 1.62257481\n",
      "Trained batch 752 batch loss 1.44737184 epoch total loss 1.62234187\n",
      "Trained batch 753 batch loss 1.58565104 epoch total loss 1.62229323\n",
      "Trained batch 754 batch loss 1.49980104 epoch total loss 1.62213075\n",
      "Trained batch 755 batch loss 1.49471 epoch total loss 1.62196195\n",
      "Trained batch 756 batch loss 1.51676142 epoch total loss 1.62182283\n",
      "Trained batch 757 batch loss 1.43087947 epoch total loss 1.62157059\n",
      "Trained batch 758 batch loss 1.46595597 epoch total loss 1.62136531\n",
      "Trained batch 759 batch loss 1.58845711 epoch total loss 1.62132192\n",
      "Trained batch 760 batch loss 1.47785699 epoch total loss 1.62113321\n",
      "Trained batch 761 batch loss 1.61647975 epoch total loss 1.62112713\n",
      "Trained batch 762 batch loss 1.59538364 epoch total loss 1.62109327\n",
      "Trained batch 763 batch loss 1.55157864 epoch total loss 1.6210022\n",
      "Trained batch 764 batch loss 1.50601971 epoch total loss 1.62085164\n",
      "Trained batch 765 batch loss 1.53305364 epoch total loss 1.62073696\n",
      "Trained batch 766 batch loss 1.53111219 epoch total loss 1.62062\n",
      "Trained batch 767 batch loss 1.50333714 epoch total loss 1.62046695\n",
      "Trained batch 768 batch loss 1.46734881 epoch total loss 1.62026775\n",
      "Trained batch 769 batch loss 1.49222589 epoch total loss 1.62010121\n",
      "Trained batch 770 batch loss 1.49020243 epoch total loss 1.61993253\n",
      "Trained batch 771 batch loss 1.49152541 epoch total loss 1.619766\n",
      "Trained batch 772 batch loss 1.4608407 epoch total loss 1.61956012\n",
      "Trained batch 773 batch loss 1.47920573 epoch total loss 1.61937857\n",
      "Trained batch 774 batch loss 1.49274802 epoch total loss 1.61921501\n",
      "Trained batch 775 batch loss 1.45560157 epoch total loss 1.61900389\n",
      "Trained batch 776 batch loss 1.4225409 epoch total loss 1.61875069\n",
      "Trained batch 777 batch loss 1.48210716 epoch total loss 1.61857474\n",
      "Trained batch 778 batch loss 1.4200654 epoch total loss 1.61831951\n",
      "Trained batch 779 batch loss 1.50870311 epoch total loss 1.61817884\n",
      "Trained batch 780 batch loss 1.49192703 epoch total loss 1.61801696\n",
      "Trained batch 781 batch loss 1.53953493 epoch total loss 1.61791646\n",
      "Trained batch 782 batch loss 1.50102448 epoch total loss 1.61776698\n",
      "Trained batch 783 batch loss 1.49035239 epoch total loss 1.61760426\n",
      "Trained batch 784 batch loss 1.57076597 epoch total loss 1.61754453\n",
      "Trained batch 785 batch loss 1.62890184 epoch total loss 1.61755896\n",
      "Trained batch 786 batch loss 1.66414821 epoch total loss 1.61761832\n",
      "Trained batch 787 batch loss 1.68104982 epoch total loss 1.61769891\n",
      "Trained batch 788 batch loss 1.58747804 epoch total loss 1.61766064\n",
      "Trained batch 789 batch loss 1.40876114 epoch total loss 1.61739588\n",
      "Trained batch 790 batch loss 1.44508612 epoch total loss 1.61717772\n",
      "Trained batch 791 batch loss 1.50681221 epoch total loss 1.61703825\n",
      "Trained batch 792 batch loss 1.50466228 epoch total loss 1.61689639\n",
      "Trained batch 793 batch loss 1.52216399 epoch total loss 1.61677694\n",
      "Trained batch 794 batch loss 1.51409435 epoch total loss 1.6166476\n",
      "Trained batch 795 batch loss 1.47887158 epoch total loss 1.61647427\n",
      "Trained batch 796 batch loss 1.52413177 epoch total loss 1.61635828\n",
      "Trained batch 797 batch loss 1.49141645 epoch total loss 1.61620164\n",
      "Trained batch 798 batch loss 1.50638747 epoch total loss 1.61606395\n",
      "Trained batch 799 batch loss 1.54832327 epoch total loss 1.61597919\n",
      "Trained batch 800 batch loss 1.5162828 epoch total loss 1.6158545\n",
      "Trained batch 801 batch loss 1.45651281 epoch total loss 1.61565566\n",
      "Trained batch 802 batch loss 1.44668913 epoch total loss 1.6154449\n",
      "Trained batch 803 batch loss 1.44875097 epoch total loss 1.61523724\n",
      "Trained batch 804 batch loss 1.46924829 epoch total loss 1.61505568\n",
      "Trained batch 805 batch loss 1.46222973 epoch total loss 1.6148659\n",
      "Trained batch 806 batch loss 1.45997941 epoch total loss 1.61467373\n",
      "Trained batch 807 batch loss 1.5594275 epoch total loss 1.61460531\n",
      "Trained batch 808 batch loss 1.54412103 epoch total loss 1.61451793\n",
      "Trained batch 809 batch loss 1.53355491 epoch total loss 1.61441791\n",
      "Trained batch 810 batch loss 1.53032506 epoch total loss 1.61431408\n",
      "Trained batch 811 batch loss 1.37967145 epoch total loss 1.61402464\n",
      "Trained batch 812 batch loss 1.3856138 epoch total loss 1.61374342\n",
      "Trained batch 813 batch loss 1.51411557 epoch total loss 1.61362088\n",
      "Trained batch 814 batch loss 1.48711264 epoch total loss 1.61346543\n",
      "Trained batch 815 batch loss 1.51316905 epoch total loss 1.6133424\n",
      "Trained batch 816 batch loss 1.62174964 epoch total loss 1.61335266\n",
      "Trained batch 817 batch loss 1.49252391 epoch total loss 1.61320472\n",
      "Trained batch 818 batch loss 1.51186514 epoch total loss 1.61308086\n",
      "Trained batch 819 batch loss 1.5442946 epoch total loss 1.61299682\n",
      "Trained batch 820 batch loss 1.51384902 epoch total loss 1.61287594\n",
      "Trained batch 821 batch loss 1.43732464 epoch total loss 1.61266208\n",
      "Trained batch 822 batch loss 1.46756828 epoch total loss 1.61248553\n",
      "Trained batch 823 batch loss 1.5009985 epoch total loss 1.61235011\n",
      "Trained batch 824 batch loss 1.61722434 epoch total loss 1.61235595\n",
      "Trained batch 825 batch loss 1.50592721 epoch total loss 1.61222696\n",
      "Trained batch 826 batch loss 1.59814155 epoch total loss 1.61220992\n",
      "Trained batch 827 batch loss 1.70296597 epoch total loss 1.61231971\n",
      "Trained batch 828 batch loss 1.69490969 epoch total loss 1.61241949\n",
      "Trained batch 829 batch loss 1.70760381 epoch total loss 1.6125344\n",
      "Trained batch 830 batch loss 1.58230472 epoch total loss 1.61249793\n",
      "Trained batch 831 batch loss 1.53329587 epoch total loss 1.61240268\n",
      "Trained batch 832 batch loss 1.4850105 epoch total loss 1.61224949\n",
      "Trained batch 833 batch loss 1.54298019 epoch total loss 1.6121664\n",
      "Trained batch 834 batch loss 1.55672681 epoch total loss 1.61209989\n",
      "Trained batch 835 batch loss 1.54114866 epoch total loss 1.61201489\n",
      "Trained batch 836 batch loss 1.55113196 epoch total loss 1.61194217\n",
      "Trained batch 837 batch loss 1.49699771 epoch total loss 1.61180472\n",
      "Trained batch 838 batch loss 1.44709444 epoch total loss 1.61160827\n",
      "Trained batch 839 batch loss 1.51034224 epoch total loss 1.61148763\n",
      "Trained batch 840 batch loss 1.65635693 epoch total loss 1.61154103\n",
      "Trained batch 841 batch loss 1.67303848 epoch total loss 1.61161423\n",
      "Trained batch 842 batch loss 1.64531422 epoch total loss 1.61165416\n",
      "Trained batch 843 batch loss 1.62980676 epoch total loss 1.61167562\n",
      "Trained batch 844 batch loss 1.51766348 epoch total loss 1.61156428\n",
      "Trained batch 845 batch loss 1.68834603 epoch total loss 1.61165524\n",
      "Trained batch 846 batch loss 1.41895604 epoch total loss 1.61142743\n",
      "Trained batch 847 batch loss 1.32672477 epoch total loss 1.61109138\n",
      "Trained batch 848 batch loss 1.45805097 epoch total loss 1.61091077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 849 batch loss 1.36665988 epoch total loss 1.61062312\n",
      "Trained batch 850 batch loss 1.45548391 epoch total loss 1.61044061\n",
      "Trained batch 851 batch loss 1.53883052 epoch total loss 1.61035645\n",
      "Trained batch 852 batch loss 1.48736107 epoch total loss 1.61021197\n",
      "Trained batch 853 batch loss 1.48927522 epoch total loss 1.61007023\n",
      "Trained batch 854 batch loss 1.56749368 epoch total loss 1.6100204\n",
      "Trained batch 855 batch loss 1.47117233 epoch total loss 1.60985804\n",
      "Trained batch 856 batch loss 1.49173844 epoch total loss 1.60972\n",
      "Trained batch 857 batch loss 1.47690403 epoch total loss 1.60956502\n",
      "Trained batch 858 batch loss 1.42913151 epoch total loss 1.60935462\n",
      "Trained batch 859 batch loss 1.48900819 epoch total loss 1.60921454\n",
      "Trained batch 860 batch loss 1.43773699 epoch total loss 1.60901523\n",
      "Trained batch 861 batch loss 1.44542861 epoch total loss 1.60882521\n",
      "Trained batch 862 batch loss 1.54252195 epoch total loss 1.6087482\n",
      "Trained batch 863 batch loss 1.70065868 epoch total loss 1.60885477\n",
      "Trained batch 864 batch loss 1.51251769 epoch total loss 1.60874331\n",
      "Trained batch 865 batch loss 1.48583686 epoch total loss 1.60860121\n",
      "Trained batch 866 batch loss 1.40466201 epoch total loss 1.60836577\n",
      "Trained batch 867 batch loss 1.33933854 epoch total loss 1.60805547\n",
      "Trained batch 868 batch loss 1.4876616 epoch total loss 1.60791671\n",
      "Trained batch 869 batch loss 1.51612973 epoch total loss 1.60781109\n",
      "Trained batch 870 batch loss 1.43203044 epoch total loss 1.60760903\n",
      "Trained batch 871 batch loss 1.51406431 epoch total loss 1.60750163\n",
      "Trained batch 872 batch loss 1.46589684 epoch total loss 1.60733926\n",
      "Trained batch 873 batch loss 1.47623253 epoch total loss 1.60718906\n",
      "Trained batch 874 batch loss 1.56142235 epoch total loss 1.60713673\n",
      "Trained batch 875 batch loss 1.40534532 epoch total loss 1.60690606\n",
      "Trained batch 876 batch loss 1.31877041 epoch total loss 1.60657716\n",
      "Trained batch 877 batch loss 1.24990904 epoch total loss 1.60617042\n",
      "Trained batch 878 batch loss 1.27617073 epoch total loss 1.60579455\n",
      "Trained batch 879 batch loss 1.44192457 epoch total loss 1.60560799\n",
      "Trained batch 880 batch loss 1.57089663 epoch total loss 1.60556865\n",
      "Trained batch 881 batch loss 1.58001184 epoch total loss 1.60553956\n",
      "Trained batch 882 batch loss 1.3967675 epoch total loss 1.60530281\n",
      "Trained batch 883 batch loss 1.60151589 epoch total loss 1.60529852\n",
      "Trained batch 884 batch loss 1.43216991 epoch total loss 1.60510266\n",
      "Trained batch 885 batch loss 1.34562719 epoch total loss 1.6048094\n",
      "Trained batch 886 batch loss 1.54281116 epoch total loss 1.60473955\n",
      "Trained batch 887 batch loss 1.43878603 epoch total loss 1.60455251\n",
      "Trained batch 888 batch loss 1.41827905 epoch total loss 1.60434282\n",
      "Trained batch 889 batch loss 1.49302876 epoch total loss 1.60421753\n",
      "Trained batch 890 batch loss 1.42623019 epoch total loss 1.60401762\n",
      "Trained batch 891 batch loss 1.48572 epoch total loss 1.60388482\n",
      "Trained batch 892 batch loss 1.44743586 epoch total loss 1.60370946\n",
      "Trained batch 893 batch loss 1.49130261 epoch total loss 1.60358357\n",
      "Trained batch 894 batch loss 1.61209822 epoch total loss 1.60359299\n",
      "Trained batch 895 batch loss 1.68425143 epoch total loss 1.60368311\n",
      "Trained batch 896 batch loss 1.48219156 epoch total loss 1.60354745\n",
      "Trained batch 897 batch loss 1.45958316 epoch total loss 1.603387\n",
      "Trained batch 898 batch loss 1.5266788 epoch total loss 1.60330164\n",
      "Trained batch 899 batch loss 1.40561771 epoch total loss 1.60308182\n",
      "Trained batch 900 batch loss 1.51246035 epoch total loss 1.60298109\n",
      "Trained batch 901 batch loss 1.53390574 epoch total loss 1.60290444\n",
      "Trained batch 902 batch loss 1.55917025 epoch total loss 1.60285604\n",
      "Trained batch 903 batch loss 1.61403489 epoch total loss 1.60286832\n",
      "Trained batch 904 batch loss 1.65322268 epoch total loss 1.60292399\n",
      "Trained batch 905 batch loss 1.62603879 epoch total loss 1.60294962\n",
      "Trained batch 906 batch loss 1.65869093 epoch total loss 1.60301113\n",
      "Trained batch 907 batch loss 1.59481597 epoch total loss 1.60300219\n",
      "Trained batch 908 batch loss 1.59158278 epoch total loss 1.60298955\n",
      "Trained batch 909 batch loss 1.56159651 epoch total loss 1.60294414\n",
      "Trained batch 910 batch loss 1.54818654 epoch total loss 1.60288393\n",
      "Trained batch 911 batch loss 1.55625939 epoch total loss 1.60283279\n",
      "Trained batch 912 batch loss 1.62746143 epoch total loss 1.60285974\n",
      "Trained batch 913 batch loss 1.4371264 epoch total loss 1.6026783\n",
      "Trained batch 914 batch loss 1.52952182 epoch total loss 1.60259819\n",
      "Trained batch 915 batch loss 1.49644744 epoch total loss 1.6024822\n",
      "Trained batch 916 batch loss 1.42785454 epoch total loss 1.60229158\n",
      "Trained batch 917 batch loss 1.49215209 epoch total loss 1.60217154\n",
      "Trained batch 918 batch loss 1.3300879 epoch total loss 1.60187507\n",
      "Trained batch 919 batch loss 1.33442318 epoch total loss 1.6015842\n",
      "Trained batch 920 batch loss 1.30956984 epoch total loss 1.60126674\n",
      "Trained batch 921 batch loss 1.36667895 epoch total loss 1.60101211\n",
      "Trained batch 922 batch loss 1.40906763 epoch total loss 1.60080385\n",
      "Trained batch 923 batch loss 1.31017292 epoch total loss 1.60048902\n",
      "Trained batch 924 batch loss 1.44510353 epoch total loss 1.60032082\n",
      "Trained batch 925 batch loss 1.42155719 epoch total loss 1.60012746\n",
      "Trained batch 926 batch loss 1.52035785 epoch total loss 1.60004139\n",
      "Trained batch 927 batch loss 1.40004718 epoch total loss 1.59982562\n",
      "Trained batch 928 batch loss 1.38025856 epoch total loss 1.59958899\n",
      "Trained batch 929 batch loss 1.4176929 epoch total loss 1.59939325\n",
      "Trained batch 930 batch loss 1.56288636 epoch total loss 1.59935391\n",
      "Trained batch 931 batch loss 1.53815985 epoch total loss 1.59928823\n",
      "Trained batch 932 batch loss 1.58282161 epoch total loss 1.59927058\n",
      "Trained batch 933 batch loss 1.32947707 epoch total loss 1.59898138\n",
      "Trained batch 934 batch loss 1.60635161 epoch total loss 1.59898925\n",
      "Trained batch 935 batch loss 1.56217563 epoch total loss 1.59894979\n",
      "Trained batch 936 batch loss 1.52125919 epoch total loss 1.59886682\n",
      "Trained batch 937 batch loss 1.59659052 epoch total loss 1.59886432\n",
      "Trained batch 938 batch loss 1.53393757 epoch total loss 1.59879506\n",
      "Trained batch 939 batch loss 1.51955152 epoch total loss 1.59871066\n",
      "Trained batch 940 batch loss 1.40834105 epoch total loss 1.59850812\n",
      "Trained batch 941 batch loss 1.5752753 epoch total loss 1.59848344\n",
      "Trained batch 942 batch loss 1.46228027 epoch total loss 1.59833896\n",
      "Trained batch 943 batch loss 1.50952291 epoch total loss 1.59824467\n",
      "Trained batch 944 batch loss 1.54885864 epoch total loss 1.59819233\n",
      "Trained batch 945 batch loss 1.4681958 epoch total loss 1.59805477\n",
      "Trained batch 946 batch loss 1.39429903 epoch total loss 1.59783936\n",
      "Trained batch 947 batch loss 1.42399704 epoch total loss 1.59765577\n",
      "Trained batch 948 batch loss 1.39576507 epoch total loss 1.59744275\n",
      "Trained batch 949 batch loss 1.27528322 epoch total loss 1.59710324\n",
      "Trained batch 950 batch loss 1.43644881 epoch total loss 1.59693408\n",
      "Trained batch 951 batch loss 1.50276792 epoch total loss 1.59683514\n",
      "Trained batch 952 batch loss 1.56606293 epoch total loss 1.59680283\n",
      "Trained batch 953 batch loss 1.68195462 epoch total loss 1.59689224\n",
      "Trained batch 954 batch loss 1.51813221 epoch total loss 1.59680974\n",
      "Trained batch 955 batch loss 1.56997895 epoch total loss 1.59678161\n",
      "Trained batch 956 batch loss 1.51153743 epoch total loss 1.59669244\n",
      "Trained batch 957 batch loss 1.38912761 epoch total loss 1.5964756\n",
      "Trained batch 958 batch loss 1.50640464 epoch total loss 1.59638155\n",
      "Trained batch 959 batch loss 1.55621469 epoch total loss 1.5963397\n",
      "Trained batch 960 batch loss 1.57363629 epoch total loss 1.59631598\n",
      "Trained batch 961 batch loss 1.55105233 epoch total loss 1.59626889\n",
      "Trained batch 962 batch loss 1.4999361 epoch total loss 1.59616864\n",
      "Trained batch 963 batch loss 1.47576642 epoch total loss 1.59604359\n",
      "Trained batch 964 batch loss 1.58222914 epoch total loss 1.59602928\n",
      "Trained batch 965 batch loss 1.58263731 epoch total loss 1.59601545\n",
      "Trained batch 966 batch loss 1.53339601 epoch total loss 1.59595072\n",
      "Trained batch 967 batch loss 1.49186647 epoch total loss 1.59584296\n",
      "Trained batch 968 batch loss 1.46086383 epoch total loss 1.59570348\n",
      "Trained batch 969 batch loss 1.42573237 epoch total loss 1.59552813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 970 batch loss 1.33958125 epoch total loss 1.59526432\n",
      "Trained batch 971 batch loss 1.53130627 epoch total loss 1.59519839\n",
      "Trained batch 972 batch loss 1.50511861 epoch total loss 1.59510577\n",
      "Trained batch 973 batch loss 1.48345518 epoch total loss 1.59499085\n",
      "Trained batch 974 batch loss 1.46605706 epoch total loss 1.59485853\n",
      "Trained batch 975 batch loss 1.37513804 epoch total loss 1.59463322\n",
      "Trained batch 976 batch loss 1.51004386 epoch total loss 1.59454644\n",
      "Trained batch 977 batch loss 1.55457258 epoch total loss 1.59450555\n",
      "Trained batch 978 batch loss 1.49884105 epoch total loss 1.5944078\n",
      "Trained batch 979 batch loss 1.47718549 epoch total loss 1.59428799\n",
      "Trained batch 980 batch loss 1.44662714 epoch total loss 1.59413743\n",
      "Trained batch 981 batch loss 1.37752175 epoch total loss 1.59391665\n",
      "Trained batch 982 batch loss 1.35046101 epoch total loss 1.5936687\n",
      "Trained batch 983 batch loss 1.36116183 epoch total loss 1.59343219\n",
      "Trained batch 984 batch loss 1.3676455 epoch total loss 1.59320283\n",
      "Trained batch 985 batch loss 1.45236254 epoch total loss 1.5930599\n",
      "Trained batch 986 batch loss 1.429878 epoch total loss 1.59289443\n",
      "Trained batch 987 batch loss 1.48473752 epoch total loss 1.59278476\n",
      "Trained batch 988 batch loss 1.42928553 epoch total loss 1.59261942\n",
      "Trained batch 989 batch loss 1.51873136 epoch total loss 1.59254456\n",
      "Trained batch 990 batch loss 1.59721661 epoch total loss 1.59254932\n",
      "Trained batch 991 batch loss 1.41269016 epoch total loss 1.59236777\n",
      "Trained batch 992 batch loss 1.40856814 epoch total loss 1.59218252\n",
      "Trained batch 993 batch loss 1.46279109 epoch total loss 1.59205222\n",
      "Trained batch 994 batch loss 1.48405957 epoch total loss 1.5919435\n",
      "Trained batch 995 batch loss 1.47470713 epoch total loss 1.59182572\n",
      "Trained batch 996 batch loss 1.53025782 epoch total loss 1.59176385\n",
      "Trained batch 997 batch loss 1.63261199 epoch total loss 1.59180486\n",
      "Trained batch 998 batch loss 1.60645247 epoch total loss 1.59181952\n",
      "Trained batch 999 batch loss 1.43887305 epoch total loss 1.59166634\n",
      "Trained batch 1000 batch loss 1.54910553 epoch total loss 1.59162378\n",
      "Trained batch 1001 batch loss 1.49396324 epoch total loss 1.59152627\n",
      "Trained batch 1002 batch loss 1.51912713 epoch total loss 1.59145403\n",
      "Trained batch 1003 batch loss 1.62989867 epoch total loss 1.59149241\n",
      "Trained batch 1004 batch loss 1.45585191 epoch total loss 1.59135723\n",
      "Trained batch 1005 batch loss 1.42921543 epoch total loss 1.59119582\n",
      "Trained batch 1006 batch loss 1.61938787 epoch total loss 1.59122396\n",
      "Trained batch 1007 batch loss 1.52270591 epoch total loss 1.59115589\n",
      "Trained batch 1008 batch loss 1.51608539 epoch total loss 1.59108138\n",
      "Trained batch 1009 batch loss 1.46747851 epoch total loss 1.59095895\n",
      "Trained batch 1010 batch loss 1.38322091 epoch total loss 1.5907532\n",
      "Trained batch 1011 batch loss 1.42983747 epoch total loss 1.59059405\n",
      "Trained batch 1012 batch loss 1.47076786 epoch total loss 1.59047568\n",
      "Trained batch 1013 batch loss 1.51051545 epoch total loss 1.59039676\n",
      "Trained batch 1014 batch loss 1.45365691 epoch total loss 1.59026182\n",
      "Trained batch 1015 batch loss 1.43527842 epoch total loss 1.59010923\n",
      "Trained batch 1016 batch loss 1.4600991 epoch total loss 1.5899812\n",
      "Trained batch 1017 batch loss 1.41850281 epoch total loss 1.58981252\n",
      "Trained batch 1018 batch loss 1.48830259 epoch total loss 1.58971286\n",
      "Trained batch 1019 batch loss 1.42047179 epoch total loss 1.5895468\n",
      "Trained batch 1020 batch loss 1.66704774 epoch total loss 1.58962274\n",
      "Trained batch 1021 batch loss 1.52434886 epoch total loss 1.58955872\n",
      "Trained batch 1022 batch loss 1.54344761 epoch total loss 1.58951366\n",
      "Trained batch 1023 batch loss 1.5741272 epoch total loss 1.58949852\n",
      "Trained batch 1024 batch loss 1.50840044 epoch total loss 1.58941936\n",
      "Trained batch 1025 batch loss 1.49547863 epoch total loss 1.58932769\n",
      "Trained batch 1026 batch loss 1.46312761 epoch total loss 1.58920467\n",
      "Trained batch 1027 batch loss 1.41786027 epoch total loss 1.5890379\n",
      "Trained batch 1028 batch loss 1.5319314 epoch total loss 1.58898234\n",
      "Trained batch 1029 batch loss 1.36545908 epoch total loss 1.58876514\n",
      "Trained batch 1030 batch loss 1.54851305 epoch total loss 1.58872604\n",
      "Trained batch 1031 batch loss 1.58699143 epoch total loss 1.58872437\n",
      "Trained batch 1032 batch loss 1.56082749 epoch total loss 1.58869731\n",
      "Trained batch 1033 batch loss 1.50261986 epoch total loss 1.58861399\n",
      "Trained batch 1034 batch loss 1.54787219 epoch total loss 1.58857453\n",
      "Trained batch 1035 batch loss 1.56266618 epoch total loss 1.58854949\n",
      "Trained batch 1036 batch loss 1.46286142 epoch total loss 1.58842814\n",
      "Trained batch 1037 batch loss 1.42725337 epoch total loss 1.58827269\n",
      "Trained batch 1038 batch loss 1.49680519 epoch total loss 1.5881846\n",
      "Trained batch 1039 batch loss 1.48427558 epoch total loss 1.58808458\n",
      "Trained batch 1040 batch loss 1.37691629 epoch total loss 1.58788157\n",
      "Trained batch 1041 batch loss 1.4791975 epoch total loss 1.58777726\n",
      "Trained batch 1042 batch loss 1.42983174 epoch total loss 1.58762562\n",
      "Trained batch 1043 batch loss 1.51287854 epoch total loss 1.58755398\n",
      "Trained batch 1044 batch loss 1.54565537 epoch total loss 1.58751392\n",
      "Trained batch 1045 batch loss 1.28700101 epoch total loss 1.58722627\n",
      "Trained batch 1046 batch loss 1.37260234 epoch total loss 1.58702111\n",
      "Trained batch 1047 batch loss 1.45796466 epoch total loss 1.58689785\n",
      "Trained batch 1048 batch loss 1.42863488 epoch total loss 1.58674681\n",
      "Trained batch 1049 batch loss 1.48825 epoch total loss 1.58665287\n",
      "Trained batch 1050 batch loss 1.53923702 epoch total loss 1.58660769\n",
      "Trained batch 1051 batch loss 1.52472401 epoch total loss 1.58654892\n",
      "Trained batch 1052 batch loss 1.49755025 epoch total loss 1.58646429\n",
      "Trained batch 1053 batch loss 1.43283796 epoch total loss 1.58631849\n",
      "Trained batch 1054 batch loss 1.43849587 epoch total loss 1.58617818\n",
      "Trained batch 1055 batch loss 1.4668107 epoch total loss 1.58606505\n",
      "Trained batch 1056 batch loss 1.38461924 epoch total loss 1.58587432\n",
      "Trained batch 1057 batch loss 1.42668593 epoch total loss 1.58572364\n",
      "Trained batch 1058 batch loss 1.48383927 epoch total loss 1.58562732\n",
      "Trained batch 1059 batch loss 1.50633311 epoch total loss 1.58555245\n",
      "Trained batch 1060 batch loss 1.48505139 epoch total loss 1.58545768\n",
      "Trained batch 1061 batch loss 1.43193913 epoch total loss 1.58531296\n",
      "Trained batch 1062 batch loss 1.43662274 epoch total loss 1.58517301\n",
      "Trained batch 1063 batch loss 1.47946167 epoch total loss 1.58507359\n",
      "Trained batch 1064 batch loss 1.45227933 epoch total loss 1.58494878\n",
      "Trained batch 1065 batch loss 1.47974324 epoch total loss 1.58485\n",
      "Trained batch 1066 batch loss 1.36460817 epoch total loss 1.58464336\n",
      "Trained batch 1067 batch loss 1.27232885 epoch total loss 1.58435071\n",
      "Trained batch 1068 batch loss 1.3594389 epoch total loss 1.58414018\n",
      "Trained batch 1069 batch loss 1.37487209 epoch total loss 1.58394444\n",
      "Trained batch 1070 batch loss 1.45517361 epoch total loss 1.58382404\n",
      "Trained batch 1071 batch loss 1.49486053 epoch total loss 1.58374107\n",
      "Trained batch 1072 batch loss 1.48413658 epoch total loss 1.58364809\n",
      "Trained batch 1073 batch loss 1.41049755 epoch total loss 1.5834868\n",
      "Trained batch 1074 batch loss 1.50592506 epoch total loss 1.58341455\n",
      "Trained batch 1075 batch loss 1.48527575 epoch total loss 1.58332324\n",
      "Trained batch 1076 batch loss 1.54706311 epoch total loss 1.58328962\n",
      "Trained batch 1077 batch loss 1.4672488 epoch total loss 1.58318186\n",
      "Trained batch 1078 batch loss 1.46837807 epoch total loss 1.5830754\n",
      "Trained batch 1079 batch loss 1.46522307 epoch total loss 1.58296621\n",
      "Trained batch 1080 batch loss 1.3581208 epoch total loss 1.58275807\n",
      "Trained batch 1081 batch loss 1.4106369 epoch total loss 1.58259881\n",
      "Trained batch 1082 batch loss 1.47108746 epoch total loss 1.58249569\n",
      "Trained batch 1083 batch loss 1.35283303 epoch total loss 1.58228362\n",
      "Trained batch 1084 batch loss 1.45002472 epoch total loss 1.58216166\n",
      "Trained batch 1085 batch loss 1.35024 epoch total loss 1.58194792\n",
      "Trained batch 1086 batch loss 1.26180649 epoch total loss 1.58165312\n",
      "Trained batch 1087 batch loss 1.43891239 epoch total loss 1.58152187\n",
      "Trained batch 1088 batch loss 1.46253932 epoch total loss 1.58141243\n",
      "Trained batch 1089 batch loss 1.50158072 epoch total loss 1.58133924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1090 batch loss 1.56385064 epoch total loss 1.58132315\n",
      "Trained batch 1091 batch loss 1.55370045 epoch total loss 1.58129787\n",
      "Trained batch 1092 batch loss 1.45717776 epoch total loss 1.58118415\n",
      "Trained batch 1093 batch loss 1.52645051 epoch total loss 1.58113408\n",
      "Trained batch 1094 batch loss 1.63382626 epoch total loss 1.58118224\n",
      "Trained batch 1095 batch loss 1.48846948 epoch total loss 1.5810976\n",
      "Trained batch 1096 batch loss 1.45967627 epoch total loss 1.58098686\n",
      "Trained batch 1097 batch loss 1.4930768 epoch total loss 1.58090675\n",
      "Trained batch 1098 batch loss 1.51488912 epoch total loss 1.58084655\n",
      "Trained batch 1099 batch loss 1.55036974 epoch total loss 1.58081889\n",
      "Trained batch 1100 batch loss 1.48756301 epoch total loss 1.58073413\n",
      "Trained batch 1101 batch loss 1.42861652 epoch total loss 1.58059585\n",
      "Trained batch 1102 batch loss 1.42660022 epoch total loss 1.58045614\n",
      "Trained batch 1103 batch loss 1.42628825 epoch total loss 1.58031642\n",
      "Trained batch 1104 batch loss 1.45572877 epoch total loss 1.58020353\n",
      "Trained batch 1105 batch loss 1.44219708 epoch total loss 1.5800786\n",
      "Trained batch 1106 batch loss 1.39277482 epoch total loss 1.57990921\n",
      "Trained batch 1107 batch loss 1.44130826 epoch total loss 1.57978404\n",
      "Trained batch 1108 batch loss 1.44562984 epoch total loss 1.57966304\n",
      "Trained batch 1109 batch loss 1.45973086 epoch total loss 1.5795548\n",
      "Trained batch 1110 batch loss 1.51986134 epoch total loss 1.57950115\n",
      "Trained batch 1111 batch loss 1.59771562 epoch total loss 1.57951748\n",
      "Trained batch 1112 batch loss 1.5483315 epoch total loss 1.57948935\n",
      "Trained batch 1113 batch loss 1.52768576 epoch total loss 1.57944286\n",
      "Trained batch 1114 batch loss 1.47694111 epoch total loss 1.57935083\n",
      "Trained batch 1115 batch loss 1.48707724 epoch total loss 1.5792681\n",
      "Trained batch 1116 batch loss 1.47954428 epoch total loss 1.57917869\n",
      "Trained batch 1117 batch loss 1.50036287 epoch total loss 1.57910812\n",
      "Trained batch 1118 batch loss 1.5193013 epoch total loss 1.57905459\n",
      "Trained batch 1119 batch loss 1.64054954 epoch total loss 1.57910955\n",
      "Trained batch 1120 batch loss 1.72483718 epoch total loss 1.57923961\n",
      "Trained batch 1121 batch loss 1.57493043 epoch total loss 1.57923579\n",
      "Trained batch 1122 batch loss 1.66601562 epoch total loss 1.57931316\n",
      "Trained batch 1123 batch loss 1.5722537 epoch total loss 1.57930684\n",
      "Trained batch 1124 batch loss 1.642923 epoch total loss 1.57936347\n",
      "Trained batch 1125 batch loss 1.64722991 epoch total loss 1.57942379\n",
      "Trained batch 1126 batch loss 1.51752007 epoch total loss 1.57936895\n",
      "Trained batch 1127 batch loss 1.49868953 epoch total loss 1.5792973\n",
      "Trained batch 1128 batch loss 1.47119486 epoch total loss 1.57920146\n",
      "Trained batch 1129 batch loss 1.51119161 epoch total loss 1.57914126\n",
      "Trained batch 1130 batch loss 1.40699422 epoch total loss 1.57898891\n",
      "Trained batch 1131 batch loss 1.48061323 epoch total loss 1.57890189\n",
      "Trained batch 1132 batch loss 1.44108653 epoch total loss 1.57878\n",
      "Trained batch 1133 batch loss 1.42935252 epoch total loss 1.57864821\n",
      "Trained batch 1134 batch loss 1.40796471 epoch total loss 1.57849765\n",
      "Trained batch 1135 batch loss 1.46168447 epoch total loss 1.57839477\n",
      "Trained batch 1136 batch loss 1.35168529 epoch total loss 1.57819521\n",
      "Trained batch 1137 batch loss 1.32459891 epoch total loss 1.57797217\n",
      "Trained batch 1138 batch loss 1.42035818 epoch total loss 1.57783365\n",
      "Trained batch 1139 batch loss 1.35710132 epoch total loss 1.57763982\n",
      "Trained batch 1140 batch loss 1.27302289 epoch total loss 1.57737267\n",
      "Trained batch 1141 batch loss 1.38733387 epoch total loss 1.57720613\n",
      "Trained batch 1142 batch loss 1.41747129 epoch total loss 1.57706618\n",
      "Trained batch 1143 batch loss 1.42025054 epoch total loss 1.57692909\n",
      "Trained batch 1144 batch loss 1.45889091 epoch total loss 1.57682586\n",
      "Trained batch 1145 batch loss 1.53877163 epoch total loss 1.57679272\n",
      "Trained batch 1146 batch loss 1.55349398 epoch total loss 1.57677233\n",
      "Trained batch 1147 batch loss 1.53840208 epoch total loss 1.57673895\n",
      "Trained batch 1148 batch loss 1.50886989 epoch total loss 1.57667983\n",
      "Trained batch 1149 batch loss 1.53107691 epoch total loss 1.57664013\n",
      "Trained batch 1150 batch loss 1.58365655 epoch total loss 1.57664621\n",
      "Trained batch 1151 batch loss 1.53306639 epoch total loss 1.57660842\n",
      "Trained batch 1152 batch loss 1.52834 epoch total loss 1.57656646\n",
      "Trained batch 1153 batch loss 1.48791134 epoch total loss 1.57648957\n",
      "Trained batch 1154 batch loss 1.54845512 epoch total loss 1.57646537\n",
      "Trained batch 1155 batch loss 1.52224112 epoch total loss 1.5764184\n",
      "Trained batch 1156 batch loss 1.49847221 epoch total loss 1.57635081\n",
      "Trained batch 1157 batch loss 1.48965609 epoch total loss 1.57627594\n",
      "Trained batch 1158 batch loss 1.35789204 epoch total loss 1.57608736\n",
      "Trained batch 1159 batch loss 1.44255447 epoch total loss 1.57597208\n",
      "Trained batch 1160 batch loss 1.27335787 epoch total loss 1.57571113\n",
      "Trained batch 1161 batch loss 1.32244074 epoch total loss 1.57549298\n",
      "Trained batch 1162 batch loss 1.3755672 epoch total loss 1.57532096\n",
      "Trained batch 1163 batch loss 1.27035534 epoch total loss 1.5750587\n",
      "Trained batch 1164 batch loss 1.22038651 epoch total loss 1.574754\n",
      "Trained batch 1165 batch loss 1.16674984 epoch total loss 1.57440376\n",
      "Trained batch 1166 batch loss 1.40993547 epoch total loss 1.57426274\n",
      "Trained batch 1167 batch loss 1.45116591 epoch total loss 1.57415724\n",
      "Trained batch 1168 batch loss 1.42137182 epoch total loss 1.57402647\n",
      "Trained batch 1169 batch loss 1.49989688 epoch total loss 1.57396305\n",
      "Trained batch 1170 batch loss 1.43004751 epoch total loss 1.57384\n",
      "Trained batch 1171 batch loss 1.45870376 epoch total loss 1.57374167\n",
      "Trained batch 1172 batch loss 1.47996044 epoch total loss 1.57366168\n",
      "Trained batch 1173 batch loss 1.43263161 epoch total loss 1.57354152\n",
      "Trained batch 1174 batch loss 1.37267208 epoch total loss 1.57337034\n",
      "Trained batch 1175 batch loss 1.34051418 epoch total loss 1.57317221\n",
      "Trained batch 1176 batch loss 1.30743086 epoch total loss 1.57294619\n",
      "Trained batch 1177 batch loss 1.48407257 epoch total loss 1.57287073\n",
      "Trained batch 1178 batch loss 1.69619215 epoch total loss 1.5729754\n",
      "Trained batch 1179 batch loss 1.54368782 epoch total loss 1.57295048\n",
      "Trained batch 1180 batch loss 1.53469539 epoch total loss 1.57291806\n",
      "Trained batch 1181 batch loss 1.45459116 epoch total loss 1.57281792\n",
      "Trained batch 1182 batch loss 1.53384805 epoch total loss 1.5727849\n",
      "Trained batch 1183 batch loss 1.44177914 epoch total loss 1.57267416\n",
      "Trained batch 1184 batch loss 1.43843591 epoch total loss 1.57256079\n",
      "Trained batch 1185 batch loss 1.43461335 epoch total loss 1.57244432\n",
      "Trained batch 1186 batch loss 1.51738322 epoch total loss 1.57239783\n",
      "Trained batch 1187 batch loss 1.49670744 epoch total loss 1.57233417\n",
      "Trained batch 1188 batch loss 1.49488425 epoch total loss 1.57226896\n",
      "Trained batch 1189 batch loss 1.47163463 epoch total loss 1.57218432\n",
      "Trained batch 1190 batch loss 1.49593544 epoch total loss 1.57212031\n",
      "Trained batch 1191 batch loss 1.36485934 epoch total loss 1.57194626\n",
      "Trained batch 1192 batch loss 1.4648993 epoch total loss 1.57185638\n",
      "Trained batch 1193 batch loss 1.43058038 epoch total loss 1.571738\n",
      "Trained batch 1194 batch loss 1.44018495 epoch total loss 1.57162774\n",
      "Trained batch 1195 batch loss 1.49598396 epoch total loss 1.57156444\n",
      "Trained batch 1196 batch loss 1.37106228 epoch total loss 1.57139683\n",
      "Trained batch 1197 batch loss 1.40593255 epoch total loss 1.57125854\n",
      "Trained batch 1198 batch loss 1.47723639 epoch total loss 1.57118011\n",
      "Trained batch 1199 batch loss 1.39505482 epoch total loss 1.57103324\n",
      "Trained batch 1200 batch loss 1.45362139 epoch total loss 1.57093537\n",
      "Trained batch 1201 batch loss 1.39051425 epoch total loss 1.57078516\n",
      "Trained batch 1202 batch loss 1.43435669 epoch total loss 1.57067156\n",
      "Trained batch 1203 batch loss 1.39620256 epoch total loss 1.5705266\n",
      "Trained batch 1204 batch loss 1.38248754 epoch total loss 1.57037044\n",
      "Trained batch 1205 batch loss 1.35295224 epoch total loss 1.57019\n",
      "Trained batch 1206 batch loss 1.48173261 epoch total loss 1.57011652\n",
      "Trained batch 1207 batch loss 1.57492697 epoch total loss 1.57012057\n",
      "Trained batch 1208 batch loss 1.57876778 epoch total loss 1.57012773\n",
      "Trained batch 1209 batch loss 1.50099337 epoch total loss 1.57007051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1210 batch loss 1.7723887 epoch total loss 1.57023764\n",
      "Trained batch 1211 batch loss 1.62616801 epoch total loss 1.57028389\n",
      "Trained batch 1212 batch loss 1.62366498 epoch total loss 1.57032788\n",
      "Trained batch 1213 batch loss 1.48903704 epoch total loss 1.57026088\n",
      "Trained batch 1214 batch loss 1.42336237 epoch total loss 1.57013988\n",
      "Trained batch 1215 batch loss 1.3280673 epoch total loss 1.56994069\n",
      "Trained batch 1216 batch loss 1.29975414 epoch total loss 1.56971848\n",
      "Trained batch 1217 batch loss 1.42347741 epoch total loss 1.56959832\n",
      "Trained batch 1218 batch loss 1.58178949 epoch total loss 1.56960833\n",
      "Trained batch 1219 batch loss 1.56376648 epoch total loss 1.56960356\n",
      "Trained batch 1220 batch loss 1.52157223 epoch total loss 1.56956422\n",
      "Trained batch 1221 batch loss 1.61679471 epoch total loss 1.56960285\n",
      "Trained batch 1222 batch loss 1.52196681 epoch total loss 1.56956387\n",
      "Trained batch 1223 batch loss 1.49550033 epoch total loss 1.56950331\n",
      "Trained batch 1224 batch loss 1.55433178 epoch total loss 1.56949091\n",
      "Trained batch 1225 batch loss 1.56560099 epoch total loss 1.56948769\n",
      "Trained batch 1226 batch loss 1.52769148 epoch total loss 1.5694536\n",
      "Trained batch 1227 batch loss 1.48189986 epoch total loss 1.56938231\n",
      "Trained batch 1228 batch loss 1.49615061 epoch total loss 1.56932259\n",
      "Trained batch 1229 batch loss 1.33621347 epoch total loss 1.56913292\n",
      "Trained batch 1230 batch loss 1.31325388 epoch total loss 1.5689249\n",
      "Trained batch 1231 batch loss 1.48143184 epoch total loss 1.56885386\n",
      "Trained batch 1232 batch loss 1.45426261 epoch total loss 1.56876075\n",
      "Trained batch 1233 batch loss 1.53199816 epoch total loss 1.56873095\n",
      "Trained batch 1234 batch loss 1.5084939 epoch total loss 1.56868219\n",
      "Trained batch 1235 batch loss 1.59827459 epoch total loss 1.56870615\n",
      "Trained batch 1236 batch loss 1.50251162 epoch total loss 1.56865263\n",
      "Trained batch 1237 batch loss 1.46635222 epoch total loss 1.5685699\n",
      "Trained batch 1238 batch loss 1.53540194 epoch total loss 1.56854308\n",
      "Trained batch 1239 batch loss 1.54600751 epoch total loss 1.56852484\n",
      "Trained batch 1240 batch loss 1.47534502 epoch total loss 1.56844974\n",
      "Trained batch 1241 batch loss 1.56002212 epoch total loss 1.56844294\n",
      "Trained batch 1242 batch loss 1.60716605 epoch total loss 1.56847417\n",
      "Trained batch 1243 batch loss 1.51708686 epoch total loss 1.56843281\n",
      "Trained batch 1244 batch loss 1.42209744 epoch total loss 1.56831527\n",
      "Trained batch 1245 batch loss 1.46837366 epoch total loss 1.56823492\n",
      "Trained batch 1246 batch loss 1.52793062 epoch total loss 1.56820261\n",
      "Trained batch 1247 batch loss 1.61955392 epoch total loss 1.56824374\n",
      "Trained batch 1248 batch loss 1.48654866 epoch total loss 1.5681783\n",
      "Trained batch 1249 batch loss 1.50729418 epoch total loss 1.56812966\n",
      "Trained batch 1250 batch loss 1.56786346 epoch total loss 1.56812942\n",
      "Trained batch 1251 batch loss 1.50301266 epoch total loss 1.56807733\n",
      "Trained batch 1252 batch loss 1.50217795 epoch total loss 1.56802475\n",
      "Trained batch 1253 batch loss 1.4917959 epoch total loss 1.56796396\n",
      "Trained batch 1254 batch loss 1.60049868 epoch total loss 1.56798983\n",
      "Trained batch 1255 batch loss 1.51721907 epoch total loss 1.56794941\n",
      "Trained batch 1256 batch loss 1.41458154 epoch total loss 1.56782722\n",
      "Trained batch 1257 batch loss 1.40970719 epoch total loss 1.56770146\n",
      "Trained batch 1258 batch loss 1.32401919 epoch total loss 1.56750774\n",
      "Trained batch 1259 batch loss 1.52089059 epoch total loss 1.56747067\n",
      "Trained batch 1260 batch loss 1.32077587 epoch total loss 1.56727493\n",
      "Trained batch 1261 batch loss 1.59471583 epoch total loss 1.56729662\n",
      "Trained batch 1262 batch loss 1.40858173 epoch total loss 1.56717086\n",
      "Trained batch 1263 batch loss 1.40222859 epoch total loss 1.56704032\n",
      "Trained batch 1264 batch loss 1.34492528 epoch total loss 1.56686461\n",
      "Trained batch 1265 batch loss 1.46405947 epoch total loss 1.56678331\n",
      "Trained batch 1266 batch loss 1.39038134 epoch total loss 1.56664407\n",
      "Trained batch 1267 batch loss 1.44675255 epoch total loss 1.56654942\n",
      "Trained batch 1268 batch loss 1.51776755 epoch total loss 1.56651103\n",
      "Trained batch 1269 batch loss 1.43496704 epoch total loss 1.56640732\n",
      "Trained batch 1270 batch loss 1.48607385 epoch total loss 1.56634402\n",
      "Trained batch 1271 batch loss 1.33622885 epoch total loss 1.56616294\n",
      "Trained batch 1272 batch loss 1.49342954 epoch total loss 1.56610572\n",
      "Trained batch 1273 batch loss 1.59538126 epoch total loss 1.56612873\n",
      "Trained batch 1274 batch loss 1.42244124 epoch total loss 1.56601596\n",
      "Trained batch 1275 batch loss 1.48937905 epoch total loss 1.56595588\n",
      "Trained batch 1276 batch loss 1.49048913 epoch total loss 1.56589675\n",
      "Trained batch 1277 batch loss 1.38037682 epoch total loss 1.56575143\n",
      "Trained batch 1278 batch loss 1.37159967 epoch total loss 1.56559956\n",
      "Trained batch 1279 batch loss 1.46155775 epoch total loss 1.56551814\n",
      "Trained batch 1280 batch loss 1.44283438 epoch total loss 1.5654223\n",
      "Trained batch 1281 batch loss 1.42388725 epoch total loss 1.56531179\n",
      "Trained batch 1282 batch loss 1.36633289 epoch total loss 1.56515658\n",
      "Trained batch 1283 batch loss 1.49366271 epoch total loss 1.56510091\n",
      "Trained batch 1284 batch loss 1.35014319 epoch total loss 1.56493342\n",
      "Trained batch 1285 batch loss 1.44084156 epoch total loss 1.56483686\n",
      "Trained batch 1286 batch loss 1.38316989 epoch total loss 1.5646956\n",
      "Trained batch 1287 batch loss 1.39171338 epoch total loss 1.56456113\n",
      "Trained batch 1288 batch loss 1.36871779 epoch total loss 1.56440914\n",
      "Trained batch 1289 batch loss 1.41070557 epoch total loss 1.56428981\n",
      "Trained batch 1290 batch loss 1.50608087 epoch total loss 1.56424475\n",
      "Trained batch 1291 batch loss 1.3980875 epoch total loss 1.564116\n",
      "Trained batch 1292 batch loss 1.36093342 epoch total loss 1.56395876\n",
      "Trained batch 1293 batch loss 1.41766334 epoch total loss 1.56384563\n",
      "Trained batch 1294 batch loss 1.50750923 epoch total loss 1.56380212\n",
      "Trained batch 1295 batch loss 1.46325707 epoch total loss 1.56372452\n",
      "Trained batch 1296 batch loss 1.46694934 epoch total loss 1.56364977\n",
      "Trained batch 1297 batch loss 1.45385408 epoch total loss 1.56356514\n",
      "Trained batch 1298 batch loss 1.36328 epoch total loss 1.56341088\n",
      "Trained batch 1299 batch loss 1.40457308 epoch total loss 1.56328857\n",
      "Trained batch 1300 batch loss 1.35585523 epoch total loss 1.56312895\n",
      "Trained batch 1301 batch loss 1.33353114 epoch total loss 1.5629524\n",
      "Trained batch 1302 batch loss 1.45077467 epoch total loss 1.56286633\n",
      "Trained batch 1303 batch loss 1.52384639 epoch total loss 1.56283629\n",
      "Trained batch 1304 batch loss 1.39989567 epoch total loss 1.56271136\n",
      "Trained batch 1305 batch loss 1.31995058 epoch total loss 1.56252539\n",
      "Trained batch 1306 batch loss 1.4557935 epoch total loss 1.56244361\n",
      "Trained batch 1307 batch loss 1.4472183 epoch total loss 1.56235552\n",
      "Trained batch 1308 batch loss 1.38296163 epoch total loss 1.56221831\n",
      "Trained batch 1309 batch loss 1.46913397 epoch total loss 1.56214726\n",
      "Trained batch 1310 batch loss 1.46633136 epoch total loss 1.56207407\n",
      "Trained batch 1311 batch loss 1.38310313 epoch total loss 1.56193745\n",
      "Trained batch 1312 batch loss 1.48459864 epoch total loss 1.56187844\n",
      "Trained batch 1313 batch loss 1.4839747 epoch total loss 1.56181908\n",
      "Trained batch 1314 batch loss 1.49794602 epoch total loss 1.56177056\n",
      "Trained batch 1315 batch loss 1.55267727 epoch total loss 1.56176364\n",
      "Trained batch 1316 batch loss 1.67422163 epoch total loss 1.56184924\n",
      "Trained batch 1317 batch loss 1.60823858 epoch total loss 1.5618844\n",
      "Trained batch 1318 batch loss 1.47786868 epoch total loss 1.56182051\n",
      "Trained batch 1319 batch loss 1.27620387 epoch total loss 1.5616039\n",
      "Trained batch 1320 batch loss 1.47371495 epoch total loss 1.56153727\n",
      "Trained batch 1321 batch loss 1.57339621 epoch total loss 1.56154633\n",
      "Trained batch 1322 batch loss 1.52203381 epoch total loss 1.5615164\n",
      "Trained batch 1323 batch loss 1.51898205 epoch total loss 1.56148434\n",
      "Trained batch 1324 batch loss 1.44987679 epoch total loss 1.5614\n",
      "Trained batch 1325 batch loss 1.43273127 epoch total loss 1.5613029\n",
      "Trained batch 1326 batch loss 1.23003149 epoch total loss 1.56105304\n",
      "Trained batch 1327 batch loss 1.27716899 epoch total loss 1.56083906\n",
      "Trained batch 1328 batch loss 1.39599121 epoch total loss 1.56071496\n",
      "Trained batch 1329 batch loss 1.41084719 epoch total loss 1.56060219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1330 batch loss 1.31526971 epoch total loss 1.56041765\n",
      "Trained batch 1331 batch loss 1.39311337 epoch total loss 1.56029189\n",
      "Trained batch 1332 batch loss 1.50465012 epoch total loss 1.56025016\n",
      "Trained batch 1333 batch loss 1.38512874 epoch total loss 1.56011868\n",
      "Trained batch 1334 batch loss 1.42502141 epoch total loss 1.56001747\n",
      "Trained batch 1335 batch loss 1.43785751 epoch total loss 1.55992579\n",
      "Trained batch 1336 batch loss 1.46783566 epoch total loss 1.55985689\n",
      "Trained batch 1337 batch loss 1.46133304 epoch total loss 1.55978322\n",
      "Trained batch 1338 batch loss 1.21700358 epoch total loss 1.55952704\n",
      "Trained batch 1339 batch loss 1.11275411 epoch total loss 1.55919349\n",
      "Trained batch 1340 batch loss 1.18883467 epoch total loss 1.55891693\n",
      "Trained batch 1341 batch loss 1.45607853 epoch total loss 1.55884027\n",
      "Trained batch 1342 batch loss 1.59947538 epoch total loss 1.55887043\n",
      "Trained batch 1343 batch loss 1.65085423 epoch total loss 1.55893898\n",
      "Trained batch 1344 batch loss 1.60358751 epoch total loss 1.55897212\n",
      "Trained batch 1345 batch loss 1.45923734 epoch total loss 1.55889797\n",
      "Trained batch 1346 batch loss 1.57422483 epoch total loss 1.5589093\n",
      "Trained batch 1347 batch loss 1.40516138 epoch total loss 1.55879533\n",
      "Trained batch 1348 batch loss 1.22661436 epoch total loss 1.55854881\n",
      "Trained batch 1349 batch loss 1.24891758 epoch total loss 1.55831933\n",
      "Trained batch 1350 batch loss 1.39379549 epoch total loss 1.5581975\n",
      "Trained batch 1351 batch loss 1.42027366 epoch total loss 1.55809534\n",
      "Trained batch 1352 batch loss 1.52457666 epoch total loss 1.55807066\n",
      "Trained batch 1353 batch loss 1.53894854 epoch total loss 1.55805659\n",
      "Trained batch 1354 batch loss 1.56373513 epoch total loss 1.55806077\n",
      "Trained batch 1355 batch loss 1.46668494 epoch total loss 1.55799341\n",
      "Trained batch 1356 batch loss 1.47569358 epoch total loss 1.55793262\n",
      "Trained batch 1357 batch loss 1.47374725 epoch total loss 1.55787051\n",
      "Trained batch 1358 batch loss 1.51829529 epoch total loss 1.55784142\n",
      "Trained batch 1359 batch loss 1.57926202 epoch total loss 1.55785716\n",
      "Trained batch 1360 batch loss 1.57151949 epoch total loss 1.55786729\n",
      "Trained batch 1361 batch loss 1.57531476 epoch total loss 1.55787992\n",
      "Trained batch 1362 batch loss 1.57280934 epoch total loss 1.55789089\n",
      "Trained batch 1363 batch loss 1.51811981 epoch total loss 1.55786169\n",
      "Trained batch 1364 batch loss 1.50895643 epoch total loss 1.55782592\n",
      "Trained batch 1365 batch loss 1.35116172 epoch total loss 1.55767441\n",
      "Trained batch 1366 batch loss 1.32490921 epoch total loss 1.55750406\n",
      "Trained batch 1367 batch loss 1.40458894 epoch total loss 1.55739212\n",
      "Trained batch 1368 batch loss 1.43610179 epoch total loss 1.55730343\n",
      "Trained batch 1369 batch loss 1.42292643 epoch total loss 1.5572052\n",
      "Trained batch 1370 batch loss 1.41995132 epoch total loss 1.55710506\n",
      "Trained batch 1371 batch loss 1.43658257 epoch total loss 1.55701709\n",
      "Trained batch 1372 batch loss 1.31720746 epoch total loss 1.55684221\n",
      "Trained batch 1373 batch loss 1.32247865 epoch total loss 1.55667162\n",
      "Trained batch 1374 batch loss 1.33816576 epoch total loss 1.55651248\n",
      "Trained batch 1375 batch loss 1.42315078 epoch total loss 1.55641544\n",
      "Trained batch 1376 batch loss 1.42513859 epoch total loss 1.55632007\n",
      "Trained batch 1377 batch loss 1.51295161 epoch total loss 1.55628848\n",
      "Trained batch 1378 batch loss 1.36635745 epoch total loss 1.55615079\n",
      "Trained batch 1379 batch loss 1.36588752 epoch total loss 1.55601287\n",
      "Trained batch 1380 batch loss 1.35473287 epoch total loss 1.55586696\n",
      "Trained batch 1381 batch loss 1.46027696 epoch total loss 1.5557977\n",
      "Trained batch 1382 batch loss 1.59009922 epoch total loss 1.55582249\n",
      "Trained batch 1383 batch loss 1.36706829 epoch total loss 1.55568612\n",
      "Trained batch 1384 batch loss 1.47866106 epoch total loss 1.55563056\n",
      "Trained batch 1385 batch loss 1.34928274 epoch total loss 1.55548167\n",
      "Trained batch 1386 batch loss 1.32116497 epoch total loss 1.55531251\n",
      "Trained batch 1387 batch loss 1.32442331 epoch total loss 1.55514598\n",
      "Trained batch 1388 batch loss 1.32644176 epoch total loss 1.55498123\n",
      "Epoch 1 train loss 1.5549812316894531\n",
      "Validated batch 1 batch loss 1.41596735\n",
      "Validated batch 2 batch loss 1.33326864\n",
      "Validated batch 3 batch loss 1.4619832\n",
      "Validated batch 4 batch loss 1.37941194\n",
      "Validated batch 5 batch loss 1.43414283\n",
      "Validated batch 6 batch loss 1.47713363\n",
      "Validated batch 7 batch loss 1.43117821\n",
      "Validated batch 8 batch loss 1.55468929\n",
      "Validated batch 9 batch loss 1.50633597\n",
      "Validated batch 10 batch loss 1.44702101\n",
      "Validated batch 11 batch loss 1.42149234\n",
      "Validated batch 12 batch loss 1.48436701\n",
      "Validated batch 13 batch loss 1.50351763\n",
      "Validated batch 14 batch loss 1.50872326\n",
      "Validated batch 15 batch loss 1.51515162\n",
      "Validated batch 16 batch loss 1.47701287\n",
      "Validated batch 17 batch loss 1.46935844\n",
      "Validated batch 18 batch loss 1.31275141\n",
      "Validated batch 19 batch loss 1.41561973\n",
      "Validated batch 20 batch loss 1.48905885\n",
      "Validated batch 21 batch loss 1.46213818\n",
      "Validated batch 22 batch loss 1.47860146\n",
      "Validated batch 23 batch loss 1.40325642\n",
      "Validated batch 24 batch loss 1.41344106\n",
      "Validated batch 25 batch loss 1.40015221\n",
      "Validated batch 26 batch loss 1.372738\n",
      "Validated batch 27 batch loss 1.38358665\n",
      "Validated batch 28 batch loss 1.43575048\n",
      "Validated batch 29 batch loss 1.42921817\n",
      "Validated batch 30 batch loss 1.4797709\n",
      "Validated batch 31 batch loss 1.40089417\n",
      "Validated batch 32 batch loss 1.43100142\n",
      "Validated batch 33 batch loss 1.50180924\n",
      "Validated batch 34 batch loss 1.51298821\n",
      "Validated batch 35 batch loss 1.4752785\n",
      "Validated batch 36 batch loss 1.39864171\n",
      "Validated batch 37 batch loss 1.37686539\n",
      "Validated batch 38 batch loss 1.46055543\n",
      "Validated batch 39 batch loss 1.45192552\n",
      "Validated batch 40 batch loss 1.47958469\n",
      "Validated batch 41 batch loss 1.47309983\n",
      "Validated batch 42 batch loss 1.37505651\n",
      "Validated batch 43 batch loss 1.49971116\n",
      "Validated batch 44 batch loss 1.38364506\n",
      "Validated batch 45 batch loss 1.42319405\n",
      "Validated batch 46 batch loss 1.49478626\n",
      "Validated batch 47 batch loss 1.48094082\n",
      "Validated batch 48 batch loss 1.45959508\n",
      "Validated batch 49 batch loss 1.38372242\n",
      "Validated batch 50 batch loss 1.31863165\n",
      "Validated batch 51 batch loss 1.47487497\n",
      "Validated batch 52 batch loss 1.4197948\n",
      "Validated batch 53 batch loss 1.35468781\n",
      "Validated batch 54 batch loss 1.42777312\n",
      "Validated batch 55 batch loss 1.37596512\n",
      "Validated batch 56 batch loss 1.5072248\n",
      "Validated batch 57 batch loss 1.34038508\n",
      "Validated batch 58 batch loss 1.30917883\n",
      "Validated batch 59 batch loss 1.48281515\n",
      "Validated batch 60 batch loss 1.49330819\n",
      "Validated batch 61 batch loss 1.49116683\n",
      "Validated batch 62 batch loss 1.49496198\n",
      "Validated batch 63 batch loss 1.36148953\n",
      "Validated batch 64 batch loss 1.54291832\n",
      "Validated batch 65 batch loss 1.43903446\n",
      "Validated batch 66 batch loss 1.45910478\n",
      "Validated batch 67 batch loss 1.48850822\n",
      "Validated batch 68 batch loss 1.22281992\n",
      "Validated batch 69 batch loss 1.50305319\n",
      "Validated batch 70 batch loss 1.31749427\n",
      "Validated batch 71 batch loss 1.51305008\n",
      "Validated batch 72 batch loss 1.50700641\n",
      "Validated batch 73 batch loss 1.44568157\n",
      "Validated batch 74 batch loss 1.45780921\n",
      "Validated batch 75 batch loss 1.55313933\n",
      "Validated batch 76 batch loss 1.32456446\n",
      "Validated batch 77 batch loss 1.51877451\n",
      "Validated batch 78 batch loss 1.41692877\n",
      "Validated batch 79 batch loss 1.45681477\n",
      "Validated batch 80 batch loss 1.46898746\n",
      "Validated batch 81 batch loss 1.35601258\n",
      "Validated batch 82 batch loss 1.26951742\n",
      "Validated batch 83 batch loss 1.47053039\n",
      "Validated batch 84 batch loss 1.45901632\n",
      "Validated batch 85 batch loss 1.4174931\n",
      "Validated batch 86 batch loss 1.46981871\n",
      "Validated batch 87 batch loss 1.43337822\n",
      "Validated batch 88 batch loss 1.47293913\n",
      "Validated batch 89 batch loss 1.5196569\n",
      "Validated batch 90 batch loss 1.48618507\n",
      "Validated batch 91 batch loss 1.45091128\n",
      "Validated batch 92 batch loss 1.34538758\n",
      "Validated batch 93 batch loss 1.47032785\n",
      "Validated batch 94 batch loss 1.4181931\n",
      "Validated batch 95 batch loss 1.41673708\n",
      "Validated batch 96 batch loss 1.37962461\n",
      "Validated batch 97 batch loss 1.39427245\n",
      "Validated batch 98 batch loss 1.55513704\n",
      "Validated batch 99 batch loss 1.35713494\n",
      "Validated batch 100 batch loss 1.45905197\n",
      "Validated batch 101 batch loss 1.45073366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 102 batch loss 1.4503535\n",
      "Validated batch 103 batch loss 1.41913903\n",
      "Validated batch 104 batch loss 1.34133303\n",
      "Validated batch 105 batch loss 1.48769891\n",
      "Validated batch 106 batch loss 1.47495198\n",
      "Validated batch 107 batch loss 1.45484006\n",
      "Validated batch 108 batch loss 1.42604136\n",
      "Validated batch 109 batch loss 1.42854786\n",
      "Validated batch 110 batch loss 1.32712054\n",
      "Validated batch 111 batch loss 1.37119913\n",
      "Validated batch 112 batch loss 1.42632532\n",
      "Validated batch 113 batch loss 1.33677387\n",
      "Validated batch 114 batch loss 1.45265341\n",
      "Validated batch 115 batch loss 1.430233\n",
      "Validated batch 116 batch loss 1.47123\n",
      "Validated batch 117 batch loss 1.46019316\n",
      "Validated batch 118 batch loss 1.40691626\n",
      "Validated batch 119 batch loss 1.26811123\n",
      "Validated batch 120 batch loss 1.33515441\n",
      "Validated batch 121 batch loss 1.51675272\n",
      "Validated batch 122 batch loss 1.31468439\n",
      "Validated batch 123 batch loss 1.33524752\n",
      "Validated batch 124 batch loss 1.41909826\n",
      "Validated batch 125 batch loss 1.42930889\n",
      "Validated batch 126 batch loss 1.4128468\n",
      "Validated batch 127 batch loss 1.44571114\n",
      "Validated batch 128 batch loss 1.4209764\n",
      "Validated batch 129 batch loss 1.38845825\n",
      "Validated batch 130 batch loss 1.46709514\n",
      "Validated batch 131 batch loss 1.54455316\n",
      "Validated batch 132 batch loss 1.37280476\n",
      "Validated batch 133 batch loss 1.55263829\n",
      "Validated batch 134 batch loss 1.34576082\n",
      "Validated batch 135 batch loss 1.36648667\n",
      "Validated batch 136 batch loss 1.41832006\n",
      "Validated batch 137 batch loss 1.43811798\n",
      "Validated batch 138 batch loss 1.51113248\n",
      "Validated batch 139 batch loss 1.58561802\n",
      "Validated batch 140 batch loss 1.42853248\n",
      "Validated batch 141 batch loss 1.44815242\n",
      "Validated batch 142 batch loss 1.41742146\n",
      "Validated batch 143 batch loss 1.38064408\n",
      "Validated batch 144 batch loss 1.49366224\n",
      "Validated batch 145 batch loss 1.44526196\n",
      "Validated batch 146 batch loss 1.36801434\n",
      "Validated batch 147 batch loss 1.38218606\n",
      "Validated batch 148 batch loss 1.46171\n",
      "Validated batch 149 batch loss 1.42468\n",
      "Validated batch 150 batch loss 1.4543438\n",
      "Validated batch 151 batch loss 1.43523526\n",
      "Validated batch 152 batch loss 1.3715477\n",
      "Validated batch 153 batch loss 1.3978014\n",
      "Validated batch 154 batch loss 1.4322294\n",
      "Validated batch 155 batch loss 1.44385028\n",
      "Validated batch 156 batch loss 1.47352421\n",
      "Validated batch 157 batch loss 1.52178919\n",
      "Validated batch 158 batch loss 1.66029227\n",
      "Validated batch 159 batch loss 1.57079983\n",
      "Validated batch 160 batch loss 1.46162105\n",
      "Validated batch 161 batch loss 1.34414887\n",
      "Validated batch 162 batch loss 1.34482527\n",
      "Validated batch 163 batch loss 1.44878542\n",
      "Validated batch 164 batch loss 1.38620865\n",
      "Validated batch 165 batch loss 1.41524625\n",
      "Validated batch 166 batch loss 1.49261045\n",
      "Validated batch 167 batch loss 1.43834472\n",
      "Validated batch 168 batch loss 1.4798497\n",
      "Validated batch 169 batch loss 1.5166806\n",
      "Validated batch 170 batch loss 1.48905027\n",
      "Validated batch 171 batch loss 1.45292175\n",
      "Validated batch 172 batch loss 1.49828613\n",
      "Validated batch 173 batch loss 1.5218246\n",
      "Validated batch 174 batch loss 1.37543654\n",
      "Validated batch 175 batch loss 1.47456622\n",
      "Validated batch 176 batch loss 1.52334\n",
      "Validated batch 177 batch loss 1.43049598\n",
      "Validated batch 178 batch loss 1.48421323\n",
      "Validated batch 179 batch loss 1.38390529\n",
      "Validated batch 180 batch loss 1.33446717\n",
      "Validated batch 181 batch loss 1.45580721\n",
      "Validated batch 182 batch loss 1.36744416\n",
      "Validated batch 183 batch loss 1.53276098\n",
      "Validated batch 184 batch loss 1.40746665\n",
      "Validated batch 185 batch loss 1.42899168\n",
      "Epoch 1 val loss 1.4370572566986084\n",
      "Model /aiffel/aiffel/mpii/trained/model-epoch-1-loss-1.4371.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.47129798 epoch total loss 1.47129798\n",
      "Trained batch 2 batch loss 1.42781544 epoch total loss 1.44955671\n",
      "Trained batch 3 batch loss 1.4863658 epoch total loss 1.46182632\n",
      "Trained batch 4 batch loss 1.46866393 epoch total loss 1.46353579\n",
      "Trained batch 5 batch loss 1.50268209 epoch total loss 1.47136509\n",
      "Trained batch 6 batch loss 1.50344062 epoch total loss 1.47671092\n",
      "Trained batch 7 batch loss 1.44051147 epoch total loss 1.47153962\n",
      "Trained batch 8 batch loss 1.45458138 epoch total loss 1.46941984\n",
      "Trained batch 9 batch loss 1.35129464 epoch total loss 1.45629478\n",
      "Trained batch 10 batch loss 1.42767847 epoch total loss 1.45343316\n",
      "Trained batch 11 batch loss 1.44693446 epoch total loss 1.45284235\n",
      "Trained batch 12 batch loss 1.53454876 epoch total loss 1.45965135\n",
      "Trained batch 13 batch loss 1.52126431 epoch total loss 1.46439087\n",
      "Trained batch 14 batch loss 1.40215468 epoch total loss 1.45994544\n",
      "Trained batch 15 batch loss 1.46407533 epoch total loss 1.46022069\n",
      "Trained batch 16 batch loss 1.47611547 epoch total loss 1.46121418\n",
      "Trained batch 17 batch loss 1.37869883 epoch total loss 1.45636034\n",
      "Trained batch 18 batch loss 1.34013844 epoch total loss 1.44990349\n",
      "Trained batch 19 batch loss 1.44846368 epoch total loss 1.44982767\n",
      "Trained batch 20 batch loss 1.56108773 epoch total loss 1.45539069\n",
      "Trained batch 21 batch loss 1.53036451 epoch total loss 1.45896089\n",
      "Trained batch 22 batch loss 1.4572134 epoch total loss 1.4588815\n",
      "Trained batch 23 batch loss 1.42033124 epoch total loss 1.45720541\n",
      "Trained batch 24 batch loss 1.45202231 epoch total loss 1.45698941\n",
      "Trained batch 25 batch loss 1.50452089 epoch total loss 1.45889068\n",
      "Trained batch 26 batch loss 1.42737961 epoch total loss 1.45767868\n",
      "Trained batch 27 batch loss 1.44828057 epoch total loss 1.45733058\n",
      "Trained batch 28 batch loss 1.43907356 epoch total loss 1.45667863\n",
      "Trained batch 29 batch loss 1.46191716 epoch total loss 1.45685935\n",
      "Trained batch 30 batch loss 1.43492281 epoch total loss 1.456128\n",
      "Trained batch 31 batch loss 1.47142863 epoch total loss 1.45662165\n",
      "Trained batch 32 batch loss 1.38010085 epoch total loss 1.45423031\n",
      "Trained batch 33 batch loss 1.42810285 epoch total loss 1.45343864\n",
      "Trained batch 34 batch loss 1.5439074 epoch total loss 1.45609951\n",
      "Trained batch 35 batch loss 1.40405798 epoch total loss 1.45461249\n",
      "Trained batch 36 batch loss 1.46392155 epoch total loss 1.45487106\n",
      "Trained batch 37 batch loss 1.38542283 epoch total loss 1.45299411\n",
      "Trained batch 38 batch loss 1.30841982 epoch total loss 1.44918942\n",
      "Trained batch 39 batch loss 1.37432408 epoch total loss 1.4472698\n",
      "Trained batch 40 batch loss 1.36675429 epoch total loss 1.44525695\n",
      "Trained batch 41 batch loss 1.38722479 epoch total loss 1.44384146\n",
      "Trained batch 42 batch loss 1.50529277 epoch total loss 1.44530463\n",
      "Trained batch 43 batch loss 1.5112772 epoch total loss 1.44683886\n",
      "Trained batch 44 batch loss 1.46326399 epoch total loss 1.4472121\n",
      "Trained batch 45 batch loss 1.46906078 epoch total loss 1.44769764\n",
      "Trained batch 46 batch loss 1.39378858 epoch total loss 1.44652569\n",
      "Trained batch 47 batch loss 1.41903257 epoch total loss 1.44594073\n",
      "Trained batch 48 batch loss 1.50300384 epoch total loss 1.44712961\n",
      "Trained batch 49 batch loss 1.46283638 epoch total loss 1.44745016\n",
      "Trained batch 50 batch loss 1.52835965 epoch total loss 1.44906831\n",
      "Trained batch 51 batch loss 1.32528102 epoch total loss 1.44664109\n",
      "Trained batch 52 batch loss 1.40028167 epoch total loss 1.44574952\n",
      "Trained batch 53 batch loss 1.3821367 epoch total loss 1.44454932\n",
      "Trained batch 54 batch loss 1.32102954 epoch total loss 1.44226182\n",
      "Trained batch 55 batch loss 1.4584893 epoch total loss 1.44255686\n",
      "Trained batch 56 batch loss 1.47680962 epoch total loss 1.44316852\n",
      "Trained batch 57 batch loss 1.66596651 epoch total loss 1.44707727\n",
      "Trained batch 58 batch loss 1.57951188 epoch total loss 1.44936073\n",
      "Trained batch 59 batch loss 1.55709946 epoch total loss 1.45118678\n",
      "Trained batch 60 batch loss 1.53438461 epoch total loss 1.45257342\n",
      "Trained batch 61 batch loss 1.47466326 epoch total loss 1.45293546\n",
      "Trained batch 62 batch loss 1.27533221 epoch total loss 1.45007086\n",
      "Trained batch 63 batch loss 1.22779715 epoch total loss 1.44654274\n",
      "Trained batch 64 batch loss 1.17992449 epoch total loss 1.44237685\n",
      "Trained batch 65 batch loss 1.36309874 epoch total loss 1.44115722\n",
      "Trained batch 66 batch loss 1.39027977 epoch total loss 1.4403863\n",
      "Trained batch 67 batch loss 1.55013382 epoch total loss 1.44202435\n",
      "Trained batch 68 batch loss 1.48927426 epoch total loss 1.44271922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 69 batch loss 1.50640881 epoch total loss 1.44364226\n",
      "Trained batch 70 batch loss 1.47838855 epoch total loss 1.44413853\n",
      "Trained batch 71 batch loss 1.48912776 epoch total loss 1.44477224\n",
      "Trained batch 72 batch loss 1.44124985 epoch total loss 1.44472337\n",
      "Trained batch 73 batch loss 1.45420122 epoch total loss 1.44485319\n",
      "Trained batch 74 batch loss 1.44199419 epoch total loss 1.44481456\n",
      "Trained batch 75 batch loss 1.51414442 epoch total loss 1.44573891\n",
      "Trained batch 76 batch loss 1.5478332 epoch total loss 1.44708228\n",
      "Trained batch 77 batch loss 1.49977112 epoch total loss 1.44776654\n",
      "Trained batch 78 batch loss 1.45222855 epoch total loss 1.44782376\n",
      "Trained batch 79 batch loss 1.50024271 epoch total loss 1.44848728\n",
      "Trained batch 80 batch loss 1.41729093 epoch total loss 1.44809735\n",
      "Trained batch 81 batch loss 1.48761737 epoch total loss 1.44858527\n",
      "Trained batch 82 batch loss 1.48401475 epoch total loss 1.44901729\n",
      "Trained batch 83 batch loss 1.5595628 epoch total loss 1.45034921\n",
      "Trained batch 84 batch loss 1.44802201 epoch total loss 1.45032144\n",
      "Trained batch 85 batch loss 1.33761787 epoch total loss 1.44899547\n",
      "Trained batch 86 batch loss 1.27899683 epoch total loss 1.44701886\n",
      "Trained batch 87 batch loss 1.35249829 epoch total loss 1.44593239\n",
      "Trained batch 88 batch loss 1.35154295 epoch total loss 1.44485974\n",
      "Trained batch 89 batch loss 1.36730886 epoch total loss 1.44398844\n",
      "Trained batch 90 batch loss 1.32139051 epoch total loss 1.44262624\n",
      "Trained batch 91 batch loss 1.28054523 epoch total loss 1.44084525\n",
      "Trained batch 92 batch loss 1.3340441 epoch total loss 1.43968427\n",
      "Trained batch 93 batch loss 1.32221961 epoch total loss 1.43842125\n",
      "Trained batch 94 batch loss 1.45473289 epoch total loss 1.4385947\n",
      "Trained batch 95 batch loss 1.36381435 epoch total loss 1.43780756\n",
      "Trained batch 96 batch loss 1.48680854 epoch total loss 1.43831789\n",
      "Trained batch 97 batch loss 1.33590639 epoch total loss 1.43726218\n",
      "Trained batch 98 batch loss 1.41720235 epoch total loss 1.4370575\n",
      "Trained batch 99 batch loss 1.4369024 epoch total loss 1.43705595\n",
      "Trained batch 100 batch loss 1.49044609 epoch total loss 1.43758988\n",
      "Trained batch 101 batch loss 1.44548166 epoch total loss 1.43766797\n",
      "Trained batch 102 batch loss 1.41509223 epoch total loss 1.43744659\n",
      "Trained batch 103 batch loss 1.36307335 epoch total loss 1.43672442\n",
      "Trained batch 104 batch loss 1.36400938 epoch total loss 1.43602538\n",
      "Trained batch 105 batch loss 1.32131219 epoch total loss 1.43493295\n",
      "Trained batch 106 batch loss 1.36142051 epoch total loss 1.43423939\n",
      "Trained batch 107 batch loss 1.35255218 epoch total loss 1.43347597\n",
      "Trained batch 108 batch loss 1.36024761 epoch total loss 1.43279791\n",
      "Trained batch 109 batch loss 1.3447516 epoch total loss 1.43199015\n",
      "Trained batch 110 batch loss 1.38379288 epoch total loss 1.43155193\n",
      "Trained batch 111 batch loss 1.28200555 epoch total loss 1.43020475\n",
      "Trained batch 112 batch loss 1.42115068 epoch total loss 1.43012404\n",
      "Trained batch 113 batch loss 1.35985363 epoch total loss 1.42950213\n",
      "Trained batch 114 batch loss 1.34357882 epoch total loss 1.42874837\n",
      "Trained batch 115 batch loss 1.28942466 epoch total loss 1.42753696\n",
      "Trained batch 116 batch loss 1.3108294 epoch total loss 1.42653096\n",
      "Trained batch 117 batch loss 1.38692927 epoch total loss 1.4261924\n",
      "Trained batch 118 batch loss 1.45886827 epoch total loss 1.42646933\n",
      "Trained batch 119 batch loss 1.40731895 epoch total loss 1.42630839\n",
      "Trained batch 120 batch loss 1.32948399 epoch total loss 1.42550147\n",
      "Trained batch 121 batch loss 1.3316896 epoch total loss 1.42472625\n",
      "Trained batch 122 batch loss 1.41168022 epoch total loss 1.42461932\n",
      "Trained batch 123 batch loss 1.39602888 epoch total loss 1.42438686\n",
      "Trained batch 124 batch loss 1.46818495 epoch total loss 1.42474008\n",
      "Trained batch 125 batch loss 1.43179202 epoch total loss 1.42479646\n",
      "Trained batch 126 batch loss 1.38991201 epoch total loss 1.42451966\n",
      "Trained batch 127 batch loss 1.347857 epoch total loss 1.42391598\n",
      "Trained batch 128 batch loss 1.32244611 epoch total loss 1.42312324\n",
      "Trained batch 129 batch loss 1.37853181 epoch total loss 1.42277753\n",
      "Trained batch 130 batch loss 1.23928559 epoch total loss 1.4213661\n",
      "Trained batch 131 batch loss 1.3548758 epoch total loss 1.4208585\n",
      "Trained batch 132 batch loss 1.32923245 epoch total loss 1.42016435\n",
      "Trained batch 133 batch loss 1.35460305 epoch total loss 1.41967142\n",
      "Trained batch 134 batch loss 1.37880516 epoch total loss 1.41936636\n",
      "Trained batch 135 batch loss 1.41449559 epoch total loss 1.41933024\n",
      "Trained batch 136 batch loss 1.41943884 epoch total loss 1.41933107\n",
      "Trained batch 137 batch loss 1.54297054 epoch total loss 1.42023349\n",
      "Trained batch 138 batch loss 1.4527384 epoch total loss 1.42046905\n",
      "Trained batch 139 batch loss 1.44664979 epoch total loss 1.42065752\n",
      "Trained batch 140 batch loss 1.54145622 epoch total loss 1.42152035\n",
      "Trained batch 141 batch loss 1.47967684 epoch total loss 1.42193282\n",
      "Trained batch 142 batch loss 1.49485159 epoch total loss 1.42244637\n",
      "Trained batch 143 batch loss 1.55229783 epoch total loss 1.42335439\n",
      "Trained batch 144 batch loss 1.46714187 epoch total loss 1.42365849\n",
      "Trained batch 145 batch loss 1.53542578 epoch total loss 1.4244293\n",
      "Trained batch 146 batch loss 1.3888849 epoch total loss 1.42418587\n",
      "Trained batch 147 batch loss 1.51086807 epoch total loss 1.42477548\n",
      "Trained batch 148 batch loss 1.58400464 epoch total loss 1.42585135\n",
      "Trained batch 149 batch loss 1.61112499 epoch total loss 1.42709482\n",
      "Trained batch 150 batch loss 1.45075595 epoch total loss 1.42725265\n",
      "Trained batch 151 batch loss 1.48398137 epoch total loss 1.42762828\n",
      "Trained batch 152 batch loss 1.42102885 epoch total loss 1.42758489\n",
      "Trained batch 153 batch loss 1.30636883 epoch total loss 1.42679262\n",
      "Trained batch 154 batch loss 1.43713427 epoch total loss 1.42685974\n",
      "Trained batch 155 batch loss 1.51191151 epoch total loss 1.42740858\n",
      "Trained batch 156 batch loss 1.3231833 epoch total loss 1.42674041\n",
      "Trained batch 157 batch loss 1.40329432 epoch total loss 1.42659104\n",
      "Trained batch 158 batch loss 1.39915621 epoch total loss 1.42641735\n",
      "Trained batch 159 batch loss 1.47619152 epoch total loss 1.42673051\n",
      "Trained batch 160 batch loss 1.48365688 epoch total loss 1.42708623\n",
      "Trained batch 161 batch loss 1.31289387 epoch total loss 1.42637706\n",
      "Trained batch 162 batch loss 1.21039057 epoch total loss 1.4250437\n",
      "Trained batch 163 batch loss 1.26867628 epoch total loss 1.42408442\n",
      "Trained batch 164 batch loss 1.36966562 epoch total loss 1.42375255\n",
      "Trained batch 165 batch loss 1.35657501 epoch total loss 1.42334545\n",
      "Trained batch 166 batch loss 1.29582763 epoch total loss 1.42257714\n",
      "Trained batch 167 batch loss 1.41843307 epoch total loss 1.42255235\n",
      "Trained batch 168 batch loss 1.35907829 epoch total loss 1.42217457\n",
      "Trained batch 169 batch loss 1.46077728 epoch total loss 1.42240298\n",
      "Trained batch 170 batch loss 1.51000893 epoch total loss 1.42291832\n",
      "Trained batch 171 batch loss 1.44303727 epoch total loss 1.42303598\n",
      "Trained batch 172 batch loss 1.74932241 epoch total loss 1.42493308\n",
      "Trained batch 173 batch loss 1.6813556 epoch total loss 1.42641521\n",
      "Trained batch 174 batch loss 1.48556888 epoch total loss 1.42675519\n",
      "Trained batch 175 batch loss 1.52298653 epoch total loss 1.42730498\n",
      "Trained batch 176 batch loss 1.42027533 epoch total loss 1.42726505\n",
      "Trained batch 177 batch loss 1.47834277 epoch total loss 1.42755365\n",
      "Trained batch 178 batch loss 1.49973822 epoch total loss 1.4279592\n",
      "Trained batch 179 batch loss 1.51983809 epoch total loss 1.42847252\n",
      "Trained batch 180 batch loss 1.35804033 epoch total loss 1.42808127\n",
      "Trained batch 181 batch loss 1.50775599 epoch total loss 1.42852139\n",
      "Trained batch 182 batch loss 1.49198961 epoch total loss 1.4288702\n",
      "Trained batch 183 batch loss 1.51896381 epoch total loss 1.42936254\n",
      "Trained batch 184 batch loss 1.46388066 epoch total loss 1.42955\n",
      "Trained batch 185 batch loss 1.34197211 epoch total loss 1.42907667\n",
      "Trained batch 186 batch loss 1.3650918 epoch total loss 1.42873263\n",
      "Trained batch 187 batch loss 1.37662852 epoch total loss 1.42845392\n",
      "Trained batch 188 batch loss 1.44296253 epoch total loss 1.42853105\n",
      "Trained batch 189 batch loss 1.43439317 epoch total loss 1.42856205\n",
      "Trained batch 190 batch loss 1.4403019 epoch total loss 1.42862391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 191 batch loss 1.34680915 epoch total loss 1.42819548\n",
      "Trained batch 192 batch loss 1.3594799 epoch total loss 1.42783749\n",
      "Trained batch 193 batch loss 1.34785569 epoch total loss 1.42742324\n",
      "Trained batch 194 batch loss 1.35999525 epoch total loss 1.42707562\n",
      "Trained batch 195 batch loss 1.39676833 epoch total loss 1.42692\n",
      "Trained batch 196 batch loss 1.38838983 epoch total loss 1.4267236\n",
      "Trained batch 197 batch loss 1.37724626 epoch total loss 1.42647243\n",
      "Trained batch 198 batch loss 1.34553576 epoch total loss 1.42606378\n",
      "Trained batch 199 batch loss 1.28374314 epoch total loss 1.42534864\n",
      "Trained batch 200 batch loss 1.40125859 epoch total loss 1.42522812\n",
      "Trained batch 201 batch loss 1.44215703 epoch total loss 1.4253124\n",
      "Trained batch 202 batch loss 1.48797989 epoch total loss 1.42562258\n",
      "Trained batch 203 batch loss 1.30126488 epoch total loss 1.42501009\n",
      "Trained batch 204 batch loss 1.4131403 epoch total loss 1.42495191\n",
      "Trained batch 205 batch loss 1.38465023 epoch total loss 1.42475522\n",
      "Trained batch 206 batch loss 1.44591594 epoch total loss 1.42485797\n",
      "Trained batch 207 batch loss 1.35171819 epoch total loss 1.42450464\n",
      "Trained batch 208 batch loss 1.38663292 epoch total loss 1.42432261\n",
      "Trained batch 209 batch loss 1.33161592 epoch total loss 1.42387891\n",
      "Trained batch 210 batch loss 1.46047699 epoch total loss 1.42405319\n",
      "Trained batch 211 batch loss 1.38200712 epoch total loss 1.42385399\n",
      "Trained batch 212 batch loss 1.461097 epoch total loss 1.42402971\n",
      "Trained batch 213 batch loss 1.42107081 epoch total loss 1.42401576\n",
      "Trained batch 214 batch loss 1.46414042 epoch total loss 1.42420328\n",
      "Trained batch 215 batch loss 1.48096371 epoch total loss 1.42446733\n",
      "Trained batch 216 batch loss 1.37709832 epoch total loss 1.42424798\n",
      "Trained batch 217 batch loss 1.49095893 epoch total loss 1.42455554\n",
      "Trained batch 218 batch loss 1.53771269 epoch total loss 1.42507458\n",
      "Trained batch 219 batch loss 1.35801911 epoch total loss 1.42476845\n",
      "Trained batch 220 batch loss 1.21061563 epoch total loss 1.42379498\n",
      "Trained batch 221 batch loss 1.24784613 epoch total loss 1.42299879\n",
      "Trained batch 222 batch loss 1.45709383 epoch total loss 1.42315233\n",
      "Trained batch 223 batch loss 1.46191788 epoch total loss 1.42332613\n",
      "Trained batch 224 batch loss 1.48753023 epoch total loss 1.42361271\n",
      "Trained batch 225 batch loss 1.36750484 epoch total loss 1.42336333\n",
      "Trained batch 226 batch loss 1.36631846 epoch total loss 1.42311096\n",
      "Trained batch 227 batch loss 1.50882578 epoch total loss 1.4234885\n",
      "Trained batch 228 batch loss 1.39595747 epoch total loss 1.42336786\n",
      "Trained batch 229 batch loss 1.40390408 epoch total loss 1.42328286\n",
      "Trained batch 230 batch loss 1.2744205 epoch total loss 1.42263556\n",
      "Trained batch 231 batch loss 1.32684529 epoch total loss 1.42222083\n",
      "Trained batch 232 batch loss 1.42799842 epoch total loss 1.42224586\n",
      "Trained batch 233 batch loss 1.40183401 epoch total loss 1.42215812\n",
      "Trained batch 234 batch loss 1.4292562 epoch total loss 1.42218852\n",
      "Trained batch 235 batch loss 1.4156456 epoch total loss 1.42216074\n",
      "Trained batch 236 batch loss 1.42760289 epoch total loss 1.42218375\n",
      "Trained batch 237 batch loss 1.43659031 epoch total loss 1.42224455\n",
      "Trained batch 238 batch loss 1.43874288 epoch total loss 1.42231393\n",
      "Trained batch 239 batch loss 1.40576148 epoch total loss 1.42224467\n",
      "Trained batch 240 batch loss 1.38707066 epoch total loss 1.42209816\n",
      "Trained batch 241 batch loss 1.43326545 epoch total loss 1.42214441\n",
      "Trained batch 242 batch loss 1.48366427 epoch total loss 1.42239869\n",
      "Trained batch 243 batch loss 1.38667965 epoch total loss 1.4222517\n",
      "Trained batch 244 batch loss 1.44724107 epoch total loss 1.4223541\n",
      "Trained batch 245 batch loss 1.33379269 epoch total loss 1.42199266\n",
      "Trained batch 246 batch loss 1.37629139 epoch total loss 1.42180693\n",
      "Trained batch 247 batch loss 1.46154261 epoch total loss 1.42196774\n",
      "Trained batch 248 batch loss 1.43880081 epoch total loss 1.42203569\n",
      "Trained batch 249 batch loss 1.52963972 epoch total loss 1.42246783\n",
      "Trained batch 250 batch loss 1.33144271 epoch total loss 1.42210376\n",
      "Trained batch 251 batch loss 1.30271804 epoch total loss 1.42162812\n",
      "Trained batch 252 batch loss 1.39082563 epoch total loss 1.42150593\n",
      "Trained batch 253 batch loss 1.43001294 epoch total loss 1.42153955\n",
      "Trained batch 254 batch loss 1.49009454 epoch total loss 1.42180943\n",
      "Trained batch 255 batch loss 1.49331439 epoch total loss 1.42208982\n",
      "Trained batch 256 batch loss 1.54125 epoch total loss 1.42255533\n",
      "Trained batch 257 batch loss 1.52039969 epoch total loss 1.42293596\n",
      "Trained batch 258 batch loss 1.4952811 epoch total loss 1.42321634\n",
      "Trained batch 259 batch loss 1.4249599 epoch total loss 1.42322302\n",
      "Trained batch 260 batch loss 1.4055891 epoch total loss 1.42315519\n",
      "Trained batch 261 batch loss 1.36371708 epoch total loss 1.4229275\n",
      "Trained batch 262 batch loss 1.50021672 epoch total loss 1.42322242\n",
      "Trained batch 263 batch loss 1.46759295 epoch total loss 1.4233911\n",
      "Trained batch 264 batch loss 1.53036904 epoch total loss 1.4237963\n",
      "Trained batch 265 batch loss 1.48595238 epoch total loss 1.4240309\n",
      "Trained batch 266 batch loss 1.45356953 epoch total loss 1.424142\n",
      "Trained batch 267 batch loss 1.41971099 epoch total loss 1.42412543\n",
      "Trained batch 268 batch loss 1.50231719 epoch total loss 1.42441714\n",
      "Trained batch 269 batch loss 1.45564568 epoch total loss 1.42453337\n",
      "Trained batch 270 batch loss 1.44220591 epoch total loss 1.42459869\n",
      "Trained batch 271 batch loss 1.51521349 epoch total loss 1.4249332\n",
      "Trained batch 272 batch loss 1.47356904 epoch total loss 1.42511201\n",
      "Trained batch 273 batch loss 1.38686395 epoch total loss 1.42497194\n",
      "Trained batch 274 batch loss 1.52615774 epoch total loss 1.42534125\n",
      "Trained batch 275 batch loss 1.41871142 epoch total loss 1.42531705\n",
      "Trained batch 276 batch loss 1.44297981 epoch total loss 1.42538106\n",
      "Trained batch 277 batch loss 1.41377878 epoch total loss 1.42533922\n",
      "Trained batch 278 batch loss 1.51072407 epoch total loss 1.42564631\n",
      "Trained batch 279 batch loss 1.38276947 epoch total loss 1.42549264\n",
      "Trained batch 280 batch loss 1.33174145 epoch total loss 1.4251579\n",
      "Trained batch 281 batch loss 1.27984405 epoch total loss 1.42464077\n",
      "Trained batch 282 batch loss 1.32914662 epoch total loss 1.4243021\n",
      "Trained batch 283 batch loss 1.37733793 epoch total loss 1.42413616\n",
      "Trained batch 284 batch loss 1.32888877 epoch total loss 1.42380083\n",
      "Trained batch 285 batch loss 1.33879209 epoch total loss 1.42350256\n",
      "Trained batch 286 batch loss 1.37021732 epoch total loss 1.42331624\n",
      "Trained batch 287 batch loss 1.41209126 epoch total loss 1.42327714\n",
      "Trained batch 288 batch loss 1.54719019 epoch total loss 1.42370737\n",
      "Trained batch 289 batch loss 1.72969854 epoch total loss 1.42476618\n",
      "Trained batch 290 batch loss 1.44846225 epoch total loss 1.42484784\n",
      "Trained batch 291 batch loss 1.57745433 epoch total loss 1.42537224\n",
      "Trained batch 292 batch loss 1.54341602 epoch total loss 1.4257766\n",
      "Trained batch 293 batch loss 1.50984418 epoch total loss 1.42606354\n",
      "Trained batch 294 batch loss 1.41227198 epoch total loss 1.42601657\n",
      "Trained batch 295 batch loss 1.48035169 epoch total loss 1.42620075\n",
      "Trained batch 296 batch loss 1.42334127 epoch total loss 1.42619109\n",
      "Trained batch 297 batch loss 1.41173482 epoch total loss 1.42614245\n",
      "Trained batch 298 batch loss 1.46444654 epoch total loss 1.42627096\n",
      "Trained batch 299 batch loss 1.54700863 epoch total loss 1.42667472\n",
      "Trained batch 300 batch loss 1.45043635 epoch total loss 1.426754\n",
      "Trained batch 301 batch loss 1.38687289 epoch total loss 1.42662144\n",
      "Trained batch 302 batch loss 1.38586164 epoch total loss 1.42648649\n",
      "Trained batch 303 batch loss 1.39415979 epoch total loss 1.4263798\n",
      "Trained batch 304 batch loss 1.43449855 epoch total loss 1.4264065\n",
      "Trained batch 305 batch loss 1.51920414 epoch total loss 1.42671084\n",
      "Trained batch 306 batch loss 1.43092108 epoch total loss 1.42672455\n",
      "Trained batch 307 batch loss 1.36738431 epoch total loss 1.4265312\n",
      "Trained batch 308 batch loss 1.29066157 epoch total loss 1.42609\n",
      "Trained batch 309 batch loss 1.40611529 epoch total loss 1.42602539\n",
      "Trained batch 310 batch loss 1.38903582 epoch total loss 1.42590606\n",
      "Trained batch 311 batch loss 1.3748045 epoch total loss 1.42574179\n",
      "Trained batch 312 batch loss 1.41730189 epoch total loss 1.42571473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 313 batch loss 1.49217641 epoch total loss 1.42592716\n",
      "Trained batch 314 batch loss 1.30218887 epoch total loss 1.42553306\n",
      "Trained batch 315 batch loss 1.36958766 epoch total loss 1.42535543\n",
      "Trained batch 316 batch loss 1.46350574 epoch total loss 1.42547619\n",
      "Trained batch 317 batch loss 1.55199 epoch total loss 1.42587531\n",
      "Trained batch 318 batch loss 1.59041071 epoch total loss 1.42639279\n",
      "Trained batch 319 batch loss 1.54069185 epoch total loss 1.42675102\n",
      "Trained batch 320 batch loss 1.44397521 epoch total loss 1.42680478\n",
      "Trained batch 321 batch loss 1.26061249 epoch total loss 1.42628717\n",
      "Trained batch 322 batch loss 1.43002272 epoch total loss 1.42629874\n",
      "Trained batch 323 batch loss 1.61874497 epoch total loss 1.42689455\n",
      "Trained batch 324 batch loss 1.57199383 epoch total loss 1.42734241\n",
      "Trained batch 325 batch loss 1.52841818 epoch total loss 1.42765331\n",
      "Trained batch 326 batch loss 1.42649364 epoch total loss 1.42764974\n",
      "Trained batch 327 batch loss 1.3449955 epoch total loss 1.42739701\n",
      "Trained batch 328 batch loss 1.46435428 epoch total loss 1.42750967\n",
      "Trained batch 329 batch loss 1.33261108 epoch total loss 1.42722118\n",
      "Trained batch 330 batch loss 1.31294227 epoch total loss 1.42687488\n",
      "Trained batch 331 batch loss 1.43597579 epoch total loss 1.42690241\n",
      "Trained batch 332 batch loss 1.38441634 epoch total loss 1.4267745\n",
      "Trained batch 333 batch loss 1.31459534 epoch total loss 1.42643762\n",
      "Trained batch 334 batch loss 1.30425346 epoch total loss 1.42607176\n",
      "Trained batch 335 batch loss 1.28714633 epoch total loss 1.42565703\n",
      "Trained batch 336 batch loss 1.34784555 epoch total loss 1.42542553\n",
      "Trained batch 337 batch loss 1.27521479 epoch total loss 1.42497969\n",
      "Trained batch 338 batch loss 1.23349667 epoch total loss 1.4244132\n",
      "Trained batch 339 batch loss 1.33227777 epoch total loss 1.42414141\n",
      "Trained batch 340 batch loss 1.37277853 epoch total loss 1.42399037\n",
      "Trained batch 341 batch loss 1.39250231 epoch total loss 1.42389798\n",
      "Trained batch 342 batch loss 1.36283267 epoch total loss 1.42371941\n",
      "Trained batch 343 batch loss 1.48094511 epoch total loss 1.4238863\n",
      "Trained batch 344 batch loss 1.49724436 epoch total loss 1.42409956\n",
      "Trained batch 345 batch loss 1.43660629 epoch total loss 1.42413592\n",
      "Trained batch 346 batch loss 1.46526313 epoch total loss 1.42425478\n",
      "Trained batch 347 batch loss 1.45243609 epoch total loss 1.42433596\n",
      "Trained batch 348 batch loss 1.46024156 epoch total loss 1.42443907\n",
      "Trained batch 349 batch loss 1.37261784 epoch total loss 1.42429066\n",
      "Trained batch 350 batch loss 1.4816041 epoch total loss 1.42445433\n",
      "Trained batch 351 batch loss 1.42008042 epoch total loss 1.42444181\n",
      "Trained batch 352 batch loss 1.42429435 epoch total loss 1.42444146\n",
      "Trained batch 353 batch loss 1.40545142 epoch total loss 1.42438769\n",
      "Trained batch 354 batch loss 1.38419783 epoch total loss 1.42427409\n",
      "Trained batch 355 batch loss 1.39421272 epoch total loss 1.42418945\n",
      "Trained batch 356 batch loss 1.47262132 epoch total loss 1.42432547\n",
      "Trained batch 357 batch loss 1.51829135 epoch total loss 1.42458868\n",
      "Trained batch 358 batch loss 1.45481157 epoch total loss 1.42467308\n",
      "Trained batch 359 batch loss 1.52424383 epoch total loss 1.42495036\n",
      "Trained batch 360 batch loss 1.3703568 epoch total loss 1.42479873\n",
      "Trained batch 361 batch loss 1.37450135 epoch total loss 1.42465949\n",
      "Trained batch 362 batch loss 1.37592685 epoch total loss 1.42452478\n",
      "Trained batch 363 batch loss 1.42766428 epoch total loss 1.42453349\n",
      "Trained batch 364 batch loss 1.42242813 epoch total loss 1.42452765\n",
      "Trained batch 365 batch loss 1.44697738 epoch total loss 1.42458916\n",
      "Trained batch 366 batch loss 1.39398611 epoch total loss 1.42450547\n",
      "Trained batch 367 batch loss 1.37741196 epoch total loss 1.42437732\n",
      "Trained batch 368 batch loss 1.37182 epoch total loss 1.42423451\n",
      "Trained batch 369 batch loss 1.40855098 epoch total loss 1.42419207\n",
      "Trained batch 370 batch loss 1.26541162 epoch total loss 1.42376292\n",
      "Trained batch 371 batch loss 1.41592717 epoch total loss 1.42374194\n",
      "Trained batch 372 batch loss 1.37403023 epoch total loss 1.4236083\n",
      "Trained batch 373 batch loss 1.45984578 epoch total loss 1.42370546\n",
      "Trained batch 374 batch loss 1.3711462 epoch total loss 1.42356491\n",
      "Trained batch 375 batch loss 1.43388605 epoch total loss 1.42359245\n",
      "Trained batch 376 batch loss 1.36409378 epoch total loss 1.42343414\n",
      "Trained batch 377 batch loss 1.51067078 epoch total loss 1.42366552\n",
      "Trained batch 378 batch loss 1.44572413 epoch total loss 1.42372394\n",
      "Trained batch 379 batch loss 1.40972495 epoch total loss 1.4236871\n",
      "Trained batch 380 batch loss 1.43432546 epoch total loss 1.423715\n",
      "Trained batch 381 batch loss 1.38343191 epoch total loss 1.42360926\n",
      "Trained batch 382 batch loss 1.3569026 epoch total loss 1.42343462\n",
      "Trained batch 383 batch loss 1.37260842 epoch total loss 1.42330194\n",
      "Trained batch 384 batch loss 1.38393855 epoch total loss 1.4231993\n",
      "Trained batch 385 batch loss 1.43496525 epoch total loss 1.42322981\n",
      "Trained batch 386 batch loss 1.40721667 epoch total loss 1.42318833\n",
      "Trained batch 387 batch loss 1.48466182 epoch total loss 1.42334723\n",
      "Trained batch 388 batch loss 1.31439507 epoch total loss 1.4230665\n",
      "Trained batch 389 batch loss 1.31545472 epoch total loss 1.42278969\n",
      "Trained batch 390 batch loss 1.41316938 epoch total loss 1.42276502\n",
      "Trained batch 391 batch loss 1.44047964 epoch total loss 1.42281032\n",
      "Trained batch 392 batch loss 1.31213987 epoch total loss 1.42252803\n",
      "Trained batch 393 batch loss 1.40116835 epoch total loss 1.42247367\n",
      "Trained batch 394 batch loss 1.41723394 epoch total loss 1.42246044\n",
      "Trained batch 395 batch loss 1.35827255 epoch total loss 1.42229795\n",
      "Trained batch 396 batch loss 1.30250669 epoch total loss 1.4219954\n",
      "Trained batch 397 batch loss 1.4595319 epoch total loss 1.42208993\n",
      "Trained batch 398 batch loss 1.4195745 epoch total loss 1.42208362\n",
      "Trained batch 399 batch loss 1.50246859 epoch total loss 1.42228496\n",
      "Trained batch 400 batch loss 1.36112261 epoch total loss 1.42213213\n",
      "Trained batch 401 batch loss 1.35615993 epoch total loss 1.42196751\n",
      "Trained batch 402 batch loss 1.3399601 epoch total loss 1.42176354\n",
      "Trained batch 403 batch loss 1.31673884 epoch total loss 1.42150283\n",
      "Trained batch 404 batch loss 1.24342716 epoch total loss 1.42106211\n",
      "Trained batch 405 batch loss 1.31686234 epoch total loss 1.42080474\n",
      "Trained batch 406 batch loss 1.30862498 epoch total loss 1.42052841\n",
      "Trained batch 407 batch loss 1.30056334 epoch total loss 1.42023361\n",
      "Trained batch 408 batch loss 1.39549661 epoch total loss 1.42017305\n",
      "Trained batch 409 batch loss 1.42837524 epoch total loss 1.4201932\n",
      "Trained batch 410 batch loss 1.50314701 epoch total loss 1.42039561\n",
      "Trained batch 411 batch loss 1.42396653 epoch total loss 1.4204042\n",
      "Trained batch 412 batch loss 1.33494639 epoch total loss 1.42019677\n",
      "Trained batch 413 batch loss 1.41904902 epoch total loss 1.42019415\n",
      "Trained batch 414 batch loss 1.31875265 epoch total loss 1.41994905\n",
      "Trained batch 415 batch loss 1.36128139 epoch total loss 1.41980755\n",
      "Trained batch 416 batch loss 1.31810009 epoch total loss 1.41956317\n",
      "Trained batch 417 batch loss 1.38899124 epoch total loss 1.41948974\n",
      "Trained batch 418 batch loss 1.2622081 epoch total loss 1.41911352\n",
      "Trained batch 419 batch loss 1.33310544 epoch total loss 1.41890836\n",
      "Trained batch 420 batch loss 1.31391728 epoch total loss 1.41865826\n",
      "Trained batch 421 batch loss 1.36422682 epoch total loss 1.41852891\n",
      "Trained batch 422 batch loss 1.40103567 epoch total loss 1.41848755\n",
      "Trained batch 423 batch loss 1.35430169 epoch total loss 1.4183358\n",
      "Trained batch 424 batch loss 1.27897298 epoch total loss 1.41800714\n",
      "Trained batch 425 batch loss 1.41639876 epoch total loss 1.41800332\n",
      "Trained batch 426 batch loss 1.40523648 epoch total loss 1.41797328\n",
      "Trained batch 427 batch loss 1.58763409 epoch total loss 1.41837072\n",
      "Trained batch 428 batch loss 1.4279294 epoch total loss 1.41839302\n",
      "Trained batch 429 batch loss 1.56081033 epoch total loss 1.41872489\n",
      "Trained batch 430 batch loss 1.5018059 epoch total loss 1.41891825\n",
      "Trained batch 431 batch loss 1.55028117 epoch total loss 1.41922307\n",
      "Trained batch 432 batch loss 1.50589681 epoch total loss 1.4194237\n",
      "Trained batch 433 batch loss 1.36594868 epoch total loss 1.4193002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 434 batch loss 1.29011011 epoch total loss 1.41900253\n",
      "Trained batch 435 batch loss 1.38706064 epoch total loss 1.41892922\n",
      "Trained batch 436 batch loss 1.33325648 epoch total loss 1.41873264\n",
      "Trained batch 437 batch loss 1.37117219 epoch total loss 1.41862381\n",
      "Trained batch 438 batch loss 1.48143792 epoch total loss 1.41876721\n",
      "Trained batch 439 batch loss 1.48880637 epoch total loss 1.41892684\n",
      "Trained batch 440 batch loss 1.47957575 epoch total loss 1.41906464\n",
      "Trained batch 441 batch loss 1.39914298 epoch total loss 1.41901946\n",
      "Trained batch 442 batch loss 1.34097779 epoch total loss 1.41884303\n",
      "Trained batch 443 batch loss 1.22534168 epoch total loss 1.41840625\n",
      "Trained batch 444 batch loss 1.4067862 epoch total loss 1.41838\n",
      "Trained batch 445 batch loss 1.32408941 epoch total loss 1.41816819\n",
      "Trained batch 446 batch loss 1.41250336 epoch total loss 1.41815543\n",
      "Trained batch 447 batch loss 1.46657979 epoch total loss 1.41826367\n",
      "Trained batch 448 batch loss 1.49289894 epoch total loss 1.41843033\n",
      "Trained batch 449 batch loss 1.40132117 epoch total loss 1.41839218\n",
      "Trained batch 450 batch loss 1.42690182 epoch total loss 1.41841102\n",
      "Trained batch 451 batch loss 1.3682456 epoch total loss 1.41829979\n",
      "Trained batch 452 batch loss 1.42787993 epoch total loss 1.41832089\n",
      "Trained batch 453 batch loss 1.28896701 epoch total loss 1.41803527\n",
      "Trained batch 454 batch loss 1.3849504 epoch total loss 1.41796243\n",
      "Trained batch 455 batch loss 1.55563605 epoch total loss 1.4182651\n",
      "Trained batch 456 batch loss 1.44410062 epoch total loss 1.41832173\n",
      "Trained batch 457 batch loss 1.40346909 epoch total loss 1.41828918\n",
      "Trained batch 458 batch loss 1.26164603 epoch total loss 1.41794717\n",
      "Trained batch 459 batch loss 1.25024724 epoch total loss 1.4175818\n",
      "Trained batch 460 batch loss 1.19560075 epoch total loss 1.41709924\n",
      "Trained batch 461 batch loss 1.30684757 epoch total loss 1.4168601\n",
      "Trained batch 462 batch loss 1.38409388 epoch total loss 1.41678917\n",
      "Trained batch 463 batch loss 1.46825624 epoch total loss 1.41690028\n",
      "Trained batch 464 batch loss 1.63938618 epoch total loss 1.41737986\n",
      "Trained batch 465 batch loss 1.40872955 epoch total loss 1.41736126\n",
      "Trained batch 466 batch loss 1.47146738 epoch total loss 1.41747749\n",
      "Trained batch 467 batch loss 1.3472091 epoch total loss 1.41732705\n",
      "Trained batch 468 batch loss 1.44393587 epoch total loss 1.41738379\n",
      "Trained batch 469 batch loss 1.47600663 epoch total loss 1.41750884\n",
      "Trained batch 470 batch loss 1.54940963 epoch total loss 1.41778958\n",
      "Trained batch 471 batch loss 1.43547058 epoch total loss 1.41782713\n",
      "Trained batch 472 batch loss 1.56697011 epoch total loss 1.41814303\n",
      "Trained batch 473 batch loss 1.42959642 epoch total loss 1.41816735\n",
      "Trained batch 474 batch loss 1.47915924 epoch total loss 1.4182961\n",
      "Trained batch 475 batch loss 1.4208802 epoch total loss 1.41830158\n",
      "Trained batch 476 batch loss 1.29794323 epoch total loss 1.41804874\n",
      "Trained batch 477 batch loss 1.27128375 epoch total loss 1.41774106\n",
      "Trained batch 478 batch loss 1.39413261 epoch total loss 1.41769171\n",
      "Trained batch 479 batch loss 1.49374807 epoch total loss 1.41785049\n",
      "Trained batch 480 batch loss 1.54430056 epoch total loss 1.41811395\n",
      "Trained batch 481 batch loss 1.49635351 epoch total loss 1.41827655\n",
      "Trained batch 482 batch loss 1.52215815 epoch total loss 1.41849208\n",
      "Trained batch 483 batch loss 1.42722619 epoch total loss 1.4185102\n",
      "Trained batch 484 batch loss 1.4184351 epoch total loss 1.41851008\n",
      "Trained batch 485 batch loss 1.34498572 epoch total loss 1.41835845\n",
      "Trained batch 486 batch loss 1.40314436 epoch total loss 1.41832721\n",
      "Trained batch 487 batch loss 1.43721986 epoch total loss 1.41836596\n",
      "Trained batch 488 batch loss 1.43516195 epoch total loss 1.41840041\n",
      "Trained batch 489 batch loss 1.38065672 epoch total loss 1.41832328\n",
      "Trained batch 490 batch loss 1.32234943 epoch total loss 1.4181273\n",
      "Trained batch 491 batch loss 1.30400825 epoch total loss 1.41789496\n",
      "Trained batch 492 batch loss 1.35493672 epoch total loss 1.41776693\n",
      "Trained batch 493 batch loss 1.48160052 epoch total loss 1.41789639\n",
      "Trained batch 494 batch loss 1.66763425 epoch total loss 1.41840208\n",
      "Trained batch 495 batch loss 1.6130029 epoch total loss 1.41879511\n",
      "Trained batch 496 batch loss 1.54129899 epoch total loss 1.41904211\n",
      "Trained batch 497 batch loss 1.470649 epoch total loss 1.41914594\n",
      "Trained batch 498 batch loss 1.63083768 epoch total loss 1.41957116\n",
      "Trained batch 499 batch loss 1.55343962 epoch total loss 1.41983938\n",
      "Trained batch 500 batch loss 1.55610454 epoch total loss 1.42011189\n",
      "Trained batch 501 batch loss 1.50374854 epoch total loss 1.42027879\n",
      "Trained batch 502 batch loss 1.52335882 epoch total loss 1.42048419\n",
      "Trained batch 503 batch loss 1.36585331 epoch total loss 1.42037559\n",
      "Trained batch 504 batch loss 1.48187637 epoch total loss 1.42049754\n",
      "Trained batch 505 batch loss 1.37298715 epoch total loss 1.42040348\n",
      "Trained batch 506 batch loss 1.37054753 epoch total loss 1.42030501\n",
      "Trained batch 507 batch loss 1.35907638 epoch total loss 1.42018414\n",
      "Trained batch 508 batch loss 1.44262612 epoch total loss 1.42022836\n",
      "Trained batch 509 batch loss 1.45882082 epoch total loss 1.42030418\n",
      "Trained batch 510 batch loss 1.34798229 epoch total loss 1.42016232\n",
      "Trained batch 511 batch loss 1.44410682 epoch total loss 1.42020917\n",
      "Trained batch 512 batch loss 1.35059679 epoch total loss 1.42007315\n",
      "Trained batch 513 batch loss 1.35820365 epoch total loss 1.41995251\n",
      "Trained batch 514 batch loss 1.36300135 epoch total loss 1.41984177\n",
      "Trained batch 515 batch loss 1.43165135 epoch total loss 1.41986465\n",
      "Trained batch 516 batch loss 1.36358523 epoch total loss 1.41975558\n",
      "Trained batch 517 batch loss 1.29002964 epoch total loss 1.41950464\n",
      "Trained batch 518 batch loss 1.33217287 epoch total loss 1.41933608\n",
      "Trained batch 519 batch loss 1.35243833 epoch total loss 1.4192071\n",
      "Trained batch 520 batch loss 1.48713076 epoch total loss 1.41933775\n",
      "Trained batch 521 batch loss 1.46549749 epoch total loss 1.41942632\n",
      "Trained batch 522 batch loss 1.44114089 epoch total loss 1.41946793\n",
      "Trained batch 523 batch loss 1.3367151 epoch total loss 1.41930974\n",
      "Trained batch 524 batch loss 1.49288583 epoch total loss 1.41945016\n",
      "Trained batch 525 batch loss 1.50568223 epoch total loss 1.41961432\n",
      "Trained batch 526 batch loss 1.36618757 epoch total loss 1.41951287\n",
      "Trained batch 527 batch loss 1.42580557 epoch total loss 1.41952479\n",
      "Trained batch 528 batch loss 1.39907134 epoch total loss 1.41948593\n",
      "Trained batch 529 batch loss 1.32833135 epoch total loss 1.41931355\n",
      "Trained batch 530 batch loss 1.34976125 epoch total loss 1.4191823\n",
      "Trained batch 531 batch loss 1.42835391 epoch total loss 1.41919959\n",
      "Trained batch 532 batch loss 1.47603631 epoch total loss 1.4193064\n",
      "Trained batch 533 batch loss 1.48338592 epoch total loss 1.41942656\n",
      "Trained batch 534 batch loss 1.43060291 epoch total loss 1.41944754\n",
      "Trained batch 535 batch loss 1.46668434 epoch total loss 1.41953576\n",
      "Trained batch 536 batch loss 1.44169712 epoch total loss 1.41957724\n",
      "Trained batch 537 batch loss 1.40841389 epoch total loss 1.41955638\n",
      "Trained batch 538 batch loss 1.46185589 epoch total loss 1.41963494\n",
      "Trained batch 539 batch loss 1.41183627 epoch total loss 1.41962051\n",
      "Trained batch 540 batch loss 1.40884459 epoch total loss 1.41960061\n",
      "Trained batch 541 batch loss 1.29181147 epoch total loss 1.41936445\n",
      "Trained batch 542 batch loss 1.49272466 epoch total loss 1.41949975\n",
      "Trained batch 543 batch loss 1.46095908 epoch total loss 1.41957617\n",
      "Trained batch 544 batch loss 1.33524966 epoch total loss 1.4194212\n",
      "Trained batch 545 batch loss 1.43301284 epoch total loss 1.41944599\n",
      "Trained batch 546 batch loss 1.38271523 epoch total loss 1.41937876\n",
      "Trained batch 547 batch loss 1.24036872 epoch total loss 1.41905141\n",
      "Trained batch 548 batch loss 1.36178815 epoch total loss 1.41894698\n",
      "Trained batch 549 batch loss 1.2469641 epoch total loss 1.4186337\n",
      "Trained batch 550 batch loss 1.4466306 epoch total loss 1.4186846\n",
      "Trained batch 551 batch loss 1.43230033 epoch total loss 1.4187094\n",
      "Trained batch 552 batch loss 1.48517907 epoch total loss 1.4188298\n",
      "Trained batch 553 batch loss 1.44826484 epoch total loss 1.41888297\n",
      "Trained batch 554 batch loss 1.31930327 epoch total loss 1.4187032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 555 batch loss 1.3466574 epoch total loss 1.41857338\n",
      "Trained batch 556 batch loss 1.37859893 epoch total loss 1.4185015\n",
      "Trained batch 557 batch loss 1.30712652 epoch total loss 1.41830158\n",
      "Trained batch 558 batch loss 1.38340783 epoch total loss 1.418239\n",
      "Trained batch 559 batch loss 1.380445 epoch total loss 1.41817141\n",
      "Trained batch 560 batch loss 1.48069096 epoch total loss 1.4182831\n",
      "Trained batch 561 batch loss 1.37535 epoch total loss 1.41820657\n",
      "Trained batch 562 batch loss 1.344697 epoch total loss 1.4180758\n",
      "Trained batch 563 batch loss 1.34498167 epoch total loss 1.41794598\n",
      "Trained batch 564 batch loss 1.31028414 epoch total loss 1.41775513\n",
      "Trained batch 565 batch loss 1.36392236 epoch total loss 1.41765988\n",
      "Trained batch 566 batch loss 1.4433527 epoch total loss 1.4177053\n",
      "Trained batch 567 batch loss 1.25634432 epoch total loss 1.41742074\n",
      "Trained batch 568 batch loss 1.23156059 epoch total loss 1.41709352\n",
      "Trained batch 569 batch loss 1.32917976 epoch total loss 1.41693902\n",
      "Trained batch 570 batch loss 1.33040309 epoch total loss 1.41678715\n",
      "Trained batch 571 batch loss 1.34371698 epoch total loss 1.41665912\n",
      "Trained batch 572 batch loss 1.48237491 epoch total loss 1.41677403\n",
      "Trained batch 573 batch loss 1.2760222 epoch total loss 1.41652834\n",
      "Trained batch 574 batch loss 1.39563227 epoch total loss 1.41649187\n",
      "Trained batch 575 batch loss 1.41024113 epoch total loss 1.41648102\n",
      "Trained batch 576 batch loss 1.44812715 epoch total loss 1.41653585\n",
      "Trained batch 577 batch loss 1.34182608 epoch total loss 1.41640639\n",
      "Trained batch 578 batch loss 1.41960776 epoch total loss 1.41641188\n",
      "Trained batch 579 batch loss 1.26073432 epoch total loss 1.41614306\n",
      "Trained batch 580 batch loss 1.383551 epoch total loss 1.41608691\n",
      "Trained batch 581 batch loss 1.39973283 epoch total loss 1.41605866\n",
      "Trained batch 582 batch loss 1.64300287 epoch total loss 1.41644859\n",
      "Trained batch 583 batch loss 1.37590885 epoch total loss 1.41637909\n",
      "Trained batch 584 batch loss 1.46125805 epoch total loss 1.41645598\n",
      "Trained batch 585 batch loss 1.22145367 epoch total loss 1.41612256\n",
      "Trained batch 586 batch loss 1.1592015 epoch total loss 1.4156841\n",
      "Trained batch 587 batch loss 1.13662052 epoch total loss 1.4152087\n",
      "Trained batch 588 batch loss 1.23416781 epoch total loss 1.41490078\n",
      "Trained batch 589 batch loss 1.34878945 epoch total loss 1.4147886\n",
      "Trained batch 590 batch loss 1.58218157 epoch total loss 1.41507232\n",
      "Trained batch 591 batch loss 1.58522654 epoch total loss 1.41536009\n",
      "Trained batch 592 batch loss 1.48895478 epoch total loss 1.41548443\n",
      "Trained batch 593 batch loss 1.38281369 epoch total loss 1.41542935\n",
      "Trained batch 594 batch loss 1.35454321 epoch total loss 1.41532683\n",
      "Trained batch 595 batch loss 1.47209644 epoch total loss 1.41542232\n",
      "Trained batch 596 batch loss 1.47903454 epoch total loss 1.41552913\n",
      "Trained batch 597 batch loss 1.45562673 epoch total loss 1.41559625\n",
      "Trained batch 598 batch loss 1.47632599 epoch total loss 1.41569781\n",
      "Trained batch 599 batch loss 1.47068477 epoch total loss 1.4157896\n",
      "Trained batch 600 batch loss 1.44448328 epoch total loss 1.41583741\n",
      "Trained batch 601 batch loss 1.36975956 epoch total loss 1.41576076\n",
      "Trained batch 602 batch loss 1.36809349 epoch total loss 1.4156816\n",
      "Trained batch 603 batch loss 1.31844604 epoch total loss 1.41552031\n",
      "Trained batch 604 batch loss 1.38163555 epoch total loss 1.41546416\n",
      "Trained batch 605 batch loss 1.39635932 epoch total loss 1.41543257\n",
      "Trained batch 606 batch loss 1.34062207 epoch total loss 1.41530919\n",
      "Trained batch 607 batch loss 1.34494567 epoch total loss 1.41519332\n",
      "Trained batch 608 batch loss 1.48151934 epoch total loss 1.4153024\n",
      "Trained batch 609 batch loss 1.37148964 epoch total loss 1.41523039\n",
      "Trained batch 610 batch loss 1.48973632 epoch total loss 1.41535258\n",
      "Trained batch 611 batch loss 1.41443348 epoch total loss 1.41535103\n",
      "Trained batch 612 batch loss 1.50102091 epoch total loss 1.41549098\n",
      "Trained batch 613 batch loss 1.50703764 epoch total loss 1.41564035\n",
      "Trained batch 614 batch loss 1.33785439 epoch total loss 1.41551363\n",
      "Trained batch 615 batch loss 1.45092463 epoch total loss 1.41557121\n",
      "Trained batch 616 batch loss 1.31223631 epoch total loss 1.41540349\n",
      "Trained batch 617 batch loss 1.29107833 epoch total loss 1.41520202\n",
      "Trained batch 618 batch loss 1.17909706 epoch total loss 1.41482\n",
      "Trained batch 619 batch loss 1.32831514 epoch total loss 1.41468012\n",
      "Trained batch 620 batch loss 1.19513178 epoch total loss 1.41432607\n",
      "Trained batch 621 batch loss 1.09143472 epoch total loss 1.41380608\n",
      "Trained batch 622 batch loss 1.12511301 epoch total loss 1.413342\n",
      "Trained batch 623 batch loss 1.0746839 epoch total loss 1.4127984\n",
      "Trained batch 624 batch loss 1.2562964 epoch total loss 1.41254759\n",
      "Trained batch 625 batch loss 1.36063695 epoch total loss 1.4124645\n",
      "Trained batch 626 batch loss 1.45817769 epoch total loss 1.41253757\n",
      "Trained batch 627 batch loss 1.38249588 epoch total loss 1.41248965\n",
      "Trained batch 628 batch loss 1.29085398 epoch total loss 1.41229594\n",
      "Trained batch 629 batch loss 1.32094753 epoch total loss 1.41215074\n",
      "Trained batch 630 batch loss 1.12275 epoch total loss 1.41169131\n",
      "Trained batch 631 batch loss 1.24804235 epoch total loss 1.41143203\n",
      "Trained batch 632 batch loss 1.40086281 epoch total loss 1.41141534\n",
      "Trained batch 633 batch loss 1.47729135 epoch total loss 1.41151941\n",
      "Trained batch 634 batch loss 1.54896045 epoch total loss 1.41173613\n",
      "Trained batch 635 batch loss 1.48858261 epoch total loss 1.41185713\n",
      "Trained batch 636 batch loss 1.51630735 epoch total loss 1.4120214\n",
      "Trained batch 637 batch loss 1.40588391 epoch total loss 1.41201174\n",
      "Trained batch 638 batch loss 1.45679951 epoch total loss 1.41208196\n",
      "Trained batch 639 batch loss 1.46303165 epoch total loss 1.41216159\n",
      "Trained batch 640 batch loss 1.44116735 epoch total loss 1.41220689\n",
      "Trained batch 641 batch loss 1.41532421 epoch total loss 1.41221178\n",
      "Trained batch 642 batch loss 1.37871492 epoch total loss 1.41215968\n",
      "Trained batch 643 batch loss 1.25912237 epoch total loss 1.41192162\n",
      "Trained batch 644 batch loss 1.28284991 epoch total loss 1.41172123\n",
      "Trained batch 645 batch loss 1.35038948 epoch total loss 1.4116261\n",
      "Trained batch 646 batch loss 1.43048656 epoch total loss 1.41165531\n",
      "Trained batch 647 batch loss 1.38091803 epoch total loss 1.41160774\n",
      "Trained batch 648 batch loss 1.410869 epoch total loss 1.41160667\n",
      "Trained batch 649 batch loss 1.52560246 epoch total loss 1.41178226\n",
      "Trained batch 650 batch loss 1.40527296 epoch total loss 1.41177225\n",
      "Trained batch 651 batch loss 1.37300193 epoch total loss 1.41171265\n",
      "Trained batch 652 batch loss 1.37539303 epoch total loss 1.41165698\n",
      "Trained batch 653 batch loss 1.44041324 epoch total loss 1.41170096\n",
      "Trained batch 654 batch loss 1.26133883 epoch total loss 1.41147113\n",
      "Trained batch 655 batch loss 1.32373536 epoch total loss 1.41133714\n",
      "Trained batch 656 batch loss 1.34020841 epoch total loss 1.41122878\n",
      "Trained batch 657 batch loss 1.310359 epoch total loss 1.41107523\n",
      "Trained batch 658 batch loss 1.23532593 epoch total loss 1.41080821\n",
      "Trained batch 659 batch loss 1.33059907 epoch total loss 1.41068649\n",
      "Trained batch 660 batch loss 1.33002687 epoch total loss 1.4105643\n",
      "Trained batch 661 batch loss 1.36352503 epoch total loss 1.41049314\n",
      "Trained batch 662 batch loss 1.27295136 epoch total loss 1.41028535\n",
      "Trained batch 663 batch loss 1.39811039 epoch total loss 1.410267\n",
      "Trained batch 664 batch loss 1.37045193 epoch total loss 1.41020703\n",
      "Trained batch 665 batch loss 1.39588451 epoch total loss 1.41018546\n",
      "Trained batch 666 batch loss 1.47019196 epoch total loss 1.41027558\n",
      "Trained batch 667 batch loss 1.5015136 epoch total loss 1.41041243\n",
      "Trained batch 668 batch loss 1.44490194 epoch total loss 1.41046393\n",
      "Trained batch 669 batch loss 1.40159392 epoch total loss 1.4104507\n",
      "Trained batch 670 batch loss 1.4043293 epoch total loss 1.41044164\n",
      "Trained batch 671 batch loss 1.40178049 epoch total loss 1.41042876\n",
      "Trained batch 672 batch loss 1.38066173 epoch total loss 1.41038454\n",
      "Trained batch 673 batch loss 1.51555681 epoch total loss 1.41054082\n",
      "Trained batch 674 batch loss 1.40230656 epoch total loss 1.41052854\n",
      "Trained batch 675 batch loss 1.36060917 epoch total loss 1.41045451\n",
      "Trained batch 676 batch loss 1.32199597 epoch total loss 1.41032374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 677 batch loss 1.34335494 epoch total loss 1.41022491\n",
      "Trained batch 678 batch loss 1.30185151 epoch total loss 1.41006505\n",
      "Trained batch 679 batch loss 1.31543577 epoch total loss 1.4099257\n",
      "Trained batch 680 batch loss 1.25247431 epoch total loss 1.40969419\n",
      "Trained batch 681 batch loss 1.16168451 epoch total loss 1.40933\n",
      "Trained batch 682 batch loss 1.19007313 epoch total loss 1.4090085\n",
      "Trained batch 683 batch loss 1.34393454 epoch total loss 1.40891325\n",
      "Trained batch 684 batch loss 1.22779107 epoch total loss 1.40864837\n",
      "Trained batch 685 batch loss 1.18775856 epoch total loss 1.40832591\n",
      "Trained batch 686 batch loss 1.33100736 epoch total loss 1.40821314\n",
      "Trained batch 687 batch loss 1.32552314 epoch total loss 1.40809274\n",
      "Trained batch 688 batch loss 1.35864151 epoch total loss 1.40802085\n",
      "Trained batch 689 batch loss 1.27619755 epoch total loss 1.40782952\n",
      "Trained batch 690 batch loss 1.26283789 epoch total loss 1.40761936\n",
      "Trained batch 691 batch loss 1.40260863 epoch total loss 1.40761209\n",
      "Trained batch 692 batch loss 1.39401197 epoch total loss 1.40759242\n",
      "Trained batch 693 batch loss 1.41896117 epoch total loss 1.40760875\n",
      "Trained batch 694 batch loss 1.42109132 epoch total loss 1.40762818\n",
      "Trained batch 695 batch loss 1.35173237 epoch total loss 1.40754783\n",
      "Trained batch 696 batch loss 1.3665427 epoch total loss 1.40748882\n",
      "Trained batch 697 batch loss 1.44904947 epoch total loss 1.40754843\n",
      "Trained batch 698 batch loss 1.46020722 epoch total loss 1.40762389\n",
      "Trained batch 699 batch loss 1.52949786 epoch total loss 1.40779829\n",
      "Trained batch 700 batch loss 1.60074186 epoch total loss 1.4080739\n",
      "Trained batch 701 batch loss 1.5966959 epoch total loss 1.40834296\n",
      "Trained batch 702 batch loss 1.34931195 epoch total loss 1.40825891\n",
      "Trained batch 703 batch loss 1.31017685 epoch total loss 1.40811932\n",
      "Trained batch 704 batch loss 1.32531381 epoch total loss 1.40800178\n",
      "Trained batch 705 batch loss 1.38012743 epoch total loss 1.4079622\n",
      "Trained batch 706 batch loss 1.23972583 epoch total loss 1.4077239\n",
      "Trained batch 707 batch loss 1.32124162 epoch total loss 1.40760159\n",
      "Trained batch 708 batch loss 1.36152709 epoch total loss 1.40753651\n",
      "Trained batch 709 batch loss 1.32196462 epoch total loss 1.40741575\n",
      "Trained batch 710 batch loss 1.23930717 epoch total loss 1.407179\n",
      "Trained batch 711 batch loss 1.23175764 epoch total loss 1.40693223\n",
      "Trained batch 712 batch loss 1.34461617 epoch total loss 1.40684474\n",
      "Trained batch 713 batch loss 1.35540533 epoch total loss 1.40677261\n",
      "Trained batch 714 batch loss 1.30826855 epoch total loss 1.40663469\n",
      "Trained batch 715 batch loss 1.28485942 epoch total loss 1.40646434\n",
      "Trained batch 716 batch loss 1.23176432 epoch total loss 1.40622032\n",
      "Trained batch 717 batch loss 1.38255179 epoch total loss 1.4061873\n",
      "Trained batch 718 batch loss 1.50284278 epoch total loss 1.406322\n",
      "Trained batch 719 batch loss 1.43603098 epoch total loss 1.40636337\n",
      "Trained batch 720 batch loss 1.44576728 epoch total loss 1.40641797\n",
      "Trained batch 721 batch loss 1.43111873 epoch total loss 1.40645218\n",
      "Trained batch 722 batch loss 1.38879132 epoch total loss 1.40642774\n",
      "Trained batch 723 batch loss 1.46203792 epoch total loss 1.40650475\n",
      "Trained batch 724 batch loss 1.48863161 epoch total loss 1.40661812\n",
      "Trained batch 725 batch loss 1.4569211 epoch total loss 1.4066875\n",
      "Trained batch 726 batch loss 1.34491444 epoch total loss 1.40660238\n",
      "Trained batch 727 batch loss 1.32230043 epoch total loss 1.40648651\n",
      "Trained batch 728 batch loss 1.52475345 epoch total loss 1.40664887\n",
      "Trained batch 729 batch loss 1.41801727 epoch total loss 1.40666449\n",
      "Trained batch 730 batch loss 1.40794325 epoch total loss 1.40666616\n",
      "Trained batch 731 batch loss 1.3796134 epoch total loss 1.4066292\n",
      "Trained batch 732 batch loss 1.41810179 epoch total loss 1.40664494\n",
      "Trained batch 733 batch loss 1.38633943 epoch total loss 1.40661716\n",
      "Trained batch 734 batch loss 1.40451682 epoch total loss 1.40661442\n",
      "Trained batch 735 batch loss 1.52804399 epoch total loss 1.40677965\n",
      "Trained batch 736 batch loss 1.38594329 epoch total loss 1.40675139\n",
      "Trained batch 737 batch loss 1.48935676 epoch total loss 1.40686345\n",
      "Trained batch 738 batch loss 1.31157315 epoch total loss 1.40673435\n",
      "Trained batch 739 batch loss 1.37923527 epoch total loss 1.40669715\n",
      "Trained batch 740 batch loss 1.33692586 epoch total loss 1.40660286\n",
      "Trained batch 741 batch loss 1.24462175 epoch total loss 1.40638423\n",
      "Trained batch 742 batch loss 1.19433939 epoch total loss 1.40609848\n",
      "Trained batch 743 batch loss 1.3673799 epoch total loss 1.40604639\n",
      "Trained batch 744 batch loss 1.69844925 epoch total loss 1.40643954\n",
      "Trained batch 745 batch loss 1.41593933 epoch total loss 1.40645218\n",
      "Trained batch 746 batch loss 1.39320529 epoch total loss 1.40643442\n",
      "Trained batch 747 batch loss 1.38759422 epoch total loss 1.40640914\n",
      "Trained batch 748 batch loss 1.40327525 epoch total loss 1.40640509\n",
      "Trained batch 749 batch loss 1.34696591 epoch total loss 1.40632558\n",
      "Trained batch 750 batch loss 1.2823348 epoch total loss 1.40616035\n",
      "Trained batch 751 batch loss 1.36947048 epoch total loss 1.40611148\n",
      "Trained batch 752 batch loss 1.41865802 epoch total loss 1.40612829\n",
      "Trained batch 753 batch loss 1.33805597 epoch total loss 1.40603781\n",
      "Trained batch 754 batch loss 1.3615675 epoch total loss 1.4059788\n",
      "Trained batch 755 batch loss 1.43820095 epoch total loss 1.4060216\n",
      "Trained batch 756 batch loss 1.28120255 epoch total loss 1.40585649\n",
      "Trained batch 757 batch loss 1.31827009 epoch total loss 1.40574074\n",
      "Trained batch 758 batch loss 1.31554294 epoch total loss 1.40562177\n",
      "Trained batch 759 batch loss 1.35605133 epoch total loss 1.40555644\n",
      "Trained batch 760 batch loss 1.68133664 epoch total loss 1.40591943\n",
      "Trained batch 761 batch loss 1.62553549 epoch total loss 1.40620792\n",
      "Trained batch 762 batch loss 1.32734418 epoch total loss 1.40610456\n",
      "Trained batch 763 batch loss 1.35064745 epoch total loss 1.40603197\n",
      "Trained batch 764 batch loss 1.20932078 epoch total loss 1.40577447\n",
      "Trained batch 765 batch loss 1.38502276 epoch total loss 1.40574741\n",
      "Trained batch 766 batch loss 1.31543159 epoch total loss 1.4056294\n",
      "Trained batch 767 batch loss 1.40419388 epoch total loss 1.40562761\n",
      "Trained batch 768 batch loss 1.44376695 epoch total loss 1.4056772\n",
      "Trained batch 769 batch loss 1.45070672 epoch total loss 1.40573573\n",
      "Trained batch 770 batch loss 1.50336432 epoch total loss 1.40586257\n",
      "Trained batch 771 batch loss 1.48908985 epoch total loss 1.40597057\n",
      "Trained batch 772 batch loss 1.36434937 epoch total loss 1.40591669\n",
      "Trained batch 773 batch loss 1.38788629 epoch total loss 1.40589345\n",
      "Trained batch 774 batch loss 1.36143816 epoch total loss 1.40583599\n",
      "Trained batch 775 batch loss 1.33732772 epoch total loss 1.40574753\n",
      "Trained batch 776 batch loss 1.38356483 epoch total loss 1.40571892\n",
      "Trained batch 777 batch loss 1.31460166 epoch total loss 1.40560162\n",
      "Trained batch 778 batch loss 1.28645468 epoch total loss 1.40544856\n",
      "Trained batch 779 batch loss 1.32439876 epoch total loss 1.40534437\n",
      "Trained batch 780 batch loss 1.31132245 epoch total loss 1.40522385\n",
      "Trained batch 781 batch loss 1.25835133 epoch total loss 1.40503573\n",
      "Trained batch 782 batch loss 1.27290332 epoch total loss 1.40486681\n",
      "Trained batch 783 batch loss 1.28667891 epoch total loss 1.40471578\n",
      "Trained batch 784 batch loss 1.32697654 epoch total loss 1.40461671\n",
      "Trained batch 785 batch loss 1.32149792 epoch total loss 1.40451086\n",
      "Trained batch 786 batch loss 1.29075265 epoch total loss 1.40436614\n",
      "Trained batch 787 batch loss 1.44010448 epoch total loss 1.40441155\n",
      "Trained batch 788 batch loss 1.43083119 epoch total loss 1.40444493\n",
      "Trained batch 789 batch loss 1.40701747 epoch total loss 1.40444815\n",
      "Trained batch 790 batch loss 1.38750219 epoch total loss 1.40442669\n",
      "Trained batch 791 batch loss 1.34946322 epoch total loss 1.40435719\n",
      "Trained batch 792 batch loss 1.39320111 epoch total loss 1.40434313\n",
      "Trained batch 793 batch loss 1.20821869 epoch total loss 1.40409589\n",
      "Trained batch 794 batch loss 1.27251863 epoch total loss 1.40393007\n",
      "Trained batch 795 batch loss 1.29281473 epoch total loss 1.40379035\n",
      "Trained batch 796 batch loss 1.50012589 epoch total loss 1.40391135\n",
      "Trained batch 797 batch loss 1.40906763 epoch total loss 1.40391779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 798 batch loss 1.34036195 epoch total loss 1.40383816\n",
      "Trained batch 799 batch loss 1.29063284 epoch total loss 1.40369642\n",
      "Trained batch 800 batch loss 1.38457072 epoch total loss 1.40367246\n",
      "Trained batch 801 batch loss 1.34337115 epoch total loss 1.40359724\n",
      "Trained batch 802 batch loss 1.13233066 epoch total loss 1.40325892\n",
      "Trained batch 803 batch loss 1.28914165 epoch total loss 1.40311694\n",
      "Trained batch 804 batch loss 1.38661194 epoch total loss 1.40309632\n",
      "Trained batch 805 batch loss 1.24249864 epoch total loss 1.40289688\n",
      "Trained batch 806 batch loss 1.15965486 epoch total loss 1.40259516\n",
      "Trained batch 807 batch loss 1.20350277 epoch total loss 1.4023484\n",
      "Trained batch 808 batch loss 1.38393354 epoch total loss 1.40232563\n",
      "Trained batch 809 batch loss 1.33374143 epoch total loss 1.40224087\n",
      "Trained batch 810 batch loss 1.34234548 epoch total loss 1.40216684\n",
      "Trained batch 811 batch loss 1.36568117 epoch total loss 1.4021219\n",
      "Trained batch 812 batch loss 1.44723618 epoch total loss 1.40217745\n",
      "Trained batch 813 batch loss 1.28596056 epoch total loss 1.40203464\n",
      "Trained batch 814 batch loss 1.33515429 epoch total loss 1.40195251\n",
      "Trained batch 815 batch loss 1.33056211 epoch total loss 1.40186489\n",
      "Trained batch 816 batch loss 1.42070174 epoch total loss 1.40188789\n",
      "Trained batch 817 batch loss 1.1927774 epoch total loss 1.40163195\n",
      "Trained batch 818 batch loss 1.11961913 epoch total loss 1.4012872\n",
      "Trained batch 819 batch loss 1.04105687 epoch total loss 1.40084732\n",
      "Trained batch 820 batch loss 1.33855081 epoch total loss 1.40077126\n",
      "Trained batch 821 batch loss 1.38872838 epoch total loss 1.4007566\n",
      "Trained batch 822 batch loss 1.61012053 epoch total loss 1.40101123\n",
      "Trained batch 823 batch loss 1.51103091 epoch total loss 1.40114486\n",
      "Trained batch 824 batch loss 1.44879305 epoch total loss 1.4012028\n",
      "Trained batch 825 batch loss 1.50259244 epoch total loss 1.40132558\n",
      "Trained batch 826 batch loss 1.34693635 epoch total loss 1.40125978\n",
      "Trained batch 827 batch loss 1.34791136 epoch total loss 1.40119517\n",
      "Trained batch 828 batch loss 1.42330527 epoch total loss 1.40122199\n",
      "Trained batch 829 batch loss 1.33276224 epoch total loss 1.40113938\n",
      "Trained batch 830 batch loss 1.35967827 epoch total loss 1.40108931\n",
      "Trained batch 831 batch loss 1.32333589 epoch total loss 1.40099585\n",
      "Trained batch 832 batch loss 1.4099431 epoch total loss 1.40100658\n",
      "Trained batch 833 batch loss 1.35684943 epoch total loss 1.40095353\n",
      "Trained batch 834 batch loss 1.29488659 epoch total loss 1.40082633\n",
      "Trained batch 835 batch loss 1.40711915 epoch total loss 1.40083385\n",
      "Trained batch 836 batch loss 1.41192687 epoch total loss 1.4008472\n",
      "Trained batch 837 batch loss 1.2198863 epoch total loss 1.40063095\n",
      "Trained batch 838 batch loss 1.3341223 epoch total loss 1.40055156\n",
      "Trained batch 839 batch loss 1.40128851 epoch total loss 1.40055239\n",
      "Trained batch 840 batch loss 1.36879206 epoch total loss 1.4005146\n",
      "Trained batch 841 batch loss 1.33995831 epoch total loss 1.4004426\n",
      "Trained batch 842 batch loss 1.44719875 epoch total loss 1.40049803\n",
      "Trained batch 843 batch loss 1.24099624 epoch total loss 1.40030885\n",
      "Trained batch 844 batch loss 1.29428458 epoch total loss 1.4001832\n",
      "Trained batch 845 batch loss 1.40179932 epoch total loss 1.40018523\n",
      "Trained batch 846 batch loss 1.44171631 epoch total loss 1.40023434\n",
      "Trained batch 847 batch loss 1.45406449 epoch total loss 1.400298\n",
      "Trained batch 848 batch loss 1.38117337 epoch total loss 1.40027547\n",
      "Trained batch 849 batch loss 1.45764971 epoch total loss 1.40034306\n",
      "Trained batch 850 batch loss 1.47682226 epoch total loss 1.40043294\n",
      "Trained batch 851 batch loss 1.38868141 epoch total loss 1.40041912\n",
      "Trained batch 852 batch loss 1.33540881 epoch total loss 1.40034294\n",
      "Trained batch 853 batch loss 1.44475985 epoch total loss 1.40039492\n",
      "Trained batch 854 batch loss 1.40007484 epoch total loss 1.40039444\n",
      "Trained batch 855 batch loss 1.40885448 epoch total loss 1.40040433\n",
      "Trained batch 856 batch loss 1.40173507 epoch total loss 1.40040588\n",
      "Trained batch 857 batch loss 1.35451186 epoch total loss 1.40035236\n",
      "Trained batch 858 batch loss 1.36548662 epoch total loss 1.40031171\n",
      "Trained batch 859 batch loss 1.33275306 epoch total loss 1.40023303\n",
      "Trained batch 860 batch loss 1.35470772 epoch total loss 1.4001801\n",
      "Trained batch 861 batch loss 1.31154013 epoch total loss 1.4000771\n",
      "Trained batch 862 batch loss 1.28510702 epoch total loss 1.39994383\n",
      "Trained batch 863 batch loss 1.36408174 epoch total loss 1.39990234\n",
      "Trained batch 864 batch loss 1.359635 epoch total loss 1.39985573\n",
      "Trained batch 865 batch loss 1.3447113 epoch total loss 1.39979196\n",
      "Trained batch 866 batch loss 1.42336249 epoch total loss 1.39981914\n",
      "Trained batch 867 batch loss 1.3636421 epoch total loss 1.39977741\n",
      "Trained batch 868 batch loss 1.42621064 epoch total loss 1.39980793\n",
      "Trained batch 869 batch loss 1.53881371 epoch total loss 1.39996791\n",
      "Trained batch 870 batch loss 1.33049333 epoch total loss 1.39988804\n",
      "Trained batch 871 batch loss 1.32505894 epoch total loss 1.39980209\n",
      "Trained batch 872 batch loss 1.34331346 epoch total loss 1.39973724\n",
      "Trained batch 873 batch loss 1.41484797 epoch total loss 1.39975452\n",
      "Trained batch 874 batch loss 1.3661629 epoch total loss 1.39971614\n",
      "Trained batch 875 batch loss 1.42746627 epoch total loss 1.39974785\n",
      "Trained batch 876 batch loss 1.36087489 epoch total loss 1.3997035\n",
      "Trained batch 877 batch loss 1.39011073 epoch total loss 1.39969254\n",
      "Trained batch 878 batch loss 1.42641318 epoch total loss 1.39972293\n",
      "Trained batch 879 batch loss 1.42180598 epoch total loss 1.39974809\n",
      "Trained batch 880 batch loss 1.45409548 epoch total loss 1.39980984\n",
      "Trained batch 881 batch loss 1.50557041 epoch total loss 1.39992988\n",
      "Trained batch 882 batch loss 1.37831807 epoch total loss 1.39990544\n",
      "Trained batch 883 batch loss 1.35145569 epoch total loss 1.39985049\n",
      "Trained batch 884 batch loss 1.30752134 epoch total loss 1.39974606\n",
      "Trained batch 885 batch loss 1.35685635 epoch total loss 1.39969754\n",
      "Trained batch 886 batch loss 1.30558026 epoch total loss 1.39959121\n",
      "Trained batch 887 batch loss 1.26107073 epoch total loss 1.39943516\n",
      "Trained batch 888 batch loss 1.33479524 epoch total loss 1.39936233\n",
      "Trained batch 889 batch loss 1.36470199 epoch total loss 1.39932346\n",
      "Trained batch 890 batch loss 1.3197453 epoch total loss 1.39923394\n",
      "Trained batch 891 batch loss 1.34097552 epoch total loss 1.39916861\n",
      "Trained batch 892 batch loss 1.40832651 epoch total loss 1.39917886\n",
      "Trained batch 893 batch loss 1.3467679 epoch total loss 1.39912021\n",
      "Trained batch 894 batch loss 1.2743001 epoch total loss 1.3989805\n",
      "Trained batch 895 batch loss 1.32383204 epoch total loss 1.39889657\n",
      "Trained batch 896 batch loss 1.37249517 epoch total loss 1.39886701\n",
      "Trained batch 897 batch loss 1.34904444 epoch total loss 1.39881146\n",
      "Trained batch 898 batch loss 1.37136471 epoch total loss 1.39878082\n",
      "Trained batch 899 batch loss 1.37566483 epoch total loss 1.39875507\n",
      "Trained batch 900 batch loss 1.40207887 epoch total loss 1.39875877\n",
      "Trained batch 901 batch loss 1.33922887 epoch total loss 1.39869273\n",
      "Trained batch 902 batch loss 1.39054608 epoch total loss 1.39868367\n",
      "Trained batch 903 batch loss 1.34027565 epoch total loss 1.39861906\n",
      "Trained batch 904 batch loss 1.41232669 epoch total loss 1.3986342\n",
      "Trained batch 905 batch loss 1.37161672 epoch total loss 1.39860439\n",
      "Trained batch 906 batch loss 1.46067047 epoch total loss 1.39867294\n",
      "Trained batch 907 batch loss 1.38490665 epoch total loss 1.39865768\n",
      "Trained batch 908 batch loss 1.38157451 epoch total loss 1.39863884\n",
      "Trained batch 909 batch loss 1.30877781 epoch total loss 1.39854014\n",
      "Trained batch 910 batch loss 1.32306242 epoch total loss 1.39845717\n",
      "Trained batch 911 batch loss 1.35938787 epoch total loss 1.39841437\n",
      "Trained batch 912 batch loss 1.2656 epoch total loss 1.3982687\n",
      "Trained batch 913 batch loss 1.23217928 epoch total loss 1.39808679\n",
      "Trained batch 914 batch loss 1.34406626 epoch total loss 1.39802778\n",
      "Trained batch 915 batch loss 1.34928465 epoch total loss 1.39797449\n",
      "Trained batch 916 batch loss 1.33366251 epoch total loss 1.39790416\n",
      "Trained batch 917 batch loss 1.46841741 epoch total loss 1.39798105\n",
      "Trained batch 918 batch loss 1.40670609 epoch total loss 1.39799058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 919 batch loss 1.42168725 epoch total loss 1.39801633\n",
      "Trained batch 920 batch loss 1.51711154 epoch total loss 1.39814568\n",
      "Trained batch 921 batch loss 1.50734556 epoch total loss 1.39826429\n",
      "Trained batch 922 batch loss 1.35807943 epoch total loss 1.39822066\n",
      "Trained batch 923 batch loss 1.26384723 epoch total loss 1.39807498\n",
      "Trained batch 924 batch loss 1.44347227 epoch total loss 1.3981241\n",
      "Trained batch 925 batch loss 1.47662675 epoch total loss 1.3982091\n",
      "Trained batch 926 batch loss 1.35330594 epoch total loss 1.39816058\n",
      "Trained batch 927 batch loss 1.26465058 epoch total loss 1.39801645\n",
      "Trained batch 928 batch loss 1.29245543 epoch total loss 1.39790273\n",
      "Trained batch 929 batch loss 1.29202282 epoch total loss 1.39778876\n",
      "Trained batch 930 batch loss 1.29377747 epoch total loss 1.39767694\n",
      "Trained batch 931 batch loss 1.28888953 epoch total loss 1.39756024\n",
      "Trained batch 932 batch loss 1.34024227 epoch total loss 1.39749861\n",
      "Trained batch 933 batch loss 1.40822601 epoch total loss 1.39751017\n",
      "Trained batch 934 batch loss 1.35110307 epoch total loss 1.39746046\n",
      "Trained batch 935 batch loss 1.35257363 epoch total loss 1.39741242\n",
      "Trained batch 936 batch loss 1.50229871 epoch total loss 1.39752448\n",
      "Trained batch 937 batch loss 1.50431311 epoch total loss 1.39763832\n",
      "Trained batch 938 batch loss 1.33222568 epoch total loss 1.3975687\n",
      "Trained batch 939 batch loss 1.24492085 epoch total loss 1.3974061\n",
      "Trained batch 940 batch loss 1.21693182 epoch total loss 1.39721406\n",
      "Trained batch 941 batch loss 1.27791166 epoch total loss 1.39708734\n",
      "Trained batch 942 batch loss 1.34781182 epoch total loss 1.397035\n",
      "Trained batch 943 batch loss 1.37285042 epoch total loss 1.39700925\n",
      "Trained batch 944 batch loss 1.37478375 epoch total loss 1.39698577\n",
      "Trained batch 945 batch loss 1.30035782 epoch total loss 1.39688349\n",
      "Trained batch 946 batch loss 1.29811549 epoch total loss 1.39677906\n",
      "Trained batch 947 batch loss 1.44421983 epoch total loss 1.39682925\n",
      "Trained batch 948 batch loss 1.42247415 epoch total loss 1.39685631\n",
      "Trained batch 949 batch loss 1.37904739 epoch total loss 1.39683747\n",
      "Trained batch 950 batch loss 1.35360277 epoch total loss 1.39679193\n",
      "Trained batch 951 batch loss 1.3110379 epoch total loss 1.39670181\n",
      "Trained batch 952 batch loss 1.18900359 epoch total loss 1.39648366\n",
      "Trained batch 953 batch loss 1.17834496 epoch total loss 1.39625466\n",
      "Trained batch 954 batch loss 1.21061039 epoch total loss 1.39606011\n",
      "Trained batch 955 batch loss 1.31175184 epoch total loss 1.39597178\n",
      "Trained batch 956 batch loss 1.35242379 epoch total loss 1.39592624\n",
      "Trained batch 957 batch loss 1.32719278 epoch total loss 1.39585435\n",
      "Trained batch 958 batch loss 1.28636146 epoch total loss 1.39574015\n",
      "Trained batch 959 batch loss 1.24060845 epoch total loss 1.39557838\n",
      "Trained batch 960 batch loss 1.20423412 epoch total loss 1.39537895\n",
      "Trained batch 961 batch loss 1.32623029 epoch total loss 1.39530694\n",
      "Trained batch 962 batch loss 1.38622606 epoch total loss 1.39529753\n",
      "Trained batch 963 batch loss 1.55395436 epoch total loss 1.39546227\n",
      "Trained batch 964 batch loss 1.59982479 epoch total loss 1.39567435\n",
      "Trained batch 965 batch loss 1.55941737 epoch total loss 1.3958441\n",
      "Trained batch 966 batch loss 1.46933615 epoch total loss 1.39592016\n",
      "Trained batch 967 batch loss 1.37031209 epoch total loss 1.39589369\n",
      "Trained batch 968 batch loss 1.29951143 epoch total loss 1.39579415\n",
      "Trained batch 969 batch loss 1.24364829 epoch total loss 1.39563715\n",
      "Trained batch 970 batch loss 1.3499881 epoch total loss 1.39559007\n",
      "Trained batch 971 batch loss 1.37891579 epoch total loss 1.3955729\n",
      "Trained batch 972 batch loss 1.41459429 epoch total loss 1.39559245\n",
      "Trained batch 973 batch loss 1.46023071 epoch total loss 1.39565885\n",
      "Trained batch 974 batch loss 1.41136408 epoch total loss 1.39567494\n",
      "Trained batch 975 batch loss 1.35533381 epoch total loss 1.3956337\n",
      "Trained batch 976 batch loss 1.46185446 epoch total loss 1.39570153\n",
      "Trained batch 977 batch loss 1.29987025 epoch total loss 1.39560354\n",
      "Trained batch 978 batch loss 1.44791389 epoch total loss 1.39565694\n",
      "Trained batch 979 batch loss 1.32106042 epoch total loss 1.39558077\n",
      "Trained batch 980 batch loss 1.37605143 epoch total loss 1.39556086\n",
      "Trained batch 981 batch loss 1.56302619 epoch total loss 1.39573157\n",
      "Trained batch 982 batch loss 1.51104808 epoch total loss 1.39584899\n",
      "Trained batch 983 batch loss 1.35605955 epoch total loss 1.39580858\n",
      "Trained batch 984 batch loss 1.40838921 epoch total loss 1.39582145\n",
      "Trained batch 985 batch loss 1.30206478 epoch total loss 1.39572632\n",
      "Trained batch 986 batch loss 1.3478775 epoch total loss 1.3956778\n",
      "Trained batch 987 batch loss 1.31961143 epoch total loss 1.39560068\n",
      "Trained batch 988 batch loss 1.47531819 epoch total loss 1.39568138\n",
      "Trained batch 989 batch loss 1.48792696 epoch total loss 1.39577472\n",
      "Trained batch 990 batch loss 1.5051955 epoch total loss 1.39588523\n",
      "Trained batch 991 batch loss 1.54403341 epoch total loss 1.39603472\n",
      "Trained batch 992 batch loss 1.61694956 epoch total loss 1.39625752\n",
      "Trained batch 993 batch loss 1.46543932 epoch total loss 1.39632714\n",
      "Trained batch 994 batch loss 1.50428319 epoch total loss 1.39643574\n",
      "Trained batch 995 batch loss 1.44330406 epoch total loss 1.39648294\n",
      "Trained batch 996 batch loss 1.48948109 epoch total loss 1.39657629\n",
      "Trained batch 997 batch loss 1.52813 epoch total loss 1.39670825\n",
      "Trained batch 998 batch loss 1.44522667 epoch total loss 1.39675677\n",
      "Trained batch 999 batch loss 1.44379008 epoch total loss 1.39680386\n",
      "Trained batch 1000 batch loss 1.42179692 epoch total loss 1.39682889\n",
      "Trained batch 1001 batch loss 1.32277048 epoch total loss 1.39675486\n",
      "Trained batch 1002 batch loss 1.33483803 epoch total loss 1.39669311\n",
      "Trained batch 1003 batch loss 1.2710166 epoch total loss 1.3965677\n",
      "Trained batch 1004 batch loss 1.27752233 epoch total loss 1.39644909\n",
      "Trained batch 1005 batch loss 1.29255438 epoch total loss 1.39634573\n",
      "Trained batch 1006 batch loss 1.24949741 epoch total loss 1.39619982\n",
      "Trained batch 1007 batch loss 1.31688237 epoch total loss 1.39612103\n",
      "Trained batch 1008 batch loss 1.23240018 epoch total loss 1.39595866\n",
      "Trained batch 1009 batch loss 1.27835894 epoch total loss 1.39584208\n",
      "Trained batch 1010 batch loss 1.30358219 epoch total loss 1.39575076\n",
      "Trained batch 1011 batch loss 1.38353777 epoch total loss 1.39573872\n",
      "Trained batch 1012 batch loss 1.41025329 epoch total loss 1.39575303\n",
      "Trained batch 1013 batch loss 1.34096897 epoch total loss 1.3956989\n",
      "Trained batch 1014 batch loss 1.2780894 epoch total loss 1.39558291\n",
      "Trained batch 1015 batch loss 1.43930352 epoch total loss 1.39562607\n",
      "Trained batch 1016 batch loss 1.34133601 epoch total loss 1.39557254\n",
      "Trained batch 1017 batch loss 1.34866786 epoch total loss 1.39552641\n",
      "Trained batch 1018 batch loss 1.46811891 epoch total loss 1.3955977\n",
      "Trained batch 1019 batch loss 1.34571135 epoch total loss 1.39554882\n",
      "Trained batch 1020 batch loss 1.36153686 epoch total loss 1.39551544\n",
      "Trained batch 1021 batch loss 1.35920894 epoch total loss 1.39547992\n",
      "Trained batch 1022 batch loss 1.45461833 epoch total loss 1.39553773\n",
      "Trained batch 1023 batch loss 1.42435598 epoch total loss 1.39556587\n",
      "Trained batch 1024 batch loss 1.23837566 epoch total loss 1.39541245\n",
      "Trained batch 1025 batch loss 1.13660645 epoch total loss 1.39516\n",
      "Trained batch 1026 batch loss 1.23000193 epoch total loss 1.39499891\n",
      "Trained batch 1027 batch loss 1.35563457 epoch total loss 1.39496052\n",
      "Trained batch 1028 batch loss 1.42493701 epoch total loss 1.39498973\n",
      "Trained batch 1029 batch loss 1.40330637 epoch total loss 1.39499784\n",
      "Trained batch 1030 batch loss 1.40375149 epoch total loss 1.39500642\n",
      "Trained batch 1031 batch loss 1.51608288 epoch total loss 1.39512384\n",
      "Trained batch 1032 batch loss 1.48153341 epoch total loss 1.39520764\n",
      "Trained batch 1033 batch loss 1.36857092 epoch total loss 1.39518178\n",
      "Trained batch 1034 batch loss 1.40223753 epoch total loss 1.39518857\n",
      "Trained batch 1035 batch loss 1.35161686 epoch total loss 1.39514649\n",
      "Trained batch 1036 batch loss 1.42780089 epoch total loss 1.39517796\n",
      "Trained batch 1037 batch loss 1.40212977 epoch total loss 1.39518464\n",
      "Trained batch 1038 batch loss 1.49214244 epoch total loss 1.3952781\n",
      "Trained batch 1039 batch loss 1.38367391 epoch total loss 1.39526701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1040 batch loss 1.36945534 epoch total loss 1.39524221\n",
      "Trained batch 1041 batch loss 1.37432563 epoch total loss 1.39522207\n",
      "Trained batch 1042 batch loss 1.2738812 epoch total loss 1.3951056\n",
      "Trained batch 1043 batch loss 1.39585721 epoch total loss 1.39510632\n",
      "Trained batch 1044 batch loss 1.55103803 epoch total loss 1.39525568\n",
      "Trained batch 1045 batch loss 1.3853929 epoch total loss 1.39524627\n",
      "Trained batch 1046 batch loss 1.17501819 epoch total loss 1.39503574\n",
      "Trained batch 1047 batch loss 1.21180546 epoch total loss 1.39486074\n",
      "Trained batch 1048 batch loss 1.34622288 epoch total loss 1.39481425\n",
      "Trained batch 1049 batch loss 1.40494466 epoch total loss 1.39482391\n",
      "Trained batch 1050 batch loss 1.31612456 epoch total loss 1.39474905\n",
      "Trained batch 1051 batch loss 1.44218349 epoch total loss 1.39479411\n",
      "Trained batch 1052 batch loss 1.40526557 epoch total loss 1.394804\n",
      "Trained batch 1053 batch loss 1.32673216 epoch total loss 1.39473951\n",
      "Trained batch 1054 batch loss 1.33598697 epoch total loss 1.39468372\n",
      "Trained batch 1055 batch loss 1.30845 epoch total loss 1.39460194\n",
      "Trained batch 1056 batch loss 1.35583 epoch total loss 1.39456522\n",
      "Trained batch 1057 batch loss 1.50004733 epoch total loss 1.394665\n",
      "Trained batch 1058 batch loss 1.46257687 epoch total loss 1.39472914\n",
      "Trained batch 1059 batch loss 1.2618438 epoch total loss 1.39460361\n",
      "Trained batch 1060 batch loss 1.37357473 epoch total loss 1.39458382\n",
      "Trained batch 1061 batch loss 1.32206309 epoch total loss 1.3945154\n",
      "Trained batch 1062 batch loss 1.43981123 epoch total loss 1.39455807\n",
      "Trained batch 1063 batch loss 1.22377908 epoch total loss 1.39439738\n",
      "Trained batch 1064 batch loss 1.15488482 epoch total loss 1.39417231\n",
      "Trained batch 1065 batch loss 1.41643012 epoch total loss 1.39419317\n",
      "Trained batch 1066 batch loss 1.47768462 epoch total loss 1.39427137\n",
      "Trained batch 1067 batch loss 1.41077554 epoch total loss 1.39428687\n",
      "Trained batch 1068 batch loss 1.46540976 epoch total loss 1.39435351\n",
      "Trained batch 1069 batch loss 1.42708778 epoch total loss 1.39438415\n",
      "Trained batch 1070 batch loss 1.32531285 epoch total loss 1.39431965\n",
      "Trained batch 1071 batch loss 1.23974967 epoch total loss 1.39417529\n",
      "Trained batch 1072 batch loss 1.28052521 epoch total loss 1.39406931\n",
      "Trained batch 1073 batch loss 1.3846128 epoch total loss 1.39406049\n",
      "Trained batch 1074 batch loss 1.3483845 epoch total loss 1.39401793\n",
      "Trained batch 1075 batch loss 1.34822953 epoch total loss 1.39397538\n",
      "Trained batch 1076 batch loss 1.42581403 epoch total loss 1.39400494\n",
      "Trained batch 1077 batch loss 1.38034618 epoch total loss 1.3939923\n",
      "Trained batch 1078 batch loss 1.48932672 epoch total loss 1.39408076\n",
      "Trained batch 1079 batch loss 1.36662388 epoch total loss 1.39405525\n",
      "Trained batch 1080 batch loss 1.55880213 epoch total loss 1.39420784\n",
      "Trained batch 1081 batch loss 1.43266225 epoch total loss 1.39424336\n",
      "Trained batch 1082 batch loss 1.41694796 epoch total loss 1.39426446\n",
      "Trained batch 1083 batch loss 1.4766407 epoch total loss 1.39434052\n",
      "Trained batch 1084 batch loss 1.45620799 epoch total loss 1.39439762\n",
      "Trained batch 1085 batch loss 1.30591929 epoch total loss 1.39431608\n",
      "Trained batch 1086 batch loss 1.37951505 epoch total loss 1.39430237\n",
      "Trained batch 1087 batch loss 1.50108099 epoch total loss 1.3944006\n",
      "Trained batch 1088 batch loss 1.5498929 epoch total loss 1.39454365\n",
      "Trained batch 1089 batch loss 1.64379764 epoch total loss 1.39477253\n",
      "Trained batch 1090 batch loss 1.53345966 epoch total loss 1.39489973\n",
      "Trained batch 1091 batch loss 1.44900465 epoch total loss 1.39494932\n",
      "Trained batch 1092 batch loss 1.4456017 epoch total loss 1.39499557\n",
      "Trained batch 1093 batch loss 1.34759402 epoch total loss 1.39495218\n",
      "Trained batch 1094 batch loss 1.24620903 epoch total loss 1.39481628\n",
      "Trained batch 1095 batch loss 1.38703299 epoch total loss 1.39480913\n",
      "Trained batch 1096 batch loss 1.46102059 epoch total loss 1.39486957\n",
      "Trained batch 1097 batch loss 1.30259502 epoch total loss 1.39478552\n",
      "Trained batch 1098 batch loss 1.46082664 epoch total loss 1.3948456\n",
      "Trained batch 1099 batch loss 1.27540696 epoch total loss 1.39473701\n",
      "Trained batch 1100 batch loss 1.33174503 epoch total loss 1.39467978\n",
      "Trained batch 1101 batch loss 1.25143683 epoch total loss 1.39454961\n",
      "Trained batch 1102 batch loss 1.23843777 epoch total loss 1.39440799\n",
      "Trained batch 1103 batch loss 1.30714726 epoch total loss 1.39432883\n",
      "Trained batch 1104 batch loss 1.28489983 epoch total loss 1.39422977\n",
      "Trained batch 1105 batch loss 1.34579933 epoch total loss 1.3941859\n",
      "Trained batch 1106 batch loss 1.39289796 epoch total loss 1.39418483\n",
      "Trained batch 1107 batch loss 1.42299557 epoch total loss 1.39421082\n",
      "Trained batch 1108 batch loss 1.34704828 epoch total loss 1.39416826\n",
      "Trained batch 1109 batch loss 1.39083958 epoch total loss 1.39416528\n",
      "Trained batch 1110 batch loss 1.45857167 epoch total loss 1.39422333\n",
      "Trained batch 1111 batch loss 1.39093828 epoch total loss 1.39422047\n",
      "Trained batch 1112 batch loss 1.44661689 epoch total loss 1.39426756\n",
      "Trained batch 1113 batch loss 1.4062115 epoch total loss 1.39427829\n",
      "Trained batch 1114 batch loss 1.38128865 epoch total loss 1.39426672\n",
      "Trained batch 1115 batch loss 1.39782143 epoch total loss 1.39427\n",
      "Trained batch 1116 batch loss 1.35987973 epoch total loss 1.39423907\n",
      "Trained batch 1117 batch loss 1.39841843 epoch total loss 1.39424288\n",
      "Trained batch 1118 batch loss 1.35917032 epoch total loss 1.39421141\n",
      "Trained batch 1119 batch loss 1.40854502 epoch total loss 1.39422429\n",
      "Trained batch 1120 batch loss 1.32420111 epoch total loss 1.39416182\n",
      "Trained batch 1121 batch loss 1.26792622 epoch total loss 1.39404917\n",
      "Trained batch 1122 batch loss 1.21305275 epoch total loss 1.39388788\n",
      "Trained batch 1123 batch loss 1.21115398 epoch total loss 1.39372516\n",
      "Trained batch 1124 batch loss 1.30888021 epoch total loss 1.39364958\n",
      "Trained batch 1125 batch loss 1.28153872 epoch total loss 1.39354992\n",
      "Trained batch 1126 batch loss 1.25159717 epoch total loss 1.3934238\n",
      "Trained batch 1127 batch loss 1.18135035 epoch total loss 1.39323568\n",
      "Trained batch 1128 batch loss 1.26777256 epoch total loss 1.39312446\n",
      "Trained batch 1129 batch loss 1.33495009 epoch total loss 1.39307296\n",
      "Trained batch 1130 batch loss 1.45167351 epoch total loss 1.39312482\n",
      "Trained batch 1131 batch loss 1.45570779 epoch total loss 1.39318013\n",
      "Trained batch 1132 batch loss 1.5241102 epoch total loss 1.39329588\n",
      "Trained batch 1133 batch loss 1.40127051 epoch total loss 1.39330292\n",
      "Trained batch 1134 batch loss 1.38246202 epoch total loss 1.39329338\n",
      "Trained batch 1135 batch loss 1.28909302 epoch total loss 1.39320147\n",
      "Trained batch 1136 batch loss 1.37306285 epoch total loss 1.39318371\n",
      "Trained batch 1137 batch loss 1.44526458 epoch total loss 1.3932296\n",
      "Trained batch 1138 batch loss 1.27930617 epoch total loss 1.39312947\n",
      "Trained batch 1139 batch loss 1.35802138 epoch total loss 1.39309871\n",
      "Trained batch 1140 batch loss 1.37262809 epoch total loss 1.39308071\n",
      "Trained batch 1141 batch loss 1.27482486 epoch total loss 1.39297712\n",
      "Trained batch 1142 batch loss 1.27192974 epoch total loss 1.39287114\n",
      "Trained batch 1143 batch loss 1.22885883 epoch total loss 1.39272761\n",
      "Trained batch 1144 batch loss 1.14479613 epoch total loss 1.39251089\n",
      "Trained batch 1145 batch loss 1.18328416 epoch total loss 1.39232814\n",
      "Trained batch 1146 batch loss 1.46427143 epoch total loss 1.39239085\n",
      "Trained batch 1147 batch loss 1.38581038 epoch total loss 1.39238513\n",
      "Trained batch 1148 batch loss 1.53831196 epoch total loss 1.39251232\n",
      "Trained batch 1149 batch loss 1.55093169 epoch total loss 1.39265013\n",
      "Trained batch 1150 batch loss 1.34180403 epoch total loss 1.3926059\n",
      "Trained batch 1151 batch loss 1.31637168 epoch total loss 1.39253974\n",
      "Trained batch 1152 batch loss 1.39742041 epoch total loss 1.39254403\n",
      "Trained batch 1153 batch loss 1.32290602 epoch total loss 1.39248359\n",
      "Trained batch 1154 batch loss 1.43260753 epoch total loss 1.3925184\n",
      "Trained batch 1155 batch loss 1.50210142 epoch total loss 1.39261317\n",
      "Trained batch 1156 batch loss 1.44407952 epoch total loss 1.39265776\n",
      "Trained batch 1157 batch loss 1.38163257 epoch total loss 1.39264822\n",
      "Trained batch 1158 batch loss 1.37344372 epoch total loss 1.39263153\n",
      "Trained batch 1159 batch loss 1.33665419 epoch total loss 1.39258325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1160 batch loss 1.33398521 epoch total loss 1.39253283\n",
      "Trained batch 1161 batch loss 1.36575651 epoch total loss 1.3925097\n",
      "Trained batch 1162 batch loss 1.57087409 epoch total loss 1.39266324\n",
      "Trained batch 1163 batch loss 1.63921928 epoch total loss 1.39287519\n",
      "Trained batch 1164 batch loss 1.35728264 epoch total loss 1.39284456\n",
      "Trained batch 1165 batch loss 1.54867911 epoch total loss 1.39297843\n",
      "Trained batch 1166 batch loss 1.37354779 epoch total loss 1.39296174\n",
      "Trained batch 1167 batch loss 1.40715885 epoch total loss 1.39297378\n",
      "Trained batch 1168 batch loss 1.50498033 epoch total loss 1.39306974\n",
      "Trained batch 1169 batch loss 1.49977648 epoch total loss 1.39316106\n",
      "Trained batch 1170 batch loss 1.35613132 epoch total loss 1.39312935\n",
      "Trained batch 1171 batch loss 1.29654288 epoch total loss 1.39304686\n",
      "Trained batch 1172 batch loss 1.36654055 epoch total loss 1.39302421\n",
      "Trained batch 1173 batch loss 1.32900286 epoch total loss 1.39296961\n",
      "Trained batch 1174 batch loss 1.38395238 epoch total loss 1.39296186\n",
      "Trained batch 1175 batch loss 1.31734431 epoch total loss 1.39289761\n",
      "Trained batch 1176 batch loss 1.33812785 epoch total loss 1.392851\n",
      "Trained batch 1177 batch loss 1.4062165 epoch total loss 1.39286244\n",
      "Trained batch 1178 batch loss 1.32332611 epoch total loss 1.39280343\n",
      "Trained batch 1179 batch loss 1.34939098 epoch total loss 1.39276659\n",
      "Trained batch 1180 batch loss 1.35158515 epoch total loss 1.39273167\n",
      "Trained batch 1181 batch loss 1.40371048 epoch total loss 1.39274096\n",
      "Trained batch 1182 batch loss 1.46810079 epoch total loss 1.39280474\n",
      "Trained batch 1183 batch loss 1.51091385 epoch total loss 1.39290452\n",
      "Trained batch 1184 batch loss 1.43400216 epoch total loss 1.39293921\n",
      "Trained batch 1185 batch loss 1.35707808 epoch total loss 1.39290893\n",
      "Trained batch 1186 batch loss 1.44652557 epoch total loss 1.39295411\n",
      "Trained batch 1187 batch loss 1.34459305 epoch total loss 1.39291334\n",
      "Trained batch 1188 batch loss 1.40159142 epoch total loss 1.39292073\n",
      "Trained batch 1189 batch loss 1.34090757 epoch total loss 1.39287698\n",
      "Trained batch 1190 batch loss 1.36997962 epoch total loss 1.39285779\n",
      "Trained batch 1191 batch loss 1.26507878 epoch total loss 1.3927505\n",
      "Trained batch 1192 batch loss 1.18303502 epoch total loss 1.39257455\n",
      "Trained batch 1193 batch loss 1.22816086 epoch total loss 1.39243674\n",
      "Trained batch 1194 batch loss 1.18689442 epoch total loss 1.3922646\n",
      "Trained batch 1195 batch loss 1.16719508 epoch total loss 1.39207625\n",
      "Trained batch 1196 batch loss 1.27052891 epoch total loss 1.39197457\n",
      "Trained batch 1197 batch loss 1.32652664 epoch total loss 1.39192\n",
      "Trained batch 1198 batch loss 1.25398386 epoch total loss 1.39180481\n",
      "Trained batch 1199 batch loss 1.27815557 epoch total loss 1.39171\n",
      "Trained batch 1200 batch loss 1.23936033 epoch total loss 1.3915832\n",
      "Trained batch 1201 batch loss 1.44067883 epoch total loss 1.39162397\n",
      "Trained batch 1202 batch loss 1.2512871 epoch total loss 1.39150727\n",
      "Trained batch 1203 batch loss 1.37740731 epoch total loss 1.39149559\n",
      "Trained batch 1204 batch loss 1.20139277 epoch total loss 1.39133775\n",
      "Trained batch 1205 batch loss 1.2191844 epoch total loss 1.39119494\n",
      "Trained batch 1206 batch loss 1.25227106 epoch total loss 1.39107978\n",
      "Trained batch 1207 batch loss 1.32165968 epoch total loss 1.39102221\n",
      "Trained batch 1208 batch loss 1.28575742 epoch total loss 1.39093518\n",
      "Trained batch 1209 batch loss 1.47049677 epoch total loss 1.39100087\n",
      "Trained batch 1210 batch loss 1.50939417 epoch total loss 1.39109874\n",
      "Trained batch 1211 batch loss 1.57534957 epoch total loss 1.39125085\n",
      "Trained batch 1212 batch loss 1.29027259 epoch total loss 1.39116752\n",
      "Trained batch 1213 batch loss 1.28344178 epoch total loss 1.39107871\n",
      "Trained batch 1214 batch loss 1.34863102 epoch total loss 1.39104378\n",
      "Trained batch 1215 batch loss 1.40212607 epoch total loss 1.39105284\n",
      "Trained batch 1216 batch loss 1.45431614 epoch total loss 1.39110494\n",
      "Trained batch 1217 batch loss 1.43168545 epoch total loss 1.39113832\n",
      "Trained batch 1218 batch loss 1.45013952 epoch total loss 1.39118671\n",
      "Trained batch 1219 batch loss 1.52175283 epoch total loss 1.39129388\n",
      "Trained batch 1220 batch loss 1.431391 epoch total loss 1.39132667\n",
      "Trained batch 1221 batch loss 1.30557501 epoch total loss 1.39125645\n",
      "Trained batch 1222 batch loss 1.34127688 epoch total loss 1.39121556\n",
      "Trained batch 1223 batch loss 1.32284701 epoch total loss 1.39115965\n",
      "Trained batch 1224 batch loss 1.36731577 epoch total loss 1.39114022\n",
      "Trained batch 1225 batch loss 1.37297356 epoch total loss 1.39112532\n",
      "Trained batch 1226 batch loss 1.37908864 epoch total loss 1.39111543\n",
      "Trained batch 1227 batch loss 1.45341027 epoch total loss 1.39116621\n",
      "Trained batch 1228 batch loss 1.3737011 epoch total loss 1.39115191\n",
      "Trained batch 1229 batch loss 1.42118335 epoch total loss 1.39117634\n",
      "Trained batch 1230 batch loss 1.42167687 epoch total loss 1.39120114\n",
      "Trained batch 1231 batch loss 1.31626141 epoch total loss 1.39114022\n",
      "Trained batch 1232 batch loss 1.30850911 epoch total loss 1.39107311\n",
      "Trained batch 1233 batch loss 1.31719971 epoch total loss 1.39101326\n",
      "Trained batch 1234 batch loss 1.38542914 epoch total loss 1.39100873\n",
      "Trained batch 1235 batch loss 1.30829525 epoch total loss 1.39094174\n",
      "Trained batch 1236 batch loss 1.25607157 epoch total loss 1.39083266\n",
      "Trained batch 1237 batch loss 1.39801335 epoch total loss 1.3908385\n",
      "Trained batch 1238 batch loss 1.23216283 epoch total loss 1.39071035\n",
      "Trained batch 1239 batch loss 1.38662052 epoch total loss 1.39070702\n",
      "Trained batch 1240 batch loss 1.36822808 epoch total loss 1.39068902\n",
      "Trained batch 1241 batch loss 1.38318419 epoch total loss 1.39068294\n",
      "Trained batch 1242 batch loss 1.37973213 epoch total loss 1.39067411\n",
      "Trained batch 1243 batch loss 1.31321228 epoch total loss 1.39061189\n",
      "Trained batch 1244 batch loss 1.26629233 epoch total loss 1.39051187\n",
      "Trained batch 1245 batch loss 1.1557256 epoch total loss 1.39032328\n",
      "Trained batch 1246 batch loss 1.2130816 epoch total loss 1.39018106\n",
      "Trained batch 1247 batch loss 1.23205948 epoch total loss 1.39005435\n",
      "Trained batch 1248 batch loss 1.21547222 epoch total loss 1.38991439\n",
      "Trained batch 1249 batch loss 1.28930163 epoch total loss 1.38983381\n",
      "Trained batch 1250 batch loss 1.25389051 epoch total loss 1.38972509\n",
      "Trained batch 1251 batch loss 1.31533742 epoch total loss 1.3896656\n",
      "Trained batch 1252 batch loss 1.31876206 epoch total loss 1.38960898\n",
      "Trained batch 1253 batch loss 1.24656439 epoch total loss 1.38949478\n",
      "Trained batch 1254 batch loss 1.36813307 epoch total loss 1.38947785\n",
      "Trained batch 1255 batch loss 1.37463737 epoch total loss 1.38946593\n",
      "Trained batch 1256 batch loss 1.38790524 epoch total loss 1.38946474\n",
      "Trained batch 1257 batch loss 1.32861114 epoch total loss 1.38941634\n",
      "Trained batch 1258 batch loss 1.3411746 epoch total loss 1.38937795\n",
      "Trained batch 1259 batch loss 1.30233383 epoch total loss 1.38930893\n",
      "Trained batch 1260 batch loss 1.42663169 epoch total loss 1.38933849\n",
      "Trained batch 1261 batch loss 1.38098407 epoch total loss 1.38933194\n",
      "Trained batch 1262 batch loss 1.25967956 epoch total loss 1.38922918\n",
      "Trained batch 1263 batch loss 1.35692549 epoch total loss 1.38920355\n",
      "Trained batch 1264 batch loss 1.28861928 epoch total loss 1.38912392\n",
      "Trained batch 1265 batch loss 1.17617786 epoch total loss 1.38895559\n",
      "Trained batch 1266 batch loss 1.31325591 epoch total loss 1.38889575\n",
      "Trained batch 1267 batch loss 1.37474513 epoch total loss 1.38888466\n",
      "Trained batch 1268 batch loss 1.24195051 epoch total loss 1.38876867\n",
      "Trained batch 1269 batch loss 1.42039144 epoch total loss 1.38879371\n",
      "Trained batch 1270 batch loss 1.22328579 epoch total loss 1.38866329\n",
      "Trained batch 1271 batch loss 1.2946279 epoch total loss 1.38858938\n",
      "Trained batch 1272 batch loss 1.33534646 epoch total loss 1.38854754\n",
      "Trained batch 1273 batch loss 1.29581261 epoch total loss 1.38847458\n",
      "Trained batch 1274 batch loss 1.21165705 epoch total loss 1.38833582\n",
      "Trained batch 1275 batch loss 1.29947758 epoch total loss 1.38826609\n",
      "Trained batch 1276 batch loss 1.31383336 epoch total loss 1.38820779\n",
      "Trained batch 1277 batch loss 1.36723852 epoch total loss 1.38819134\n",
      "Trained batch 1278 batch loss 1.29733944 epoch total loss 1.38812029\n",
      "Trained batch 1279 batch loss 1.38053179 epoch total loss 1.38811433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1280 batch loss 1.32309103 epoch total loss 1.38806355\n",
      "Trained batch 1281 batch loss 1.28321862 epoch total loss 1.38798165\n",
      "Trained batch 1282 batch loss 1.21240842 epoch total loss 1.38784468\n",
      "Trained batch 1283 batch loss 1.34490418 epoch total loss 1.38781118\n",
      "Trained batch 1284 batch loss 1.31793332 epoch total loss 1.38775682\n",
      "Trained batch 1285 batch loss 1.43388391 epoch total loss 1.38779271\n",
      "Trained batch 1286 batch loss 1.39789581 epoch total loss 1.38780057\n",
      "Trained batch 1287 batch loss 1.3986578 epoch total loss 1.38780904\n",
      "Trained batch 1288 batch loss 1.37137032 epoch total loss 1.38779628\n",
      "Trained batch 1289 batch loss 1.40291238 epoch total loss 1.38780797\n",
      "Trained batch 1290 batch loss 1.38710952 epoch total loss 1.38780749\n",
      "Trained batch 1291 batch loss 1.306813 epoch total loss 1.38774467\n",
      "Trained batch 1292 batch loss 1.32495368 epoch total loss 1.38769603\n",
      "Trained batch 1293 batch loss 1.31460989 epoch total loss 1.38763952\n",
      "Trained batch 1294 batch loss 1.23752749 epoch total loss 1.38752353\n",
      "Trained batch 1295 batch loss 1.22856951 epoch total loss 1.38740075\n",
      "Trained batch 1296 batch loss 1.31831872 epoch total loss 1.38734746\n",
      "Trained batch 1297 batch loss 1.39835811 epoch total loss 1.38735592\n",
      "Trained batch 1298 batch loss 1.31019688 epoch total loss 1.38729644\n",
      "Trained batch 1299 batch loss 1.25081837 epoch total loss 1.38719141\n",
      "Trained batch 1300 batch loss 1.21694684 epoch total loss 1.3870604\n",
      "Trained batch 1301 batch loss 1.24176478 epoch total loss 1.38694882\n",
      "Trained batch 1302 batch loss 1.33937526 epoch total loss 1.38691223\n",
      "Trained batch 1303 batch loss 1.40152645 epoch total loss 1.38692343\n",
      "Trained batch 1304 batch loss 1.42615879 epoch total loss 1.38695347\n",
      "Trained batch 1305 batch loss 1.3455174 epoch total loss 1.38692176\n",
      "Trained batch 1306 batch loss 1.69235694 epoch total loss 1.38715565\n",
      "Trained batch 1307 batch loss 1.47223544 epoch total loss 1.38722074\n",
      "Trained batch 1308 batch loss 1.31385577 epoch total loss 1.38716471\n",
      "Trained batch 1309 batch loss 1.30816412 epoch total loss 1.38710427\n",
      "Trained batch 1310 batch loss 1.2815572 epoch total loss 1.38702369\n",
      "Trained batch 1311 batch loss 1.07386446 epoch total loss 1.38678491\n",
      "Trained batch 1312 batch loss 1.2657702 epoch total loss 1.38669264\n",
      "Trained batch 1313 batch loss 1.35907781 epoch total loss 1.38667166\n",
      "Trained batch 1314 batch loss 1.05646253 epoch total loss 1.38642037\n",
      "Trained batch 1315 batch loss 1.07144332 epoch total loss 1.38618076\n",
      "Trained batch 1316 batch loss 1.07088065 epoch total loss 1.38594127\n",
      "Trained batch 1317 batch loss 1.1943022 epoch total loss 1.38579571\n",
      "Trained batch 1318 batch loss 1.26550364 epoch total loss 1.38570452\n",
      "Trained batch 1319 batch loss 1.38221323 epoch total loss 1.38570189\n",
      "Trained batch 1320 batch loss 1.31805789 epoch total loss 1.38565063\n",
      "Trained batch 1321 batch loss 1.37877357 epoch total loss 1.38564539\n",
      "Trained batch 1322 batch loss 1.4766463 epoch total loss 1.38571429\n",
      "Trained batch 1323 batch loss 1.37127149 epoch total loss 1.38570333\n",
      "Trained batch 1324 batch loss 1.23670328 epoch total loss 1.38559079\n",
      "Trained batch 1325 batch loss 1.27202201 epoch total loss 1.38550508\n",
      "Trained batch 1326 batch loss 1.34465504 epoch total loss 1.38547421\n",
      "Trained batch 1327 batch loss 1.3221792 epoch total loss 1.38542652\n",
      "Trained batch 1328 batch loss 1.34601736 epoch total loss 1.38539684\n",
      "Trained batch 1329 batch loss 1.36506152 epoch total loss 1.38538158\n",
      "Trained batch 1330 batch loss 1.24361587 epoch total loss 1.38527501\n",
      "Trained batch 1331 batch loss 1.31669426 epoch total loss 1.38522351\n",
      "Trained batch 1332 batch loss 1.30427086 epoch total loss 1.38516271\n",
      "Trained batch 1333 batch loss 1.35752833 epoch total loss 1.38514197\n",
      "Trained batch 1334 batch loss 1.26491308 epoch total loss 1.38505185\n",
      "Trained batch 1335 batch loss 1.20895112 epoch total loss 1.38492\n",
      "Trained batch 1336 batch loss 1.28555632 epoch total loss 1.38484561\n",
      "Trained batch 1337 batch loss 1.29911077 epoch total loss 1.38478136\n",
      "Trained batch 1338 batch loss 1.4224422 epoch total loss 1.38480961\n",
      "Trained batch 1339 batch loss 1.35914779 epoch total loss 1.38479042\n",
      "Trained batch 1340 batch loss 1.25652337 epoch total loss 1.3846947\n",
      "Trained batch 1341 batch loss 1.19514346 epoch total loss 1.38455331\n",
      "Trained batch 1342 batch loss 1.18315876 epoch total loss 1.38440323\n",
      "Trained batch 1343 batch loss 1.14705765 epoch total loss 1.38422656\n",
      "Trained batch 1344 batch loss 1.26275182 epoch total loss 1.38413608\n",
      "Trained batch 1345 batch loss 1.28738344 epoch total loss 1.3840642\n",
      "Trained batch 1346 batch loss 1.25057745 epoch total loss 1.38396502\n",
      "Trained batch 1347 batch loss 1.40171671 epoch total loss 1.38397825\n",
      "Trained batch 1348 batch loss 1.32244658 epoch total loss 1.38393247\n",
      "Trained batch 1349 batch loss 1.30923378 epoch total loss 1.38387716\n",
      "Trained batch 1350 batch loss 1.30355704 epoch total loss 1.38381767\n",
      "Trained batch 1351 batch loss 1.31272244 epoch total loss 1.38376498\n",
      "Trained batch 1352 batch loss 1.22721481 epoch total loss 1.38364923\n",
      "Trained batch 1353 batch loss 1.29294896 epoch total loss 1.38358223\n",
      "Trained batch 1354 batch loss 1.20789742 epoch total loss 1.38345242\n",
      "Trained batch 1355 batch loss 1.17463791 epoch total loss 1.3832984\n",
      "Trained batch 1356 batch loss 1.1925658 epoch total loss 1.38315761\n",
      "Trained batch 1357 batch loss 1.26582122 epoch total loss 1.38307118\n",
      "Trained batch 1358 batch loss 1.18664968 epoch total loss 1.38292658\n",
      "Trained batch 1359 batch loss 1.31230915 epoch total loss 1.38287461\n",
      "Trained batch 1360 batch loss 1.33587 epoch total loss 1.38283992\n",
      "Trained batch 1361 batch loss 1.16801715 epoch total loss 1.38268209\n",
      "Trained batch 1362 batch loss 1.50568581 epoch total loss 1.38277245\n",
      "Trained batch 1363 batch loss 1.46861017 epoch total loss 1.38283539\n",
      "Trained batch 1364 batch loss 1.34396815 epoch total loss 1.3828069\n",
      "Trained batch 1365 batch loss 1.38834143 epoch total loss 1.38281095\n",
      "Trained batch 1366 batch loss 1.37120628 epoch total loss 1.38280249\n",
      "Trained batch 1367 batch loss 1.32871604 epoch total loss 1.38276291\n",
      "Trained batch 1368 batch loss 1.24940228 epoch total loss 1.3826654\n",
      "Trained batch 1369 batch loss 1.24242079 epoch total loss 1.38256299\n",
      "Trained batch 1370 batch loss 1.28617752 epoch total loss 1.38249266\n",
      "Trained batch 1371 batch loss 1.26337099 epoch total loss 1.38240576\n",
      "Trained batch 1372 batch loss 1.28845191 epoch total loss 1.38233733\n",
      "Trained batch 1373 batch loss 1.27631068 epoch total loss 1.38226008\n",
      "Trained batch 1374 batch loss 1.1950829 epoch total loss 1.38212383\n",
      "Trained batch 1375 batch loss 1.18844092 epoch total loss 1.38198304\n",
      "Trained batch 1376 batch loss 1.04297471 epoch total loss 1.38173664\n",
      "Trained batch 1377 batch loss 1.10078263 epoch total loss 1.38153267\n",
      "Trained batch 1378 batch loss 1.47316527 epoch total loss 1.38159919\n",
      "Trained batch 1379 batch loss 1.36794448 epoch total loss 1.38158917\n",
      "Trained batch 1380 batch loss 1.33291984 epoch total loss 1.38155389\n",
      "Trained batch 1381 batch loss 1.33724058 epoch total loss 1.38152182\n",
      "Trained batch 1382 batch loss 1.32655644 epoch total loss 1.38148212\n",
      "Trained batch 1383 batch loss 1.21549618 epoch total loss 1.38136208\n",
      "Trained batch 1384 batch loss 1.2315098 epoch total loss 1.38125384\n",
      "Trained batch 1385 batch loss 1.24759126 epoch total loss 1.38115728\n",
      "Trained batch 1386 batch loss 1.37278533 epoch total loss 1.3811512\n",
      "Trained batch 1387 batch loss 1.32580388 epoch total loss 1.38111138\n",
      "Trained batch 1388 batch loss 1.26404476 epoch total loss 1.38102698\n",
      "Epoch 2 train loss 1.3810269832611084\n",
      "Validated batch 1 batch loss 1.32311976\n",
      "Validated batch 2 batch loss 1.30781531\n",
      "Validated batch 3 batch loss 1.22573304\n",
      "Validated batch 4 batch loss 1.40414059\n",
      "Validated batch 5 batch loss 1.32861423\n",
      "Validated batch 6 batch loss 1.37520814\n",
      "Validated batch 7 batch loss 1.43031287\n",
      "Validated batch 8 batch loss 1.3836751\n",
      "Validated batch 9 batch loss 1.36419773\n",
      "Validated batch 10 batch loss 1.32932878\n",
      "Validated batch 11 batch loss 1.41496992\n",
      "Validated batch 12 batch loss 1.31859887\n",
      "Validated batch 13 batch loss 1.3221854\n",
      "Validated batch 14 batch loss 1.42217362\n",
      "Validated batch 15 batch loss 1.38124919\n",
      "Validated batch 16 batch loss 1.31492019\n",
      "Validated batch 17 batch loss 1.45510828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 18 batch loss 1.17191386\n",
      "Validated batch 19 batch loss 1.37267518\n",
      "Validated batch 20 batch loss 1.15165496\n",
      "Validated batch 21 batch loss 1.35251856\n",
      "Validated batch 22 batch loss 1.43099654\n",
      "Validated batch 23 batch loss 1.2487905\n",
      "Validated batch 24 batch loss 1.39161325\n",
      "Validated batch 25 batch loss 1.32486\n",
      "Validated batch 26 batch loss 1.28162277\n",
      "Validated batch 27 batch loss 1.27867293\n",
      "Validated batch 28 batch loss 1.35628927\n",
      "Validated batch 29 batch loss 1.47595787\n",
      "Validated batch 30 batch loss 1.23763359\n",
      "Validated batch 31 batch loss 1.35933161\n",
      "Validated batch 32 batch loss 1.35283244\n",
      "Validated batch 33 batch loss 1.34011149\n",
      "Validated batch 34 batch loss 1.3464756\n",
      "Validated batch 35 batch loss 1.20827842\n",
      "Validated batch 36 batch loss 1.4783529\n",
      "Validated batch 37 batch loss 1.22821927\n",
      "Validated batch 38 batch loss 1.34142482\n",
      "Validated batch 39 batch loss 1.34968352\n",
      "Validated batch 40 batch loss 1.38668728\n",
      "Validated batch 41 batch loss 1.19745767\n",
      "Validated batch 42 batch loss 1.2953496\n",
      "Validated batch 43 batch loss 1.29426146\n",
      "Validated batch 44 batch loss 1.33696163\n",
      "Validated batch 45 batch loss 1.34320641\n",
      "Validated batch 46 batch loss 1.3150574\n",
      "Validated batch 47 batch loss 1.23779559\n",
      "Validated batch 48 batch loss 1.37021375\n",
      "Validated batch 49 batch loss 1.37599218\n",
      "Validated batch 50 batch loss 1.26737523\n",
      "Validated batch 51 batch loss 1.39359438\n",
      "Validated batch 52 batch loss 1.45898271\n",
      "Validated batch 53 batch loss 1.19452322\n",
      "Validated batch 54 batch loss 1.36085725\n",
      "Validated batch 55 batch loss 1.28733921\n",
      "Validated batch 56 batch loss 1.3763473\n",
      "Validated batch 57 batch loss 1.35657704\n",
      "Validated batch 58 batch loss 1.20294178\n",
      "Validated batch 59 batch loss 1.20658302\n",
      "Validated batch 60 batch loss 1.31838524\n",
      "Validated batch 61 batch loss 1.31234634\n",
      "Validated batch 62 batch loss 1.25176358\n",
      "Validated batch 63 batch loss 1.3089484\n",
      "Validated batch 64 batch loss 1.25318766\n",
      "Validated batch 65 batch loss 1.33535564\n",
      "Validated batch 66 batch loss 1.35560489\n",
      "Validated batch 67 batch loss 1.33818603\n",
      "Validated batch 68 batch loss 1.35704374\n",
      "Validated batch 69 batch loss 1.20130301\n",
      "Validated batch 70 batch loss 1.25561202\n",
      "Validated batch 71 batch loss 1.22364688\n",
      "Validated batch 72 batch loss 1.31985378\n",
      "Validated batch 73 batch loss 1.24688911\n",
      "Validated batch 74 batch loss 1.29169083\n",
      "Validated batch 75 batch loss 1.39598346\n",
      "Validated batch 76 batch loss 1.33246756\n",
      "Validated batch 77 batch loss 1.43481886\n",
      "Validated batch 78 batch loss 1.39763308\n",
      "Validated batch 79 batch loss 1.3681066\n",
      "Validated batch 80 batch loss 1.31661654\n",
      "Validated batch 81 batch loss 1.46097183\n",
      "Validated batch 82 batch loss 1.37722981\n",
      "Validated batch 83 batch loss 1.4275049\n",
      "Validated batch 84 batch loss 1.43549287\n",
      "Validated batch 85 batch loss 1.38777304\n",
      "Validated batch 86 batch loss 1.38850284\n",
      "Validated batch 87 batch loss 1.22146297\n",
      "Validated batch 88 batch loss 1.30475247\n",
      "Validated batch 89 batch loss 1.34102321\n",
      "Validated batch 90 batch loss 1.37722373\n",
      "Validated batch 91 batch loss 1.32753658\n",
      "Validated batch 92 batch loss 1.31141853\n",
      "Validated batch 93 batch loss 1.36275148\n",
      "Validated batch 94 batch loss 1.40824008\n",
      "Validated batch 95 batch loss 1.2405293\n",
      "Validated batch 96 batch loss 1.28977704\n",
      "Validated batch 97 batch loss 1.35432\n",
      "Validated batch 98 batch loss 1.26489377\n",
      "Validated batch 99 batch loss 1.27734292\n",
      "Validated batch 100 batch loss 1.32679498\n",
      "Validated batch 101 batch loss 1.23497093\n",
      "Validated batch 102 batch loss 1.40583801\n",
      "Validated batch 103 batch loss 1.24077404\n",
      "Validated batch 104 batch loss 1.19647348\n",
      "Validated batch 105 batch loss 1.29243279\n",
      "Validated batch 106 batch loss 1.43508255\n",
      "Validated batch 107 batch loss 1.38482511\n",
      "Validated batch 108 batch loss 1.420421\n",
      "Validated batch 109 batch loss 1.26199186\n",
      "Validated batch 110 batch loss 1.48661327\n",
      "Validated batch 111 batch loss 1.30579734\n",
      "Validated batch 112 batch loss 1.39790404\n",
      "Validated batch 113 batch loss 1.36094725\n",
      "Validated batch 114 batch loss 1.0737741\n",
      "Validated batch 115 batch loss 1.31019104\n",
      "Validated batch 116 batch loss 1.32405734\n",
      "Validated batch 117 batch loss 1.34804666\n",
      "Validated batch 118 batch loss 1.27997959\n",
      "Validated batch 119 batch loss 1.1898402\n",
      "Validated batch 120 batch loss 1.22347128\n",
      "Validated batch 121 batch loss 1.42318606\n",
      "Validated batch 122 batch loss 1.22038913\n",
      "Validated batch 123 batch loss 1.20046413\n",
      "Validated batch 124 batch loss 1.30357409\n",
      "Validated batch 125 batch loss 1.33209527\n",
      "Validated batch 126 batch loss 1.23223615\n",
      "Validated batch 127 batch loss 1.3179996\n",
      "Validated batch 128 batch loss 1.2355926\n",
      "Validated batch 129 batch loss 1.23819\n",
      "Validated batch 130 batch loss 1.3902483\n",
      "Validated batch 131 batch loss 1.4246639\n",
      "Validated batch 132 batch loss 1.24722385\n",
      "Validated batch 133 batch loss 1.45891058\n",
      "Validated batch 134 batch loss 1.15963519\n",
      "Validated batch 135 batch loss 1.20533264\n",
      "Validated batch 136 batch loss 1.27067685\n",
      "Validated batch 137 batch loss 1.30383813\n",
      "Validated batch 138 batch loss 1.4429251\n",
      "Validated batch 139 batch loss 1.36526752\n",
      "Validated batch 140 batch loss 1.22128797\n",
      "Validated batch 141 batch loss 1.30060482\n",
      "Validated batch 142 batch loss 1.24542451\n",
      "Validated batch 143 batch loss 1.25226831\n",
      "Validated batch 144 batch loss 1.32138801\n",
      "Validated batch 145 batch loss 1.2862829\n",
      "Validated batch 146 batch loss 1.33927739\n",
      "Validated batch 147 batch loss 1.39059818\n",
      "Validated batch 148 batch loss 1.22701728\n",
      "Validated batch 149 batch loss 1.45099819\n",
      "Validated batch 150 batch loss 1.36558318\n",
      "Validated batch 151 batch loss 1.22315\n",
      "Validated batch 152 batch loss 1.32962787\n",
      "Validated batch 153 batch loss 1.34089565\n",
      "Validated batch 154 batch loss 1.28981113\n",
      "Validated batch 155 batch loss 1.45229936\n",
      "Validated batch 156 batch loss 1.34929371\n",
      "Validated batch 157 batch loss 1.37802947\n",
      "Validated batch 158 batch loss 1.27584863\n",
      "Validated batch 159 batch loss 1.31083393\n",
      "Validated batch 160 batch loss 1.30894423\n",
      "Validated batch 161 batch loss 1.28335452\n",
      "Validated batch 162 batch loss 1.42353046\n",
      "Validated batch 163 batch loss 1.31225419\n",
      "Validated batch 164 batch loss 1.36206532\n",
      "Validated batch 165 batch loss 1.32275915\n",
      "Validated batch 166 batch loss 1.23482585\n",
      "Validated batch 167 batch loss 1.42432046\n",
      "Validated batch 168 batch loss 1.36303234\n",
      "Validated batch 169 batch loss 1.27513123\n",
      "Validated batch 170 batch loss 1.23115456\n",
      "Validated batch 171 batch loss 1.33493268\n",
      "Validated batch 172 batch loss 1.34020078\n",
      "Validated batch 173 batch loss 1.37542748\n",
      "Validated batch 174 batch loss 1.32839453\n",
      "Validated batch 175 batch loss 1.20018625\n",
      "Validated batch 176 batch loss 1.31910169\n",
      "Validated batch 177 batch loss 1.30020237\n",
      "Validated batch 178 batch loss 1.27869105\n",
      "Validated batch 179 batch loss 1.37532818\n",
      "Validated batch 180 batch loss 1.40783775\n",
      "Validated batch 181 batch loss 1.52556777\n",
      "Validated batch 182 batch loss 1.49694681\n",
      "Validated batch 183 batch loss 1.34908688\n",
      "Validated batch 184 batch loss 1.23609209\n",
      "Validated batch 185 batch loss 1.18943489\n",
      "Epoch 2 val loss 1.3234355449676514\n",
      "Model /aiffel/aiffel/mpii/trained/model-epoch-2-loss-1.3234.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.38353097 epoch total loss 1.38353097\n",
      "Trained batch 2 batch loss 1.32693601 epoch total loss 1.35523343\n",
      "Trained batch 3 batch loss 1.37954223 epoch total loss 1.36333644\n",
      "Trained batch 4 batch loss 1.30554342 epoch total loss 1.34888816\n",
      "Trained batch 5 batch loss 1.38931119 epoch total loss 1.35697281\n",
      "Trained batch 6 batch loss 1.38308287 epoch total loss 1.36132443\n",
      "Trained batch 7 batch loss 1.34087467 epoch total loss 1.35840309\n",
      "Trained batch 8 batch loss 1.2826972 epoch total loss 1.3489399\n",
      "Trained batch 9 batch loss 1.21475649 epoch total loss 1.33403063\n",
      "Trained batch 10 batch loss 1.30931199 epoch total loss 1.3315587\n",
      "Trained batch 11 batch loss 1.36572909 epoch total loss 1.33466518\n",
      "Trained batch 12 batch loss 1.45865905 epoch total loss 1.34499788\n",
      "Trained batch 13 batch loss 1.34629917 epoch total loss 1.34509802\n",
      "Trained batch 14 batch loss 1.36797738 epoch total loss 1.34673226\n",
      "Trained batch 15 batch loss 1.3613472 epoch total loss 1.34770656\n",
      "Trained batch 16 batch loss 1.35462856 epoch total loss 1.34813929\n",
      "Trained batch 17 batch loss 1.27096593 epoch total loss 1.34359968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 18 batch loss 1.1743114 epoch total loss 1.33419466\n",
      "Trained batch 19 batch loss 1.34305942 epoch total loss 1.33466125\n",
      "Trained batch 20 batch loss 1.41639709 epoch total loss 1.3387481\n",
      "Trained batch 21 batch loss 1.38590705 epoch total loss 1.34099364\n",
      "Trained batch 22 batch loss 1.28374815 epoch total loss 1.33839166\n",
      "Trained batch 23 batch loss 1.20428646 epoch total loss 1.33256102\n",
      "Trained batch 24 batch loss 1.22348368 epoch total loss 1.32801616\n",
      "Trained batch 25 batch loss 1.23855042 epoch total loss 1.32443762\n",
      "Trained batch 26 batch loss 1.24813545 epoch total loss 1.3215028\n",
      "Trained batch 27 batch loss 1.32688618 epoch total loss 1.32170212\n",
      "Trained batch 28 batch loss 1.41028345 epoch total loss 1.3248657\n",
      "Trained batch 29 batch loss 1.3902719 epoch total loss 1.32712102\n",
      "Trained batch 30 batch loss 1.32202971 epoch total loss 1.32695138\n",
      "Trained batch 31 batch loss 1.38434899 epoch total loss 1.32880294\n",
      "Trained batch 32 batch loss 1.36846876 epoch total loss 1.33004248\n",
      "Trained batch 33 batch loss 1.41714621 epoch total loss 1.33268189\n",
      "Trained batch 34 batch loss 1.37096059 epoch total loss 1.33380783\n",
      "Trained batch 35 batch loss 1.37940967 epoch total loss 1.33511066\n",
      "Trained batch 36 batch loss 1.43115783 epoch total loss 1.33777857\n",
      "Trained batch 37 batch loss 1.37898564 epoch total loss 1.33889234\n",
      "Trained batch 38 batch loss 1.36184287 epoch total loss 1.33949625\n",
      "Trained batch 39 batch loss 1.33048236 epoch total loss 1.33926523\n",
      "Trained batch 40 batch loss 1.45297408 epoch total loss 1.34210789\n",
      "Trained batch 41 batch loss 1.46396923 epoch total loss 1.34508014\n",
      "Trained batch 42 batch loss 1.32288492 epoch total loss 1.34455168\n",
      "Trained batch 43 batch loss 1.33703279 epoch total loss 1.3443768\n",
      "Trained batch 44 batch loss 1.11049676 epoch total loss 1.33906126\n",
      "Trained batch 45 batch loss 1.0743494 epoch total loss 1.33317876\n",
      "Trained batch 46 batch loss 1.16584134 epoch total loss 1.32954097\n",
      "Trained batch 47 batch loss 1.33065081 epoch total loss 1.32956457\n",
      "Trained batch 48 batch loss 1.49893749 epoch total loss 1.33309317\n",
      "Trained batch 49 batch loss 1.59601188 epoch total loss 1.33845878\n",
      "Trained batch 50 batch loss 1.35035861 epoch total loss 1.33869672\n",
      "Trained batch 51 batch loss 1.31904721 epoch total loss 1.33831143\n",
      "Trained batch 52 batch loss 1.34260404 epoch total loss 1.33839405\n",
      "Trained batch 53 batch loss 1.383811 epoch total loss 1.33925092\n",
      "Trained batch 54 batch loss 1.39078033 epoch total loss 1.34020519\n",
      "Trained batch 55 batch loss 1.32889211 epoch total loss 1.33999956\n",
      "Trained batch 56 batch loss 1.330724 epoch total loss 1.33983397\n",
      "Trained batch 57 batch loss 1.43633461 epoch total loss 1.34152687\n",
      "Trained batch 58 batch loss 1.36068463 epoch total loss 1.34185719\n",
      "Trained batch 59 batch loss 1.33679533 epoch total loss 1.34177136\n",
      "Trained batch 60 batch loss 1.3028264 epoch total loss 1.34112227\n",
      "Trained batch 61 batch loss 1.25055027 epoch total loss 1.33963752\n",
      "Trained batch 62 batch loss 1.29795218 epoch total loss 1.33896518\n",
      "Trained batch 63 batch loss 1.34759581 epoch total loss 1.33910215\n",
      "Trained batch 64 batch loss 1.17609894 epoch total loss 1.33655524\n",
      "Trained batch 65 batch loss 1.09872711 epoch total loss 1.33289635\n",
      "Trained batch 66 batch loss 1.23309124 epoch total loss 1.33138418\n",
      "Trained batch 67 batch loss 1.26110721 epoch total loss 1.33033526\n",
      "Trained batch 68 batch loss 1.22743809 epoch total loss 1.32882214\n",
      "Trained batch 69 batch loss 1.16319573 epoch total loss 1.32642162\n",
      "Trained batch 70 batch loss 1.25005484 epoch total loss 1.32533073\n",
      "Trained batch 71 batch loss 1.18569708 epoch total loss 1.32336402\n",
      "Trained batch 72 batch loss 1.21883059 epoch total loss 1.32191229\n",
      "Trained batch 73 batch loss 1.20088768 epoch total loss 1.32025445\n",
      "Trained batch 74 batch loss 1.19459534 epoch total loss 1.31855631\n",
      "Trained batch 75 batch loss 1.34414601 epoch total loss 1.31889749\n",
      "Trained batch 76 batch loss 1.25951362 epoch total loss 1.31811619\n",
      "Trained batch 77 batch loss 1.14465463 epoch total loss 1.31586337\n",
      "Trained batch 78 batch loss 1.28126907 epoch total loss 1.31541991\n",
      "Trained batch 79 batch loss 1.43415987 epoch total loss 1.3169229\n",
      "Trained batch 80 batch loss 1.47420251 epoch total loss 1.3188889\n",
      "Trained batch 81 batch loss 1.40924895 epoch total loss 1.32000446\n",
      "Trained batch 82 batch loss 1.50737858 epoch total loss 1.32228959\n",
      "Trained batch 83 batch loss 1.47825837 epoch total loss 1.32416868\n",
      "Trained batch 84 batch loss 1.40319884 epoch total loss 1.32510948\n",
      "Trained batch 85 batch loss 1.37068057 epoch total loss 1.32564569\n",
      "Trained batch 86 batch loss 1.17827249 epoch total loss 1.32393205\n",
      "Trained batch 87 batch loss 1.4260453 epoch total loss 1.32510579\n",
      "Trained batch 88 batch loss 1.35758114 epoch total loss 1.32547486\n",
      "Trained batch 89 batch loss 1.45851815 epoch total loss 1.32696974\n",
      "Trained batch 90 batch loss 1.45406127 epoch total loss 1.3283819\n",
      "Trained batch 91 batch loss 1.43973923 epoch total loss 1.32960558\n",
      "Trained batch 92 batch loss 1.46553445 epoch total loss 1.33108318\n",
      "Trained batch 93 batch loss 1.3108139 epoch total loss 1.33086514\n",
      "Trained batch 94 batch loss 1.27890325 epoch total loss 1.33031237\n",
      "Trained batch 95 batch loss 1.38050008 epoch total loss 1.33084071\n",
      "Trained batch 96 batch loss 1.14367044 epoch total loss 1.32889092\n",
      "Trained batch 97 batch loss 1.14319193 epoch total loss 1.32697654\n",
      "Trained batch 98 batch loss 1.29054141 epoch total loss 1.32660472\n",
      "Trained batch 99 batch loss 1.2373724 epoch total loss 1.32570326\n",
      "Trained batch 100 batch loss 1.09065938 epoch total loss 1.32335281\n",
      "Trained batch 101 batch loss 1.07872331 epoch total loss 1.32093072\n",
      "Trained batch 102 batch loss 1.00247395 epoch total loss 1.31780851\n",
      "Trained batch 103 batch loss 1.0025996 epoch total loss 1.31474817\n",
      "Trained batch 104 batch loss 1.34037769 epoch total loss 1.31499469\n",
      "Trained batch 105 batch loss 1.41107428 epoch total loss 1.31590974\n",
      "Trained batch 106 batch loss 1.42608833 epoch total loss 1.31694913\n",
      "Trained batch 107 batch loss 1.2523725 epoch total loss 1.31634545\n",
      "Trained batch 108 batch loss 1.26878798 epoch total loss 1.31590509\n",
      "Trained batch 109 batch loss 1.41513431 epoch total loss 1.3168155\n",
      "Trained batch 110 batch loss 1.21813321 epoch total loss 1.31591833\n",
      "Trained batch 111 batch loss 1.40586829 epoch total loss 1.31672871\n",
      "Trained batch 112 batch loss 1.46751988 epoch total loss 1.31807506\n",
      "Trained batch 113 batch loss 1.27005303 epoch total loss 1.31765008\n",
      "Trained batch 114 batch loss 1.27579474 epoch total loss 1.3172828\n",
      "Trained batch 115 batch loss 1.22662461 epoch total loss 1.31649446\n",
      "Trained batch 116 batch loss 1.19558334 epoch total loss 1.31545222\n",
      "Trained batch 117 batch loss 1.28499866 epoch total loss 1.31519198\n",
      "Trained batch 118 batch loss 1.2518754 epoch total loss 1.31465542\n",
      "Trained batch 119 batch loss 1.37062275 epoch total loss 1.3151257\n",
      "Trained batch 120 batch loss 1.3383019 epoch total loss 1.31531882\n",
      "Trained batch 121 batch loss 1.2021358 epoch total loss 1.31438339\n",
      "Trained batch 122 batch loss 1.16391683 epoch total loss 1.31315\n",
      "Trained batch 123 batch loss 1.34872973 epoch total loss 1.31343925\n",
      "Trained batch 124 batch loss 1.40035701 epoch total loss 1.3141402\n",
      "Trained batch 125 batch loss 1.32732236 epoch total loss 1.31424558\n",
      "Trained batch 126 batch loss 1.28344476 epoch total loss 1.3140012\n",
      "Trained batch 127 batch loss 1.22615767 epoch total loss 1.31330943\n",
      "Trained batch 128 batch loss 1.12998867 epoch total loss 1.31187725\n",
      "Trained batch 129 batch loss 1.04912639 epoch total loss 1.30984044\n",
      "Trained batch 130 batch loss 0.993978 epoch total loss 1.30741072\n",
      "Trained batch 131 batch loss 1.06235468 epoch total loss 1.30554\n",
      "Trained batch 132 batch loss 1.05180621 epoch total loss 1.30361772\n",
      "Trained batch 133 batch loss 1.27782011 epoch total loss 1.30342376\n",
      "Trained batch 134 batch loss 1.23206437 epoch total loss 1.30289125\n",
      "Trained batch 135 batch loss 1.24253821 epoch total loss 1.30244422\n",
      "Trained batch 136 batch loss 1.21540451 epoch total loss 1.3018043\n",
      "Trained batch 137 batch loss 1.24550629 epoch total loss 1.30139339\n",
      "Trained batch 138 batch loss 1.21334994 epoch total loss 1.30075538\n",
      "Trained batch 139 batch loss 1.31778145 epoch total loss 1.30087781\n",
      "Trained batch 140 batch loss 1.33594704 epoch total loss 1.30112839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 141 batch loss 1.17730355 epoch total loss 1.30025017\n",
      "Trained batch 142 batch loss 1.21488464 epoch total loss 1.29964912\n",
      "Trained batch 143 batch loss 1.16761899 epoch total loss 1.29872584\n",
      "Trained batch 144 batch loss 1.23199463 epoch total loss 1.29826236\n",
      "Trained batch 145 batch loss 1.30613959 epoch total loss 1.29831672\n",
      "Trained batch 146 batch loss 1.3743118 epoch total loss 1.29883718\n",
      "Trained batch 147 batch loss 1.43689013 epoch total loss 1.29977632\n",
      "Trained batch 148 batch loss 1.47506285 epoch total loss 1.30096078\n",
      "Trained batch 149 batch loss 1.24893403 epoch total loss 1.30061162\n",
      "Trained batch 150 batch loss 1.43876648 epoch total loss 1.30153263\n",
      "Trained batch 151 batch loss 1.14748597 epoch total loss 1.30051243\n",
      "Trained batch 152 batch loss 1.33143067 epoch total loss 1.30071592\n",
      "Trained batch 153 batch loss 1.15439796 epoch total loss 1.29975963\n",
      "Trained batch 154 batch loss 1.42935956 epoch total loss 1.30060124\n",
      "Trained batch 155 batch loss 1.40153217 epoch total loss 1.30125237\n",
      "Trained batch 156 batch loss 1.39731264 epoch total loss 1.3018682\n",
      "Trained batch 157 batch loss 1.51943254 epoch total loss 1.30325401\n",
      "Trained batch 158 batch loss 1.5239687 epoch total loss 1.3046509\n",
      "Trained batch 159 batch loss 1.31457698 epoch total loss 1.30471337\n",
      "Trained batch 160 batch loss 1.31213939 epoch total loss 1.30475974\n",
      "Trained batch 161 batch loss 1.24976063 epoch total loss 1.30441809\n",
      "Trained batch 162 batch loss 1.28840077 epoch total loss 1.30431926\n",
      "Trained batch 163 batch loss 1.40026367 epoch total loss 1.30490792\n",
      "Trained batch 164 batch loss 1.33874643 epoch total loss 1.30511415\n",
      "Trained batch 165 batch loss 1.45841825 epoch total loss 1.30604327\n",
      "Trained batch 166 batch loss 1.47236931 epoch total loss 1.30704522\n",
      "Trained batch 167 batch loss 1.41396427 epoch total loss 1.30768549\n",
      "Trained batch 168 batch loss 1.48027503 epoch total loss 1.30871284\n",
      "Trained batch 169 batch loss 1.4516319 epoch total loss 1.30955851\n",
      "Trained batch 170 batch loss 1.43176484 epoch total loss 1.31027734\n",
      "Trained batch 171 batch loss 1.46038067 epoch total loss 1.3111552\n",
      "Trained batch 172 batch loss 1.44259059 epoch total loss 1.31191933\n",
      "Trained batch 173 batch loss 1.45045698 epoch total loss 1.31272018\n",
      "Trained batch 174 batch loss 1.3492645 epoch total loss 1.31293011\n",
      "Trained batch 175 batch loss 1.37325382 epoch total loss 1.31327486\n",
      "Trained batch 176 batch loss 1.40142465 epoch total loss 1.31377578\n",
      "Trained batch 177 batch loss 1.19904101 epoch total loss 1.31312752\n",
      "Trained batch 178 batch loss 1.25505006 epoch total loss 1.31280124\n",
      "Trained batch 179 batch loss 1.09771895 epoch total loss 1.31159961\n",
      "Trained batch 180 batch loss 1.27403569 epoch total loss 1.311391\n",
      "Trained batch 181 batch loss 1.37611175 epoch total loss 1.3117485\n",
      "Trained batch 182 batch loss 1.40301883 epoch total loss 1.31225\n",
      "Trained batch 183 batch loss 1.52072859 epoch total loss 1.31338918\n",
      "Trained batch 184 batch loss 1.43248272 epoch total loss 1.31403637\n",
      "Trained batch 185 batch loss 1.41544068 epoch total loss 1.31458449\n",
      "Trained batch 186 batch loss 1.35240722 epoch total loss 1.31478786\n",
      "Trained batch 187 batch loss 1.23554051 epoch total loss 1.31436408\n",
      "Trained batch 188 batch loss 1.22749794 epoch total loss 1.3139019\n",
      "Trained batch 189 batch loss 1.29001296 epoch total loss 1.31377554\n",
      "Trained batch 190 batch loss 1.31119764 epoch total loss 1.31376195\n",
      "Trained batch 191 batch loss 1.3641187 epoch total loss 1.31402564\n",
      "Trained batch 192 batch loss 1.3132956 epoch total loss 1.31402183\n",
      "Trained batch 193 batch loss 1.39727 epoch total loss 1.31445312\n",
      "Trained batch 194 batch loss 1.30903542 epoch total loss 1.31442523\n",
      "Trained batch 195 batch loss 1.26759863 epoch total loss 1.31418502\n",
      "Trained batch 196 batch loss 1.2529484 epoch total loss 1.31387269\n",
      "Trained batch 197 batch loss 1.26822078 epoch total loss 1.31364095\n",
      "Trained batch 198 batch loss 1.29838586 epoch total loss 1.31356394\n",
      "Trained batch 199 batch loss 1.28891373 epoch total loss 1.31344008\n",
      "Trained batch 200 batch loss 1.20952606 epoch total loss 1.31292057\n",
      "Trained batch 201 batch loss 1.27009284 epoch total loss 1.31270742\n",
      "Trained batch 202 batch loss 1.33310688 epoch total loss 1.31280839\n",
      "Trained batch 203 batch loss 1.32794249 epoch total loss 1.3128829\n",
      "Trained batch 204 batch loss 1.27423334 epoch total loss 1.31269348\n",
      "Trained batch 205 batch loss 1.33176744 epoch total loss 1.31278646\n",
      "Trained batch 206 batch loss 1.33543515 epoch total loss 1.31289649\n",
      "Trained batch 207 batch loss 1.27768707 epoch total loss 1.31272626\n",
      "Trained batch 208 batch loss 1.28117549 epoch total loss 1.31257463\n",
      "Trained batch 209 batch loss 1.31908703 epoch total loss 1.31260586\n",
      "Trained batch 210 batch loss 1.32362556 epoch total loss 1.31265843\n",
      "Trained batch 211 batch loss 1.48122835 epoch total loss 1.31345737\n",
      "Trained batch 212 batch loss 1.40350699 epoch total loss 1.31388211\n",
      "Trained batch 213 batch loss 1.20126092 epoch total loss 1.3133533\n",
      "Trained batch 214 batch loss 1.38766479 epoch total loss 1.31370056\n",
      "Trained batch 215 batch loss 1.29830682 epoch total loss 1.31362903\n",
      "Trained batch 216 batch loss 1.32518268 epoch total loss 1.31368256\n",
      "Trained batch 217 batch loss 1.40308821 epoch total loss 1.31409454\n",
      "Trained batch 218 batch loss 1.43073666 epoch total loss 1.31462955\n",
      "Trained batch 219 batch loss 1.25584817 epoch total loss 1.3143611\n",
      "Trained batch 220 batch loss 1.1402235 epoch total loss 1.31356966\n",
      "Trained batch 221 batch loss 1.18121159 epoch total loss 1.31297076\n",
      "Trained batch 222 batch loss 1.40049839 epoch total loss 1.3133651\n",
      "Trained batch 223 batch loss 1.36906719 epoch total loss 1.31361496\n",
      "Trained batch 224 batch loss 1.43168485 epoch total loss 1.31414199\n",
      "Trained batch 225 batch loss 1.35844934 epoch total loss 1.31433892\n",
      "Trained batch 226 batch loss 1.29567432 epoch total loss 1.31425643\n",
      "Trained batch 227 batch loss 1.35626984 epoch total loss 1.31444144\n",
      "Trained batch 228 batch loss 1.23800302 epoch total loss 1.31410623\n",
      "Trained batch 229 batch loss 1.45031297 epoch total loss 1.31470096\n",
      "Trained batch 230 batch loss 1.29548514 epoch total loss 1.3146174\n",
      "Trained batch 231 batch loss 1.26631343 epoch total loss 1.3144083\n",
      "Trained batch 232 batch loss 1.2483443 epoch total loss 1.31412363\n",
      "Trained batch 233 batch loss 1.37508583 epoch total loss 1.31438529\n",
      "Trained batch 234 batch loss 1.32588041 epoch total loss 1.31443429\n",
      "Trained batch 235 batch loss 1.35404229 epoch total loss 1.31460285\n",
      "Trained batch 236 batch loss 1.3104527 epoch total loss 1.31458533\n",
      "Trained batch 237 batch loss 1.35135102 epoch total loss 1.31474042\n",
      "Trained batch 238 batch loss 1.37920547 epoch total loss 1.31501126\n",
      "Trained batch 239 batch loss 1.29411948 epoch total loss 1.31492388\n",
      "Trained batch 240 batch loss 1.42254663 epoch total loss 1.31537235\n",
      "Trained batch 241 batch loss 1.16731071 epoch total loss 1.31475794\n",
      "Trained batch 242 batch loss 1.17384958 epoch total loss 1.31417572\n",
      "Trained batch 243 batch loss 1.09486818 epoch total loss 1.31327319\n",
      "Trained batch 244 batch loss 1.27092457 epoch total loss 1.31309974\n",
      "Trained batch 245 batch loss 1.28466988 epoch total loss 1.31298363\n",
      "Trained batch 246 batch loss 1.37832761 epoch total loss 1.31324935\n",
      "Trained batch 247 batch loss 1.46500027 epoch total loss 1.31386364\n",
      "Trained batch 248 batch loss 1.29053736 epoch total loss 1.31376958\n",
      "Trained batch 249 batch loss 1.38059521 epoch total loss 1.31403792\n",
      "Trained batch 250 batch loss 1.26811051 epoch total loss 1.3138541\n",
      "Trained batch 251 batch loss 1.41318095 epoch total loss 1.31424987\n",
      "Trained batch 252 batch loss 1.2978822 epoch total loss 1.3141849\n",
      "Trained batch 253 batch loss 1.42669988 epoch total loss 1.31462955\n",
      "Trained batch 254 batch loss 1.34990585 epoch total loss 1.31476855\n",
      "Trained batch 255 batch loss 1.51284075 epoch total loss 1.31554532\n",
      "Trained batch 256 batch loss 1.36717248 epoch total loss 1.31574702\n",
      "Trained batch 257 batch loss 1.40817869 epoch total loss 1.31610668\n",
      "Trained batch 258 batch loss 1.28496838 epoch total loss 1.31598604\n",
      "Trained batch 259 batch loss 1.23244905 epoch total loss 1.31566346\n",
      "Trained batch 260 batch loss 1.23626602 epoch total loss 1.31535804\n",
      "Trained batch 261 batch loss 1.22925651 epoch total loss 1.31502819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 262 batch loss 1.23417687 epoch total loss 1.31471968\n",
      "Trained batch 263 batch loss 1.22981727 epoch total loss 1.31439686\n",
      "Trained batch 264 batch loss 1.24527264 epoch total loss 1.31413496\n",
      "Trained batch 265 batch loss 1.26990914 epoch total loss 1.31396806\n",
      "Trained batch 266 batch loss 1.25149989 epoch total loss 1.31373322\n",
      "Trained batch 267 batch loss 1.44539082 epoch total loss 1.31422639\n",
      "Trained batch 268 batch loss 1.41244352 epoch total loss 1.31459284\n",
      "Trained batch 269 batch loss 1.34840417 epoch total loss 1.3147186\n",
      "Trained batch 270 batch loss 1.35984063 epoch total loss 1.31488574\n",
      "Trained batch 271 batch loss 1.27346408 epoch total loss 1.31473291\n",
      "Trained batch 272 batch loss 1.26797 epoch total loss 1.31456089\n",
      "Trained batch 273 batch loss 1.13631237 epoch total loss 1.3139081\n",
      "Trained batch 274 batch loss 1.20905781 epoch total loss 1.31352532\n",
      "Trained batch 275 batch loss 1.2942096 epoch total loss 1.3134551\n",
      "Trained batch 276 batch loss 1.45119429 epoch total loss 1.31395423\n",
      "Trained batch 277 batch loss 1.33752418 epoch total loss 1.31403935\n",
      "Trained batch 278 batch loss 1.28589642 epoch total loss 1.31393802\n",
      "Trained batch 279 batch loss 1.27039421 epoch total loss 1.31378198\n",
      "Trained batch 280 batch loss 1.29460287 epoch total loss 1.31371355\n",
      "Trained batch 281 batch loss 1.22720551 epoch total loss 1.31340563\n",
      "Trained batch 282 batch loss 1.15915477 epoch total loss 1.31285858\n",
      "Trained batch 283 batch loss 1.14331496 epoch total loss 1.31225955\n",
      "Trained batch 284 batch loss 1.14554334 epoch total loss 1.31167245\n",
      "Trained batch 285 batch loss 1.26150978 epoch total loss 1.3114965\n",
      "Trained batch 286 batch loss 1.34888387 epoch total loss 1.31162715\n",
      "Trained batch 287 batch loss 1.33039033 epoch total loss 1.31169248\n",
      "Trained batch 288 batch loss 1.38980377 epoch total loss 1.31196368\n",
      "Trained batch 289 batch loss 1.43813634 epoch total loss 1.31240034\n",
      "Trained batch 290 batch loss 1.41781664 epoch total loss 1.31276381\n",
      "Trained batch 291 batch loss 1.36756873 epoch total loss 1.31295204\n",
      "Trained batch 292 batch loss 1.5076642 epoch total loss 1.3136189\n",
      "Trained batch 293 batch loss 1.31883919 epoch total loss 1.31363678\n",
      "Trained batch 294 batch loss 1.31329989 epoch total loss 1.31363559\n",
      "Trained batch 295 batch loss 1.35757613 epoch total loss 1.31378448\n",
      "Trained batch 296 batch loss 1.43670893 epoch total loss 1.31419981\n",
      "Trained batch 297 batch loss 1.25304627 epoch total loss 1.31399393\n",
      "Trained batch 298 batch loss 1.25701571 epoch total loss 1.31380272\n",
      "Trained batch 299 batch loss 1.2336334 epoch total loss 1.31353462\n",
      "Trained batch 300 batch loss 1.24102271 epoch total loss 1.31329298\n",
      "Trained batch 301 batch loss 1.249928 epoch total loss 1.31308246\n",
      "Trained batch 302 batch loss 1.26202703 epoch total loss 1.31291342\n",
      "Trained batch 303 batch loss 1.27172709 epoch total loss 1.31277752\n",
      "Trained batch 304 batch loss 1.28718686 epoch total loss 1.31269336\n",
      "Trained batch 305 batch loss 1.32184315 epoch total loss 1.31272328\n",
      "Trained batch 306 batch loss 1.37486935 epoch total loss 1.31292641\n",
      "Trained batch 307 batch loss 1.16713226 epoch total loss 1.3124516\n",
      "Trained batch 308 batch loss 1.13584864 epoch total loss 1.3118782\n",
      "Trained batch 309 batch loss 1.19606125 epoch total loss 1.31150341\n",
      "Trained batch 310 batch loss 1.49851274 epoch total loss 1.31210661\n",
      "Trained batch 311 batch loss 1.49696052 epoch total loss 1.31270099\n",
      "Trained batch 312 batch loss 1.29344809 epoch total loss 1.31263924\n",
      "Trained batch 313 batch loss 1.30610967 epoch total loss 1.31261849\n",
      "Trained batch 314 batch loss 1.29840541 epoch total loss 1.31257319\n",
      "Trained batch 315 batch loss 1.29885948 epoch total loss 1.31252968\n",
      "Trained batch 316 batch loss 1.29137897 epoch total loss 1.31246269\n",
      "Trained batch 317 batch loss 1.24587274 epoch total loss 1.31225264\n",
      "Trained batch 318 batch loss 1.27867246 epoch total loss 1.31214714\n",
      "Trained batch 319 batch loss 1.31496525 epoch total loss 1.31215596\n",
      "Trained batch 320 batch loss 1.34870434 epoch total loss 1.31227016\n",
      "Trained batch 321 batch loss 1.33185685 epoch total loss 1.3123312\n",
      "Trained batch 322 batch loss 1.35239518 epoch total loss 1.31245553\n",
      "Trained batch 323 batch loss 1.26158452 epoch total loss 1.31229806\n",
      "Trained batch 324 batch loss 1.22725022 epoch total loss 1.31203568\n",
      "Trained batch 325 batch loss 1.31627977 epoch total loss 1.31204867\n",
      "Trained batch 326 batch loss 1.33226728 epoch total loss 1.31211078\n",
      "Trained batch 327 batch loss 1.18739319 epoch total loss 1.31172931\n",
      "Trained batch 328 batch loss 1.39243782 epoch total loss 1.31197536\n",
      "Trained batch 329 batch loss 1.29215407 epoch total loss 1.31191504\n",
      "Trained batch 330 batch loss 1.31763303 epoch total loss 1.31193233\n",
      "Trained batch 331 batch loss 1.32811141 epoch total loss 1.31198132\n",
      "Trained batch 332 batch loss 1.24923098 epoch total loss 1.31179225\n",
      "Trained batch 333 batch loss 1.38749301 epoch total loss 1.31201959\n",
      "Trained batch 334 batch loss 1.33049333 epoch total loss 1.3120749\n",
      "Trained batch 335 batch loss 1.17593622 epoch total loss 1.31166852\n",
      "Trained batch 336 batch loss 1.08801377 epoch total loss 1.31100297\n",
      "Trained batch 337 batch loss 1.22731829 epoch total loss 1.31075466\n",
      "Trained batch 338 batch loss 1.30660534 epoch total loss 1.31074238\n",
      "Trained batch 339 batch loss 1.20363915 epoch total loss 1.31042647\n",
      "Trained batch 340 batch loss 1.28377485 epoch total loss 1.31034803\n",
      "Trained batch 341 batch loss 1.35892987 epoch total loss 1.31049049\n",
      "Trained batch 342 batch loss 1.26214623 epoch total loss 1.31034911\n",
      "Trained batch 343 batch loss 1.3592248 epoch total loss 1.31049168\n",
      "Trained batch 344 batch loss 1.40837526 epoch total loss 1.31077623\n",
      "Trained batch 345 batch loss 1.62191439 epoch total loss 1.31167805\n",
      "Trained batch 346 batch loss 1.49429965 epoch total loss 1.31220579\n",
      "Trained batch 347 batch loss 1.41475558 epoch total loss 1.31250143\n",
      "Trained batch 348 batch loss 1.27944279 epoch total loss 1.31240642\n",
      "Trained batch 349 batch loss 1.36940241 epoch total loss 1.31256974\n",
      "Trained batch 350 batch loss 1.39523613 epoch total loss 1.31280601\n",
      "Trained batch 351 batch loss 1.56758237 epoch total loss 1.31353188\n",
      "Trained batch 352 batch loss 1.49108791 epoch total loss 1.31403625\n",
      "Trained batch 353 batch loss 1.44990015 epoch total loss 1.31442118\n",
      "Trained batch 354 batch loss 1.40716171 epoch total loss 1.31468308\n",
      "Trained batch 355 batch loss 1.36766303 epoch total loss 1.31483245\n",
      "Trained batch 356 batch loss 1.2142396 epoch total loss 1.3145498\n",
      "Trained batch 357 batch loss 1.27716255 epoch total loss 1.31444514\n",
      "Trained batch 358 batch loss 1.32743859 epoch total loss 1.31448138\n",
      "Trained batch 359 batch loss 1.36735451 epoch total loss 1.31462872\n",
      "Trained batch 360 batch loss 1.3806721 epoch total loss 1.31481218\n",
      "Trained batch 361 batch loss 1.27092409 epoch total loss 1.31469059\n",
      "Trained batch 362 batch loss 1.24586582 epoch total loss 1.31450045\n",
      "Trained batch 363 batch loss 1.20724344 epoch total loss 1.31420505\n",
      "Trained batch 364 batch loss 1.17319047 epoch total loss 1.31381762\n",
      "Trained batch 365 batch loss 1.21970439 epoch total loss 1.31355977\n",
      "Trained batch 366 batch loss 1.26709771 epoch total loss 1.31343281\n",
      "Trained batch 367 batch loss 1.24619043 epoch total loss 1.31324959\n",
      "Trained batch 368 batch loss 1.2992748 epoch total loss 1.31321156\n",
      "Trained batch 369 batch loss 1.28102255 epoch total loss 1.31312442\n",
      "Trained batch 370 batch loss 1.45026922 epoch total loss 1.31349504\n",
      "Trained batch 371 batch loss 1.37441468 epoch total loss 1.31365931\n",
      "Trained batch 372 batch loss 1.3839649 epoch total loss 1.31384826\n",
      "Trained batch 373 batch loss 1.36423111 epoch total loss 1.31398332\n",
      "Trained batch 374 batch loss 1.38197446 epoch total loss 1.31416512\n",
      "Trained batch 375 batch loss 1.3132596 epoch total loss 1.31416273\n",
      "Trained batch 376 batch loss 1.37092698 epoch total loss 1.31431377\n",
      "Trained batch 377 batch loss 1.34560692 epoch total loss 1.31439674\n",
      "Trained batch 378 batch loss 1.32039428 epoch total loss 1.31441271\n",
      "Trained batch 379 batch loss 1.24682415 epoch total loss 1.31423438\n",
      "Trained batch 380 batch loss 1.25852573 epoch total loss 1.31408775\n",
      "Trained batch 381 batch loss 1.32513499 epoch total loss 1.31411672\n",
      "Trained batch 382 batch loss 1.25204086 epoch total loss 1.31395423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 383 batch loss 1.23479617 epoch total loss 1.31374753\n",
      "Trained batch 384 batch loss 1.17366982 epoch total loss 1.31338274\n",
      "Trained batch 385 batch loss 1.22430766 epoch total loss 1.31315136\n",
      "Trained batch 386 batch loss 1.23354721 epoch total loss 1.31294525\n",
      "Trained batch 387 batch loss 1.32520819 epoch total loss 1.31297684\n",
      "Trained batch 388 batch loss 1.26655126 epoch total loss 1.31285715\n",
      "Trained batch 389 batch loss 1.41517377 epoch total loss 1.31312013\n",
      "Trained batch 390 batch loss 1.32105243 epoch total loss 1.31314051\n",
      "Trained batch 391 batch loss 1.39067221 epoch total loss 1.31333888\n",
      "Trained batch 392 batch loss 1.33818126 epoch total loss 1.3134023\n",
      "Trained batch 393 batch loss 1.19706798 epoch total loss 1.3131063\n",
      "Trained batch 394 batch loss 1.31349754 epoch total loss 1.31310725\n",
      "Trained batch 395 batch loss 1.32508159 epoch total loss 1.31313753\n",
      "Trained batch 396 batch loss 1.19695449 epoch total loss 1.31284416\n",
      "Trained batch 397 batch loss 1.42730129 epoch total loss 1.31313252\n",
      "Trained batch 398 batch loss 1.31382751 epoch total loss 1.31313431\n",
      "Trained batch 399 batch loss 1.32838237 epoch total loss 1.31317246\n",
      "Trained batch 400 batch loss 1.23534667 epoch total loss 1.31297791\n",
      "Trained batch 401 batch loss 1.14708042 epoch total loss 1.31256425\n",
      "Trained batch 402 batch loss 1.17221069 epoch total loss 1.31221521\n",
      "Trained batch 403 batch loss 1.19080293 epoch total loss 1.31191385\n",
      "Trained batch 404 batch loss 1.13289654 epoch total loss 1.31147075\n",
      "Trained batch 405 batch loss 1.20393753 epoch total loss 1.31120515\n",
      "Trained batch 406 batch loss 1.14336681 epoch total loss 1.31079173\n",
      "Trained batch 407 batch loss 1.29385066 epoch total loss 1.31075013\n",
      "Trained batch 408 batch loss 1.30179834 epoch total loss 1.31072819\n",
      "Trained batch 409 batch loss 1.28064489 epoch total loss 1.31065464\n",
      "Trained batch 410 batch loss 1.32394433 epoch total loss 1.31068707\n",
      "Trained batch 411 batch loss 1.38422346 epoch total loss 1.310866\n",
      "Trained batch 412 batch loss 1.28349328 epoch total loss 1.3107996\n",
      "Trained batch 413 batch loss 1.25624049 epoch total loss 1.31066751\n",
      "Trained batch 414 batch loss 1.27732778 epoch total loss 1.31058693\n",
      "Trained batch 415 batch loss 1.29033017 epoch total loss 1.31053817\n",
      "Trained batch 416 batch loss 1.29301977 epoch total loss 1.31049609\n",
      "Trained batch 417 batch loss 1.22594905 epoch total loss 1.31029332\n",
      "Trained batch 418 batch loss 1.20632195 epoch total loss 1.31004453\n",
      "Trained batch 419 batch loss 1.17255318 epoch total loss 1.30971646\n",
      "Trained batch 420 batch loss 1.32369101 epoch total loss 1.3097496\n",
      "Trained batch 421 batch loss 1.25602722 epoch total loss 1.30962205\n",
      "Trained batch 422 batch loss 1.37204814 epoch total loss 1.30977011\n",
      "Trained batch 423 batch loss 1.32988119 epoch total loss 1.30981767\n",
      "Trained batch 424 batch loss 1.18063438 epoch total loss 1.30951297\n",
      "Trained batch 425 batch loss 1.31182408 epoch total loss 1.30951846\n",
      "Trained batch 426 batch loss 1.27808022 epoch total loss 1.30944467\n",
      "Trained batch 427 batch loss 1.35163355 epoch total loss 1.30954349\n",
      "Trained batch 428 batch loss 1.3142705 epoch total loss 1.30955446\n",
      "Trained batch 429 batch loss 1.35891175 epoch total loss 1.30966949\n",
      "Trained batch 430 batch loss 1.22362649 epoch total loss 1.30946934\n",
      "Trained batch 431 batch loss 1.14316726 epoch total loss 1.30908358\n",
      "Trained batch 432 batch loss 1.25951672 epoch total loss 1.3089689\n",
      "Trained batch 433 batch loss 1.20883834 epoch total loss 1.30873764\n",
      "Trained batch 434 batch loss 1.32417238 epoch total loss 1.30877316\n",
      "Trained batch 435 batch loss 1.23883271 epoch total loss 1.30861247\n",
      "Trained batch 436 batch loss 1.28015351 epoch total loss 1.30854714\n",
      "Trained batch 437 batch loss 1.29823768 epoch total loss 1.30852354\n",
      "Trained batch 438 batch loss 1.31321418 epoch total loss 1.30853426\n",
      "Trained batch 439 batch loss 1.19692051 epoch total loss 1.30828\n",
      "Trained batch 440 batch loss 1.21238279 epoch total loss 1.30806208\n",
      "Trained batch 441 batch loss 1.2670145 epoch total loss 1.30796897\n",
      "Trained batch 442 batch loss 1.19813669 epoch total loss 1.30772042\n",
      "Trained batch 443 batch loss 1.15157604 epoch total loss 1.30736792\n",
      "Trained batch 444 batch loss 1.13450778 epoch total loss 1.3069787\n",
      "Trained batch 445 batch loss 1.15412581 epoch total loss 1.30663514\n",
      "Trained batch 446 batch loss 1.25957811 epoch total loss 1.30652964\n",
      "Trained batch 447 batch loss 1.20445979 epoch total loss 1.30630136\n",
      "Trained batch 448 batch loss 1.13419533 epoch total loss 1.30591714\n",
      "Trained batch 449 batch loss 1.24378645 epoch total loss 1.30577874\n",
      "Trained batch 450 batch loss 1.22033119 epoch total loss 1.30558896\n",
      "Trained batch 451 batch loss 1.39436722 epoch total loss 1.30578578\n",
      "Trained batch 452 batch loss 1.38639295 epoch total loss 1.30596411\n",
      "Trained batch 453 batch loss 1.35136735 epoch total loss 1.30606437\n",
      "Trained batch 454 batch loss 1.39273655 epoch total loss 1.30625534\n",
      "Trained batch 455 batch loss 1.39220679 epoch total loss 1.30644429\n",
      "Trained batch 456 batch loss 1.32289279 epoch total loss 1.30648029\n",
      "Trained batch 457 batch loss 1.3973918 epoch total loss 1.30667925\n",
      "Trained batch 458 batch loss 1.24280143 epoch total loss 1.30653977\n",
      "Trained batch 459 batch loss 1.29599547 epoch total loss 1.30651677\n",
      "Trained batch 460 batch loss 1.38156581 epoch total loss 1.30668008\n",
      "Trained batch 461 batch loss 1.34021306 epoch total loss 1.3067528\n",
      "Trained batch 462 batch loss 1.34076941 epoch total loss 1.30682635\n",
      "Trained batch 463 batch loss 1.3938899 epoch total loss 1.30701435\n",
      "Trained batch 464 batch loss 1.40941525 epoch total loss 1.307235\n",
      "Trained batch 465 batch loss 1.2612536 epoch total loss 1.30713618\n",
      "Trained batch 466 batch loss 1.33952439 epoch total loss 1.30720568\n",
      "Trained batch 467 batch loss 1.29403496 epoch total loss 1.30717742\n",
      "Trained batch 468 batch loss 1.36776209 epoch total loss 1.30730677\n",
      "Trained batch 469 batch loss 1.27820611 epoch total loss 1.30724478\n",
      "Trained batch 470 batch loss 1.19357073 epoch total loss 1.30700278\n",
      "Trained batch 471 batch loss 1.21012402 epoch total loss 1.30679715\n",
      "Trained batch 472 batch loss 1.13758814 epoch total loss 1.30643868\n",
      "Trained batch 473 batch loss 1.1878736 epoch total loss 1.30618799\n",
      "Trained batch 474 batch loss 1.24871242 epoch total loss 1.30606675\n",
      "Trained batch 475 batch loss 1.1486789 epoch total loss 1.30573535\n",
      "Trained batch 476 batch loss 1.16047716 epoch total loss 1.30543017\n",
      "Trained batch 477 batch loss 1.22403657 epoch total loss 1.30525959\n",
      "Trained batch 478 batch loss 1.25734186 epoch total loss 1.30515933\n",
      "Trained batch 479 batch loss 1.37100339 epoch total loss 1.30529678\n",
      "Trained batch 480 batch loss 1.23662376 epoch total loss 1.30515373\n",
      "Trained batch 481 batch loss 1.42601037 epoch total loss 1.30540502\n",
      "Trained batch 482 batch loss 1.45015311 epoch total loss 1.30570531\n",
      "Trained batch 483 batch loss 1.45727813 epoch total loss 1.30601919\n",
      "Trained batch 484 batch loss 1.53634357 epoch total loss 1.30649495\n",
      "Trained batch 485 batch loss 1.35034633 epoch total loss 1.30658543\n",
      "Trained batch 486 batch loss 1.21226954 epoch total loss 1.30639136\n",
      "Trained batch 487 batch loss 1.30531323 epoch total loss 1.30638909\n",
      "Trained batch 488 batch loss 1.22726941 epoch total loss 1.30622697\n",
      "Trained batch 489 batch loss 1.2103405 epoch total loss 1.30603087\n",
      "Trained batch 490 batch loss 1.22129428 epoch total loss 1.30585802\n",
      "Trained batch 491 batch loss 1.18479204 epoch total loss 1.30561149\n",
      "Trained batch 492 batch loss 1.29527187 epoch total loss 1.30559051\n",
      "Trained batch 493 batch loss 1.09730959 epoch total loss 1.30516803\n",
      "Trained batch 494 batch loss 1.15542018 epoch total loss 1.30486476\n",
      "Trained batch 495 batch loss 1.13125813 epoch total loss 1.30451417\n",
      "Trained batch 496 batch loss 1.3014698 epoch total loss 1.30450797\n",
      "Trained batch 497 batch loss 1.15184975 epoch total loss 1.30420077\n",
      "Trained batch 498 batch loss 1.26705 epoch total loss 1.30412614\n",
      "Trained batch 499 batch loss 1.31651378 epoch total loss 1.30415106\n",
      "Trained batch 500 batch loss 1.34009528 epoch total loss 1.30422294\n",
      "Trained batch 501 batch loss 1.4862715 epoch total loss 1.30458629\n",
      "Trained batch 502 batch loss 1.35127532 epoch total loss 1.30467927\n",
      "Trained batch 503 batch loss 1.37784266 epoch total loss 1.30482471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 504 batch loss 1.34899235 epoch total loss 1.30491233\n",
      "Trained batch 505 batch loss 1.28166091 epoch total loss 1.30486643\n",
      "Trained batch 506 batch loss 1.3246094 epoch total loss 1.3049053\n",
      "Trained batch 507 batch loss 1.33118248 epoch total loss 1.30495715\n",
      "Trained batch 508 batch loss 1.24878407 epoch total loss 1.30484653\n",
      "Trained batch 509 batch loss 1.36639857 epoch total loss 1.30496752\n",
      "Trained batch 510 batch loss 1.29238927 epoch total loss 1.30494285\n",
      "Trained batch 511 batch loss 1.36141205 epoch total loss 1.30505335\n",
      "Trained batch 512 batch loss 1.28058529 epoch total loss 1.30500555\n",
      "Trained batch 513 batch loss 1.28304291 epoch total loss 1.30496264\n",
      "Trained batch 514 batch loss 1.24917924 epoch total loss 1.30485427\n",
      "Trained batch 515 batch loss 1.21037042 epoch total loss 1.30467081\n",
      "Trained batch 516 batch loss 1.23447013 epoch total loss 1.30453479\n",
      "Trained batch 517 batch loss 1.18722856 epoch total loss 1.30430794\n",
      "Trained batch 518 batch loss 1.19931316 epoch total loss 1.30410528\n",
      "Trained batch 519 batch loss 1.28348851 epoch total loss 1.30406559\n",
      "Trained batch 520 batch loss 1.32872939 epoch total loss 1.30411303\n",
      "Trained batch 521 batch loss 1.34616971 epoch total loss 1.30419385\n",
      "Trained batch 522 batch loss 1.35977387 epoch total loss 1.30430031\n",
      "Trained batch 523 batch loss 1.24545658 epoch total loss 1.30418789\n",
      "Trained batch 524 batch loss 1.10600984 epoch total loss 1.30380976\n",
      "Trained batch 525 batch loss 1.09215188 epoch total loss 1.3034066\n",
      "Trained batch 526 batch loss 1.18196797 epoch total loss 1.30317569\n",
      "Trained batch 527 batch loss 1.17377615 epoch total loss 1.30293012\n",
      "Trained batch 528 batch loss 1.19409418 epoch total loss 1.302724\n",
      "Trained batch 529 batch loss 1.29146457 epoch total loss 1.30270267\n",
      "Trained batch 530 batch loss 1.29857123 epoch total loss 1.30269492\n",
      "Trained batch 531 batch loss 1.3147738 epoch total loss 1.30271757\n",
      "Trained batch 532 batch loss 1.42820084 epoch total loss 1.30295348\n",
      "Trained batch 533 batch loss 1.46391702 epoch total loss 1.30325556\n",
      "Trained batch 534 batch loss 1.20824373 epoch total loss 1.30307758\n",
      "Trained batch 535 batch loss 1.10713971 epoch total loss 1.30271137\n",
      "Trained batch 536 batch loss 1.3069315 epoch total loss 1.30271924\n",
      "Trained batch 537 batch loss 1.31036973 epoch total loss 1.30273342\n",
      "Trained batch 538 batch loss 1.40967584 epoch total loss 1.30293226\n",
      "Trained batch 539 batch loss 1.44357204 epoch total loss 1.30319309\n",
      "Trained batch 540 batch loss 1.34714174 epoch total loss 1.30327451\n",
      "Trained batch 541 batch loss 1.27214134 epoch total loss 1.30321705\n",
      "Trained batch 542 batch loss 1.19405389 epoch total loss 1.30301559\n",
      "Trained batch 543 batch loss 1.18050718 epoch total loss 1.30278993\n",
      "Trained batch 544 batch loss 1.32776117 epoch total loss 1.30283582\n",
      "Trained batch 545 batch loss 1.41436315 epoch total loss 1.3030405\n",
      "Trained batch 546 batch loss 1.37388635 epoch total loss 1.3031702\n",
      "Trained batch 547 batch loss 1.17751765 epoch total loss 1.30294049\n",
      "Trained batch 548 batch loss 1.12391174 epoch total loss 1.30261374\n",
      "Trained batch 549 batch loss 1.29068327 epoch total loss 1.30259204\n",
      "Trained batch 550 batch loss 1.27571166 epoch total loss 1.30254316\n",
      "Trained batch 551 batch loss 1.39266229 epoch total loss 1.30270672\n",
      "Trained batch 552 batch loss 1.2973845 epoch total loss 1.30269694\n",
      "Trained batch 553 batch loss 1.3090502 epoch total loss 1.30270839\n",
      "Trained batch 554 batch loss 1.23907495 epoch total loss 1.30259359\n",
      "Trained batch 555 batch loss 1.24162185 epoch total loss 1.3024838\n",
      "Trained batch 556 batch loss 1.21346819 epoch total loss 1.30232358\n",
      "Trained batch 557 batch loss 1.31835723 epoch total loss 1.30235243\n",
      "Trained batch 558 batch loss 1.49475551 epoch total loss 1.30269718\n",
      "Trained batch 559 batch loss 1.31345654 epoch total loss 1.30271649\n",
      "Trained batch 560 batch loss 1.323627 epoch total loss 1.30275381\n",
      "Trained batch 561 batch loss 1.29433811 epoch total loss 1.30273879\n",
      "Trained batch 562 batch loss 1.1813457 epoch total loss 1.30252266\n",
      "Trained batch 563 batch loss 1.31917107 epoch total loss 1.30255222\n",
      "Trained batch 564 batch loss 1.34219384 epoch total loss 1.30262256\n",
      "Trained batch 565 batch loss 1.20556545 epoch total loss 1.30245078\n",
      "Trained batch 566 batch loss 1.21087861 epoch total loss 1.30228901\n",
      "Trained batch 567 batch loss 1.27240717 epoch total loss 1.30223632\n",
      "Trained batch 568 batch loss 1.25562334 epoch total loss 1.30215418\n",
      "Trained batch 569 batch loss 1.25958848 epoch total loss 1.30207944\n",
      "Trained batch 570 batch loss 1.45475268 epoch total loss 1.3023473\n",
      "Trained batch 571 batch loss 1.3006171 epoch total loss 1.3023442\n",
      "Trained batch 572 batch loss 1.21883142 epoch total loss 1.30219817\n",
      "Trained batch 573 batch loss 1.31681514 epoch total loss 1.30222368\n",
      "Trained batch 574 batch loss 1.16719627 epoch total loss 1.30198848\n",
      "Trained batch 575 batch loss 1.26757479 epoch total loss 1.30192864\n",
      "Trained batch 576 batch loss 1.19428861 epoch total loss 1.30174172\n",
      "Trained batch 577 batch loss 1.38478696 epoch total loss 1.3018856\n",
      "Trained batch 578 batch loss 1.41015792 epoch total loss 1.30207288\n",
      "Trained batch 579 batch loss 1.29902649 epoch total loss 1.30206764\n",
      "Trained batch 580 batch loss 1.32057011 epoch total loss 1.30209947\n",
      "Trained batch 581 batch loss 1.28943872 epoch total loss 1.30207765\n",
      "Trained batch 582 batch loss 1.20101166 epoch total loss 1.30190396\n",
      "Trained batch 583 batch loss 1.31564021 epoch total loss 1.30192757\n",
      "Trained batch 584 batch loss 1.26871085 epoch total loss 1.3018707\n",
      "Trained batch 585 batch loss 1.42243671 epoch total loss 1.30207682\n",
      "Trained batch 586 batch loss 1.34356117 epoch total loss 1.30214751\n",
      "Trained batch 587 batch loss 1.33692694 epoch total loss 1.30220675\n",
      "Trained batch 588 batch loss 1.31862152 epoch total loss 1.30223465\n",
      "Trained batch 589 batch loss 1.27086961 epoch total loss 1.30218148\n",
      "Trained batch 590 batch loss 1.39576387 epoch total loss 1.30234\n",
      "Trained batch 591 batch loss 1.4588306 epoch total loss 1.30260479\n",
      "Trained batch 592 batch loss 1.27585971 epoch total loss 1.30255961\n",
      "Trained batch 593 batch loss 1.36068153 epoch total loss 1.3026576\n",
      "Trained batch 594 batch loss 1.40715635 epoch total loss 1.30283356\n",
      "Trained batch 595 batch loss 1.41858041 epoch total loss 1.30302811\n",
      "Trained batch 596 batch loss 1.34715462 epoch total loss 1.30310214\n",
      "Trained batch 597 batch loss 1.35503054 epoch total loss 1.30318916\n",
      "Trained batch 598 batch loss 1.16432858 epoch total loss 1.30295682\n",
      "Trained batch 599 batch loss 1.28838623 epoch total loss 1.30293262\n",
      "Trained batch 600 batch loss 1.31169283 epoch total loss 1.30294716\n",
      "Trained batch 601 batch loss 1.33261919 epoch total loss 1.30299664\n",
      "Trained batch 602 batch loss 1.3652885 epoch total loss 1.30310011\n",
      "Trained batch 603 batch loss 1.23552167 epoch total loss 1.30298805\n",
      "Trained batch 604 batch loss 1.24449039 epoch total loss 1.30289125\n",
      "Trained batch 605 batch loss 1.19272244 epoch total loss 1.3027091\n",
      "Trained batch 606 batch loss 1.23897791 epoch total loss 1.30260396\n",
      "Trained batch 607 batch loss 1.20936859 epoch total loss 1.3024503\n",
      "Trained batch 608 batch loss 1.23994255 epoch total loss 1.30234754\n",
      "Trained batch 609 batch loss 1.16951656 epoch total loss 1.30212939\n",
      "Trained batch 610 batch loss 1.24909735 epoch total loss 1.30204237\n",
      "Trained batch 611 batch loss 1.27268541 epoch total loss 1.30199432\n",
      "Trained batch 612 batch loss 1.28648484 epoch total loss 1.30196905\n",
      "Trained batch 613 batch loss 1.28773201 epoch total loss 1.30194581\n",
      "Trained batch 614 batch loss 1.24053371 epoch total loss 1.30184579\n",
      "Trained batch 615 batch loss 1.27180779 epoch total loss 1.30179691\n",
      "Trained batch 616 batch loss 1.51305771 epoch total loss 1.30213988\n",
      "Trained batch 617 batch loss 1.21409512 epoch total loss 1.30199718\n",
      "Trained batch 618 batch loss 1.37725067 epoch total loss 1.30211902\n",
      "Trained batch 619 batch loss 1.26307797 epoch total loss 1.30205584\n",
      "Trained batch 620 batch loss 1.26290917 epoch total loss 1.30199277\n",
      "Trained batch 621 batch loss 1.17944658 epoch total loss 1.30179548\n",
      "Trained batch 622 batch loss 1.27833664 epoch total loss 1.30175769\n",
      "Trained batch 623 batch loss 1.23622179 epoch total loss 1.30165255\n",
      "Trained batch 624 batch loss 1.42364049 epoch total loss 1.30184805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 625 batch loss 1.42609239 epoch total loss 1.30204678\n",
      "Trained batch 626 batch loss 1.37779975 epoch total loss 1.30216777\n",
      "Trained batch 627 batch loss 1.17208552 epoch total loss 1.30196035\n",
      "Trained batch 628 batch loss 1.24123 epoch total loss 1.30186355\n",
      "Trained batch 629 batch loss 1.32623339 epoch total loss 1.30190229\n",
      "Trained batch 630 batch loss 1.27563691 epoch total loss 1.30186057\n",
      "Trained batch 631 batch loss 1.34307885 epoch total loss 1.3019259\n",
      "Trained batch 632 batch loss 1.28451061 epoch total loss 1.30189836\n",
      "Trained batch 633 batch loss 1.35084343 epoch total loss 1.30197561\n",
      "Trained batch 634 batch loss 1.36531568 epoch total loss 1.30207551\n",
      "Trained batch 635 batch loss 1.26179266 epoch total loss 1.30201209\n",
      "Trained batch 636 batch loss 1.27102876 epoch total loss 1.30196333\n",
      "Trained batch 637 batch loss 1.246382 epoch total loss 1.30187619\n",
      "Trained batch 638 batch loss 1.19001985 epoch total loss 1.30170083\n",
      "Trained batch 639 batch loss 1.16281939 epoch total loss 1.30148351\n",
      "Trained batch 640 batch loss 1.17369676 epoch total loss 1.30128384\n",
      "Trained batch 641 batch loss 1.19883692 epoch total loss 1.3011241\n",
      "Trained batch 642 batch loss 1.17970216 epoch total loss 1.30093491\n",
      "Trained batch 643 batch loss 1.25607014 epoch total loss 1.30086505\n",
      "Trained batch 644 batch loss 1.22925758 epoch total loss 1.30075383\n",
      "Trained batch 645 batch loss 1.15303326 epoch total loss 1.30052483\n",
      "Trained batch 646 batch loss 1.21560681 epoch total loss 1.30039346\n",
      "Trained batch 647 batch loss 1.29766178 epoch total loss 1.30038917\n",
      "Trained batch 648 batch loss 1.3429215 epoch total loss 1.30045474\n",
      "Trained batch 649 batch loss 1.31523752 epoch total loss 1.30047762\n",
      "Trained batch 650 batch loss 1.36483586 epoch total loss 1.30057657\n",
      "Trained batch 651 batch loss 1.27334189 epoch total loss 1.30053473\n",
      "Trained batch 652 batch loss 1.20296478 epoch total loss 1.300385\n",
      "Trained batch 653 batch loss 1.12131155 epoch total loss 1.30011082\n",
      "Trained batch 654 batch loss 1.10484111 epoch total loss 1.2998122\n",
      "Trained batch 655 batch loss 1.20288765 epoch total loss 1.29966426\n",
      "Trained batch 656 batch loss 1.20640087 epoch total loss 1.29952216\n",
      "Trained batch 657 batch loss 1.3151052 epoch total loss 1.29954588\n",
      "Trained batch 658 batch loss 1.25072265 epoch total loss 1.29947174\n",
      "Trained batch 659 batch loss 1.38701 epoch total loss 1.29960454\n",
      "Trained batch 660 batch loss 1.24078774 epoch total loss 1.29951537\n",
      "Trained batch 661 batch loss 1.29427278 epoch total loss 1.2995075\n",
      "Trained batch 662 batch loss 1.26124322 epoch total loss 1.29944968\n",
      "Trained batch 663 batch loss 1.28076351 epoch total loss 1.29942143\n",
      "Trained batch 664 batch loss 1.24607599 epoch total loss 1.29934108\n",
      "Trained batch 665 batch loss 1.18576193 epoch total loss 1.29917037\n",
      "Trained batch 666 batch loss 1.08797216 epoch total loss 1.29885328\n",
      "Trained batch 667 batch loss 0.996326268 epoch total loss 1.29839969\n",
      "Trained batch 668 batch loss 1.14322805 epoch total loss 1.29816747\n",
      "Trained batch 669 batch loss 1.30768311 epoch total loss 1.29818165\n",
      "Trained batch 670 batch loss 1.50802553 epoch total loss 1.29849482\n",
      "Trained batch 671 batch loss 1.46785903 epoch total loss 1.29874718\n",
      "Trained batch 672 batch loss 1.4248749 epoch total loss 1.29893482\n",
      "Trained batch 673 batch loss 1.38797235 epoch total loss 1.29906714\n",
      "Trained batch 674 batch loss 1.28624034 epoch total loss 1.29904819\n",
      "Trained batch 675 batch loss 1.36886358 epoch total loss 1.29915154\n",
      "Trained batch 676 batch loss 1.22898066 epoch total loss 1.29904783\n",
      "Trained batch 677 batch loss 1.2612114 epoch total loss 1.29899192\n",
      "Trained batch 678 batch loss 1.32749891 epoch total loss 1.299034\n",
      "Trained batch 679 batch loss 1.32171476 epoch total loss 1.29906738\n",
      "Trained batch 680 batch loss 1.28086591 epoch total loss 1.29904068\n",
      "Trained batch 681 batch loss 1.41336262 epoch total loss 1.29920864\n",
      "Trained batch 682 batch loss 1.35400462 epoch total loss 1.29928899\n",
      "Trained batch 683 batch loss 1.3969897 epoch total loss 1.29943192\n",
      "Trained batch 684 batch loss 1.3927238 epoch total loss 1.2995683\n",
      "Trained batch 685 batch loss 1.35875428 epoch total loss 1.29965472\n",
      "Trained batch 686 batch loss 1.22997451 epoch total loss 1.29955316\n",
      "Trained batch 687 batch loss 1.23957396 epoch total loss 1.29946589\n",
      "Trained batch 688 batch loss 1.3065412 epoch total loss 1.29947615\n",
      "Trained batch 689 batch loss 1.27957654 epoch total loss 1.2994473\n",
      "Trained batch 690 batch loss 1.33854675 epoch total loss 1.29950392\n",
      "Trained batch 691 batch loss 1.30628824 epoch total loss 1.2995137\n",
      "Trained batch 692 batch loss 1.35212195 epoch total loss 1.29958975\n",
      "Trained batch 693 batch loss 1.24767923 epoch total loss 1.29951489\n",
      "Trained batch 694 batch loss 1.49706471 epoch total loss 1.29979944\n",
      "Trained batch 695 batch loss 1.45999241 epoch total loss 1.30003\n",
      "Trained batch 696 batch loss 1.44250691 epoch total loss 1.30023479\n",
      "Trained batch 697 batch loss 1.35134554 epoch total loss 1.30030799\n",
      "Trained batch 698 batch loss 1.38046288 epoch total loss 1.30042291\n",
      "Trained batch 699 batch loss 1.42026293 epoch total loss 1.30059445\n",
      "Trained batch 700 batch loss 1.27676058 epoch total loss 1.30056036\n",
      "Trained batch 701 batch loss 1.29813111 epoch total loss 1.3005569\n",
      "Trained batch 702 batch loss 1.23091233 epoch total loss 1.3004576\n",
      "Trained batch 703 batch loss 1.30175054 epoch total loss 1.3004595\n",
      "Trained batch 704 batch loss 1.26775205 epoch total loss 1.30041301\n",
      "Trained batch 705 batch loss 1.29196143 epoch total loss 1.30040097\n",
      "Trained batch 706 batch loss 1.39604616 epoch total loss 1.30053651\n",
      "Trained batch 707 batch loss 1.37826037 epoch total loss 1.30064642\n",
      "Trained batch 708 batch loss 1.28874826 epoch total loss 1.30062962\n",
      "Trained batch 709 batch loss 1.3678894 epoch total loss 1.30072439\n",
      "Trained batch 710 batch loss 1.28824854 epoch total loss 1.30070686\n",
      "Trained batch 711 batch loss 1.20382452 epoch total loss 1.30057061\n",
      "Trained batch 712 batch loss 1.23771143 epoch total loss 1.30048227\n",
      "Trained batch 713 batch loss 1.2767309 epoch total loss 1.30044901\n",
      "Trained batch 714 batch loss 1.29290247 epoch total loss 1.3004384\n",
      "Trained batch 715 batch loss 1.20121288 epoch total loss 1.30029976\n",
      "Trained batch 716 batch loss 1.22708249 epoch total loss 1.30019748\n",
      "Trained batch 717 batch loss 1.23276556 epoch total loss 1.30010343\n",
      "Trained batch 718 batch loss 1.45160532 epoch total loss 1.30031443\n",
      "Trained batch 719 batch loss 1.333529 epoch total loss 1.30036068\n",
      "Trained batch 720 batch loss 1.48648739 epoch total loss 1.30061924\n",
      "Trained batch 721 batch loss 1.35907173 epoch total loss 1.30070031\n",
      "Trained batch 722 batch loss 1.26129842 epoch total loss 1.30064571\n",
      "Trained batch 723 batch loss 1.30449092 epoch total loss 1.30065107\n",
      "Trained batch 724 batch loss 1.31707573 epoch total loss 1.30067372\n",
      "Trained batch 725 batch loss 1.29656518 epoch total loss 1.30066812\n",
      "Trained batch 726 batch loss 1.27519393 epoch total loss 1.30063307\n",
      "Trained batch 727 batch loss 1.25677896 epoch total loss 1.30057275\n",
      "Trained batch 728 batch loss 1.29260862 epoch total loss 1.30056179\n",
      "Trained batch 729 batch loss 1.2329458 epoch total loss 1.30046904\n",
      "Trained batch 730 batch loss 1.19109082 epoch total loss 1.30031919\n",
      "Trained batch 731 batch loss 1.1837008 epoch total loss 1.30015969\n",
      "Trained batch 732 batch loss 1.07777286 epoch total loss 1.29985583\n",
      "Trained batch 733 batch loss 1.17911482 epoch total loss 1.2996912\n",
      "Trained batch 734 batch loss 1.38881373 epoch total loss 1.29981256\n",
      "Trained batch 735 batch loss 1.33555758 epoch total loss 1.29986119\n",
      "Trained batch 736 batch loss 1.50970316 epoch total loss 1.30014634\n",
      "Trained batch 737 batch loss 1.37091184 epoch total loss 1.30024242\n",
      "Trained batch 738 batch loss 1.37245846 epoch total loss 1.30034018\n",
      "Trained batch 739 batch loss 1.36182785 epoch total loss 1.30042338\n",
      "Trained batch 740 batch loss 1.3389653 epoch total loss 1.30047548\n",
      "Trained batch 741 batch loss 1.27066076 epoch total loss 1.3004353\n",
      "Trained batch 742 batch loss 1.43452251 epoch total loss 1.30061603\n",
      "Trained batch 743 batch loss 1.45093691 epoch total loss 1.30081832\n",
      "Trained batch 744 batch loss 1.40457416 epoch total loss 1.3009578\n",
      "Trained batch 745 batch loss 1.35264349 epoch total loss 1.30102718\n",
      "Trained batch 746 batch loss 1.1634419 epoch total loss 1.30084276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 747 batch loss 1.14272976 epoch total loss 1.30063105\n",
      "Trained batch 748 batch loss 1.11779714 epoch total loss 1.30038667\n",
      "Trained batch 749 batch loss 1.30909753 epoch total loss 1.30039823\n",
      "Trained batch 750 batch loss 1.06528676 epoch total loss 1.30008483\n",
      "Trained batch 751 batch loss 1.07368279 epoch total loss 1.29978335\n",
      "Trained batch 752 batch loss 1.0466547 epoch total loss 1.2994467\n",
      "Trained batch 753 batch loss 1.11502886 epoch total loss 1.29920185\n",
      "Trained batch 754 batch loss 1.15497017 epoch total loss 1.29901052\n",
      "Trained batch 755 batch loss 1.27527881 epoch total loss 1.29897904\n",
      "Trained batch 756 batch loss 1.34715247 epoch total loss 1.29904282\n",
      "Trained batch 757 batch loss 1.32452464 epoch total loss 1.29907644\n",
      "Trained batch 758 batch loss 1.34438229 epoch total loss 1.29913616\n",
      "Trained batch 759 batch loss 1.32496595 epoch total loss 1.29917026\n",
      "Trained batch 760 batch loss 1.30338514 epoch total loss 1.29917574\n",
      "Trained batch 761 batch loss 1.31933582 epoch total loss 1.29920232\n",
      "Trained batch 762 batch loss 1.24349058 epoch total loss 1.29912913\n",
      "Trained batch 763 batch loss 1.1620506 epoch total loss 1.29894948\n",
      "Trained batch 764 batch loss 1.21025634 epoch total loss 1.29883337\n",
      "Trained batch 765 batch loss 1.21664762 epoch total loss 1.29872596\n",
      "Trained batch 766 batch loss 1.23896742 epoch total loss 1.298648\n",
      "Trained batch 767 batch loss 1.36922395 epoch total loss 1.29873991\n",
      "Trained batch 768 batch loss 1.34370446 epoch total loss 1.29879844\n",
      "Trained batch 769 batch loss 1.19110739 epoch total loss 1.29865849\n",
      "Trained batch 770 batch loss 1.27328193 epoch total loss 1.29862547\n",
      "Trained batch 771 batch loss 1.30556488 epoch total loss 1.29863441\n",
      "Trained batch 772 batch loss 1.33409834 epoch total loss 1.29868031\n",
      "Trained batch 773 batch loss 1.28706551 epoch total loss 1.29866529\n",
      "Trained batch 774 batch loss 1.29631519 epoch total loss 1.2986623\n",
      "Trained batch 775 batch loss 1.28351092 epoch total loss 1.29864275\n",
      "Trained batch 776 batch loss 1.2251935 epoch total loss 1.2985481\n",
      "Trained batch 777 batch loss 1.15242493 epoch total loss 1.29836\n",
      "Trained batch 778 batch loss 1.29021955 epoch total loss 1.29834962\n",
      "Trained batch 779 batch loss 1.29539251 epoch total loss 1.2983458\n",
      "Trained batch 780 batch loss 1.28361917 epoch total loss 1.29832697\n",
      "Trained batch 781 batch loss 1.1676439 epoch total loss 1.2981596\n",
      "Trained batch 782 batch loss 1.19233668 epoch total loss 1.2980243\n",
      "Trained batch 783 batch loss 1.26111412 epoch total loss 1.29797709\n",
      "Trained batch 784 batch loss 1.35747266 epoch total loss 1.29805303\n",
      "Trained batch 785 batch loss 1.31024146 epoch total loss 1.29806852\n",
      "Trained batch 786 batch loss 1.25259733 epoch total loss 1.29801071\n",
      "Trained batch 787 batch loss 1.24185777 epoch total loss 1.29793942\n",
      "Trained batch 788 batch loss 1.16732514 epoch total loss 1.2977736\n",
      "Trained batch 789 batch loss 1.29929781 epoch total loss 1.29777563\n",
      "Trained batch 790 batch loss 1.33395815 epoch total loss 1.2978214\n",
      "Trained batch 791 batch loss 1.22268617 epoch total loss 1.29772639\n",
      "Trained batch 792 batch loss 1.19547558 epoch total loss 1.29759729\n",
      "Trained batch 793 batch loss 1.13165617 epoch total loss 1.29738808\n",
      "Trained batch 794 batch loss 1.36864638 epoch total loss 1.29747784\n",
      "Trained batch 795 batch loss 1.26772916 epoch total loss 1.29744041\n",
      "Trained batch 796 batch loss 1.24959862 epoch total loss 1.29738033\n",
      "Trained batch 797 batch loss 1.32751346 epoch total loss 1.29741812\n",
      "Trained batch 798 batch loss 1.12631166 epoch total loss 1.29720378\n",
      "Trained batch 799 batch loss 1.10636318 epoch total loss 1.29696488\n",
      "Trained batch 800 batch loss 1.08412528 epoch total loss 1.29669881\n",
      "Trained batch 801 batch loss 1.04565775 epoch total loss 1.29638529\n",
      "Trained batch 802 batch loss 1.17377102 epoch total loss 1.29623258\n",
      "Trained batch 803 batch loss 1.27791739 epoch total loss 1.29620981\n",
      "Trained batch 804 batch loss 1.16551876 epoch total loss 1.29604721\n",
      "Trained batch 805 batch loss 1.31384349 epoch total loss 1.29606938\n",
      "Trained batch 806 batch loss 1.13554406 epoch total loss 1.29587007\n",
      "Trained batch 807 batch loss 1.17956614 epoch total loss 1.29572594\n",
      "Trained batch 808 batch loss 1.36405742 epoch total loss 1.29581046\n",
      "Trained batch 809 batch loss 1.15709615 epoch total loss 1.29563904\n",
      "Trained batch 810 batch loss 1.11263537 epoch total loss 1.29541314\n",
      "Trained batch 811 batch loss 1.30326116 epoch total loss 1.29542279\n",
      "Trained batch 812 batch loss 1.16417551 epoch total loss 1.29526114\n",
      "Trained batch 813 batch loss 1.16858172 epoch total loss 1.29510534\n",
      "Trained batch 814 batch loss 1.0985496 epoch total loss 1.29486382\n",
      "Trained batch 815 batch loss 1.17087626 epoch total loss 1.29471171\n",
      "Trained batch 816 batch loss 1.18233848 epoch total loss 1.29457402\n",
      "Trained batch 817 batch loss 1.25596666 epoch total loss 1.29452682\n",
      "Trained batch 818 batch loss 1.21168292 epoch total loss 1.29442549\n",
      "Trained batch 819 batch loss 1.28143334 epoch total loss 1.29440975\n",
      "Trained batch 820 batch loss 1.356179 epoch total loss 1.29448509\n",
      "Trained batch 821 batch loss 1.33447456 epoch total loss 1.29453385\n",
      "Trained batch 822 batch loss 1.38726974 epoch total loss 1.29464674\n",
      "Trained batch 823 batch loss 1.3047893 epoch total loss 1.29465902\n",
      "Trained batch 824 batch loss 1.44296384 epoch total loss 1.29483902\n",
      "Trained batch 825 batch loss 1.28252292 epoch total loss 1.29482412\n",
      "Trained batch 826 batch loss 1.29709053 epoch total loss 1.29482687\n",
      "Trained batch 827 batch loss 1.25061178 epoch total loss 1.29477334\n",
      "Trained batch 828 batch loss 1.32281733 epoch total loss 1.29480731\n",
      "Trained batch 829 batch loss 1.1498158 epoch total loss 1.29463243\n",
      "Trained batch 830 batch loss 1.13444161 epoch total loss 1.29443932\n",
      "Trained batch 831 batch loss 1.17894936 epoch total loss 1.29430032\n",
      "Trained batch 832 batch loss 1.16786683 epoch total loss 1.29414833\n",
      "Trained batch 833 batch loss 1.2123543 epoch total loss 1.29405022\n",
      "Trained batch 834 batch loss 1.14625287 epoch total loss 1.29387295\n",
      "Trained batch 835 batch loss 1.16151738 epoch total loss 1.29371452\n",
      "Trained batch 836 batch loss 1.18335581 epoch total loss 1.29358244\n",
      "Trained batch 837 batch loss 1.12796617 epoch total loss 1.29338455\n",
      "Trained batch 838 batch loss 1.47854209 epoch total loss 1.29360545\n",
      "Trained batch 839 batch loss 1.24060285 epoch total loss 1.29354227\n",
      "Trained batch 840 batch loss 1.28577316 epoch total loss 1.29353309\n",
      "Trained batch 841 batch loss 1.27005 epoch total loss 1.29350507\n",
      "Trained batch 842 batch loss 1.31244969 epoch total loss 1.2935276\n",
      "Trained batch 843 batch loss 1.20250106 epoch total loss 1.29341972\n",
      "Trained batch 844 batch loss 1.28756475 epoch total loss 1.2934128\n",
      "Trained batch 845 batch loss 1.38119936 epoch total loss 1.29351664\n",
      "Trained batch 846 batch loss 1.35704637 epoch total loss 1.29359186\n",
      "Trained batch 847 batch loss 1.39787912 epoch total loss 1.29371488\n",
      "Trained batch 848 batch loss 1.25357509 epoch total loss 1.29366744\n",
      "Trained batch 849 batch loss 1.24945188 epoch total loss 1.29361546\n",
      "Trained batch 850 batch loss 1.27957237 epoch total loss 1.29359889\n",
      "Trained batch 851 batch loss 1.33606696 epoch total loss 1.29364884\n",
      "Trained batch 852 batch loss 1.31602788 epoch total loss 1.29367507\n",
      "Trained batch 853 batch loss 1.25887179 epoch total loss 1.2936343\n",
      "Trained batch 854 batch loss 1.2781769 epoch total loss 1.29361629\n",
      "Trained batch 855 batch loss 1.25272179 epoch total loss 1.29356837\n",
      "Trained batch 856 batch loss 1.23799109 epoch total loss 1.29350352\n",
      "Trained batch 857 batch loss 1.24154437 epoch total loss 1.29344296\n",
      "Trained batch 858 batch loss 1.20724463 epoch total loss 1.29334247\n",
      "Trained batch 859 batch loss 1.32114542 epoch total loss 1.2933749\n",
      "Trained batch 860 batch loss 1.58689177 epoch total loss 1.29371619\n",
      "Trained batch 861 batch loss 1.48541248 epoch total loss 1.29393876\n",
      "Trained batch 862 batch loss 1.35138893 epoch total loss 1.29400551\n",
      "Trained batch 863 batch loss 1.40732682 epoch total loss 1.29413688\n",
      "Trained batch 864 batch loss 1.20774627 epoch total loss 1.29403687\n",
      "Trained batch 865 batch loss 1.05572879 epoch total loss 1.29376137\n",
      "Trained batch 866 batch loss 1.01883185 epoch total loss 1.29344392\n",
      "Trained batch 867 batch loss 1.11777663 epoch total loss 1.29324138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 868 batch loss 1.15326059 epoch total loss 1.29308009\n",
      "Trained batch 869 batch loss 1.28522396 epoch total loss 1.29307115\n",
      "Trained batch 870 batch loss 1.27322352 epoch total loss 1.29304826\n",
      "Trained batch 871 batch loss 1.39336681 epoch total loss 1.29316342\n",
      "Trained batch 872 batch loss 1.2733357 epoch total loss 1.29314065\n",
      "Trained batch 873 batch loss 1.39227855 epoch total loss 1.29325426\n",
      "Trained batch 874 batch loss 1.32637823 epoch total loss 1.29329216\n",
      "Trained batch 875 batch loss 1.28207839 epoch total loss 1.29327941\n",
      "Trained batch 876 batch loss 1.32949448 epoch total loss 1.29332078\n",
      "Trained batch 877 batch loss 1.30046439 epoch total loss 1.29332888\n",
      "Trained batch 878 batch loss 1.20440793 epoch total loss 1.29322767\n",
      "Trained batch 879 batch loss 1.373806 epoch total loss 1.29331923\n",
      "Trained batch 880 batch loss 1.4197557 epoch total loss 1.29346299\n",
      "Trained batch 881 batch loss 1.31600106 epoch total loss 1.29348862\n",
      "Trained batch 882 batch loss 1.24399173 epoch total loss 1.29343259\n",
      "Trained batch 883 batch loss 1.22141898 epoch total loss 1.29335105\n",
      "Trained batch 884 batch loss 1.14542127 epoch total loss 1.29318357\n",
      "Trained batch 885 batch loss 1.21886301 epoch total loss 1.29309964\n",
      "Trained batch 886 batch loss 1.21374965 epoch total loss 1.29301012\n",
      "Trained batch 887 batch loss 1.28727746 epoch total loss 1.29300356\n",
      "Trained batch 888 batch loss 1.23662972 epoch total loss 1.29294\n",
      "Trained batch 889 batch loss 1.38748908 epoch total loss 1.29304636\n",
      "Trained batch 890 batch loss 1.29366827 epoch total loss 1.29304707\n",
      "Trained batch 891 batch loss 1.34809637 epoch total loss 1.29310894\n",
      "Trained batch 892 batch loss 1.28128672 epoch total loss 1.29309559\n",
      "Trained batch 893 batch loss 1.17689598 epoch total loss 1.29296541\n",
      "Trained batch 894 batch loss 1.30742526 epoch total loss 1.29298162\n",
      "Trained batch 895 batch loss 1.31023932 epoch total loss 1.29300082\n",
      "Trained batch 896 batch loss 1.43928576 epoch total loss 1.29316413\n",
      "Trained batch 897 batch loss 1.23001337 epoch total loss 1.29309368\n",
      "Trained batch 898 batch loss 1.32820046 epoch total loss 1.29313278\n",
      "Trained batch 899 batch loss 1.25518537 epoch total loss 1.29309058\n",
      "Trained batch 900 batch loss 1.30915141 epoch total loss 1.29310846\n",
      "Trained batch 901 batch loss 1.28123462 epoch total loss 1.29309535\n",
      "Trained batch 902 batch loss 1.29139757 epoch total loss 1.29309344\n",
      "Trained batch 903 batch loss 1.24181128 epoch total loss 1.29303658\n",
      "Trained batch 904 batch loss 1.38162756 epoch total loss 1.29313457\n",
      "Trained batch 905 batch loss 1.35443532 epoch total loss 1.2932024\n",
      "Trained batch 906 batch loss 1.40619862 epoch total loss 1.29332709\n",
      "Trained batch 907 batch loss 1.31294549 epoch total loss 1.29334879\n",
      "Trained batch 908 batch loss 1.32909644 epoch total loss 1.29338825\n",
      "Trained batch 909 batch loss 1.26341569 epoch total loss 1.29335523\n",
      "Trained batch 910 batch loss 1.33476782 epoch total loss 1.29340065\n",
      "Trained batch 911 batch loss 1.40736389 epoch total loss 1.29352582\n",
      "Trained batch 912 batch loss 1.51028287 epoch total loss 1.2937634\n",
      "Trained batch 913 batch loss 1.45323157 epoch total loss 1.29393804\n",
      "Trained batch 914 batch loss 1.31901169 epoch total loss 1.29396546\n",
      "Trained batch 915 batch loss 1.26552057 epoch total loss 1.29393435\n",
      "Trained batch 916 batch loss 1.2128191 epoch total loss 1.29384577\n",
      "Trained batch 917 batch loss 1.23805249 epoch total loss 1.29378486\n",
      "Trained batch 918 batch loss 1.34149075 epoch total loss 1.29383683\n",
      "Trained batch 919 batch loss 1.32246292 epoch total loss 1.29386806\n",
      "Trained batch 920 batch loss 1.27680945 epoch total loss 1.29384947\n",
      "Trained batch 921 batch loss 1.40340877 epoch total loss 1.29396856\n",
      "Trained batch 922 batch loss 1.40919697 epoch total loss 1.29409349\n",
      "Trained batch 923 batch loss 1.46830702 epoch total loss 1.2942822\n",
      "Trained batch 924 batch loss 1.28476262 epoch total loss 1.29427195\n",
      "Trained batch 925 batch loss 1.28722572 epoch total loss 1.29426432\n",
      "Trained batch 926 batch loss 1.1959095 epoch total loss 1.2941581\n",
      "Trained batch 927 batch loss 1.27842128 epoch total loss 1.29414117\n",
      "Trained batch 928 batch loss 1.29147887 epoch total loss 1.29413831\n",
      "Trained batch 929 batch loss 1.1813693 epoch total loss 1.29401696\n",
      "Trained batch 930 batch loss 1.24287617 epoch total loss 1.293962\n",
      "Trained batch 931 batch loss 1.19904912 epoch total loss 1.29386008\n",
      "Trained batch 932 batch loss 1.22923589 epoch total loss 1.29379082\n",
      "Trained batch 933 batch loss 1.3616966 epoch total loss 1.29386353\n",
      "Trained batch 934 batch loss 1.38492024 epoch total loss 1.29396105\n",
      "Trained batch 935 batch loss 1.34925878 epoch total loss 1.29402018\n",
      "Trained batch 936 batch loss 1.1826942 epoch total loss 1.29390121\n",
      "Trained batch 937 batch loss 1.23823988 epoch total loss 1.29384184\n",
      "Trained batch 938 batch loss 1.25462806 epoch total loss 1.29380012\n",
      "Trained batch 939 batch loss 1.3004986 epoch total loss 1.29380727\n",
      "Trained batch 940 batch loss 1.20365059 epoch total loss 1.2937113\n",
      "Trained batch 941 batch loss 1.0417738 epoch total loss 1.29344356\n",
      "Trained batch 942 batch loss 1.12448931 epoch total loss 1.29326415\n",
      "Trained batch 943 batch loss 1.13458037 epoch total loss 1.29309583\n",
      "Trained batch 944 batch loss 1.15386212 epoch total loss 1.29294837\n",
      "Trained batch 945 batch loss 1.16833067 epoch total loss 1.2928164\n",
      "Trained batch 946 batch loss 1.16773963 epoch total loss 1.2926842\n",
      "Trained batch 947 batch loss 1.20274329 epoch total loss 1.29258931\n",
      "Trained batch 948 batch loss 1.29419601 epoch total loss 1.29259098\n",
      "Trained batch 949 batch loss 1.19808269 epoch total loss 1.29249144\n",
      "Trained batch 950 batch loss 1.11023319 epoch total loss 1.29229951\n",
      "Trained batch 951 batch loss 1.2612201 epoch total loss 1.29226685\n",
      "Trained batch 952 batch loss 1.37423253 epoch total loss 1.29235303\n",
      "Trained batch 953 batch loss 1.31586719 epoch total loss 1.29237771\n",
      "Trained batch 954 batch loss 1.3036654 epoch total loss 1.29238963\n",
      "Trained batch 955 batch loss 1.23812687 epoch total loss 1.29233289\n",
      "Trained batch 956 batch loss 1.3967936 epoch total loss 1.2924422\n",
      "Trained batch 957 batch loss 1.20167482 epoch total loss 1.29234731\n",
      "Trained batch 958 batch loss 1.24596059 epoch total loss 1.29229891\n",
      "Trained batch 959 batch loss 1.13581717 epoch total loss 1.29213572\n",
      "Trained batch 960 batch loss 1.11484933 epoch total loss 1.29195106\n",
      "Trained batch 961 batch loss 1.21738458 epoch total loss 1.29187357\n",
      "Trained batch 962 batch loss 1.25299883 epoch total loss 1.29183316\n",
      "Trained batch 963 batch loss 1.20225 epoch total loss 1.29174018\n",
      "Trained batch 964 batch loss 1.22144389 epoch total loss 1.29166722\n",
      "Trained batch 965 batch loss 1.19152355 epoch total loss 1.29156351\n",
      "Trained batch 966 batch loss 1.09427249 epoch total loss 1.29135919\n",
      "Trained batch 967 batch loss 1.22947288 epoch total loss 1.29129529\n",
      "Trained batch 968 batch loss 1.35388041 epoch total loss 1.2913599\n",
      "Trained batch 969 batch loss 1.25026321 epoch total loss 1.29131746\n",
      "Trained batch 970 batch loss 1.40633488 epoch total loss 1.29143608\n",
      "Trained batch 971 batch loss 1.3775785 epoch total loss 1.29152477\n",
      "Trained batch 972 batch loss 1.2883693 epoch total loss 1.29152143\n",
      "Trained batch 973 batch loss 1.26952243 epoch total loss 1.2914989\n",
      "Trained batch 974 batch loss 1.26938593 epoch total loss 1.29147625\n",
      "Trained batch 975 batch loss 1.16380072 epoch total loss 1.29134524\n",
      "Trained batch 976 batch loss 1.23512566 epoch total loss 1.29128766\n",
      "Trained batch 977 batch loss 1.34968197 epoch total loss 1.2913475\n",
      "Trained batch 978 batch loss 1.28112364 epoch total loss 1.29133701\n",
      "Trained batch 979 batch loss 1.25913799 epoch total loss 1.29130411\n",
      "Trained batch 980 batch loss 1.32296777 epoch total loss 1.29133654\n",
      "Trained batch 981 batch loss 1.33351111 epoch total loss 1.29137945\n",
      "Trained batch 982 batch loss 1.22024632 epoch total loss 1.29130697\n",
      "Trained batch 983 batch loss 1.2932744 epoch total loss 1.29130912\n",
      "Trained batch 984 batch loss 1.20527124 epoch total loss 1.29122174\n",
      "Trained batch 985 batch loss 1.17749238 epoch total loss 1.29110622\n",
      "Trained batch 986 batch loss 1.19354892 epoch total loss 1.29100728\n",
      "Trained batch 987 batch loss 1.35908413 epoch total loss 1.2910763\n",
      "Trained batch 988 batch loss 1.33264303 epoch total loss 1.29111838\n",
      "Trained batch 989 batch loss 1.39126325 epoch total loss 1.29121959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 990 batch loss 1.27019846 epoch total loss 1.29119837\n",
      "Trained batch 991 batch loss 1.2838769 epoch total loss 1.29119098\n",
      "Trained batch 992 batch loss 1.35495424 epoch total loss 1.29125535\n",
      "Trained batch 993 batch loss 1.24332786 epoch total loss 1.29120708\n",
      "Trained batch 994 batch loss 1.26650298 epoch total loss 1.29118216\n",
      "Trained batch 995 batch loss 1.26637506 epoch total loss 1.29115725\n",
      "Trained batch 996 batch loss 1.25304556 epoch total loss 1.29111898\n",
      "Trained batch 997 batch loss 1.4158833 epoch total loss 1.29124415\n",
      "Trained batch 998 batch loss 1.16224575 epoch total loss 1.29111481\n",
      "Trained batch 999 batch loss 1.41052723 epoch total loss 1.29123437\n",
      "Trained batch 1000 batch loss 1.31498182 epoch total loss 1.2912581\n",
      "Trained batch 1001 batch loss 1.23993289 epoch total loss 1.29120684\n",
      "Trained batch 1002 batch loss 1.34798241 epoch total loss 1.29126358\n",
      "Trained batch 1003 batch loss 1.17271769 epoch total loss 1.29114532\n",
      "Trained batch 1004 batch loss 1.09677649 epoch total loss 1.29095185\n",
      "Trained batch 1005 batch loss 1.21199572 epoch total loss 1.29087329\n",
      "Trained batch 1006 batch loss 1.37043083 epoch total loss 1.29095244\n",
      "Trained batch 1007 batch loss 1.33484554 epoch total loss 1.29099596\n",
      "Trained batch 1008 batch loss 1.43663335 epoch total loss 1.29114044\n",
      "Trained batch 1009 batch loss 1.4605794 epoch total loss 1.2913084\n",
      "Trained batch 1010 batch loss 1.33582437 epoch total loss 1.29135251\n",
      "Trained batch 1011 batch loss 1.27515554 epoch total loss 1.29133642\n",
      "Trained batch 1012 batch loss 1.43485546 epoch total loss 1.29147816\n",
      "Trained batch 1013 batch loss 1.33866858 epoch total loss 1.29152477\n",
      "Trained batch 1014 batch loss 1.26245987 epoch total loss 1.29149604\n",
      "Trained batch 1015 batch loss 1.31159449 epoch total loss 1.29151595\n",
      "Trained batch 1016 batch loss 1.3712728 epoch total loss 1.29159439\n",
      "Trained batch 1017 batch loss 1.41898394 epoch total loss 1.29171956\n",
      "Trained batch 1018 batch loss 1.32890439 epoch total loss 1.29175603\n",
      "Trained batch 1019 batch loss 1.31361 epoch total loss 1.29177749\n",
      "Trained batch 1020 batch loss 1.36045182 epoch total loss 1.29184484\n",
      "Trained batch 1021 batch loss 1.38094771 epoch total loss 1.29193223\n",
      "Trained batch 1022 batch loss 1.34807873 epoch total loss 1.29198706\n",
      "Trained batch 1023 batch loss 1.16860056 epoch total loss 1.29186642\n",
      "Trained batch 1024 batch loss 1.18214774 epoch total loss 1.29175925\n",
      "Trained batch 1025 batch loss 1.24605763 epoch total loss 1.29171467\n",
      "Trained batch 1026 batch loss 1.29808235 epoch total loss 1.29172087\n",
      "Trained batch 1027 batch loss 1.23952699 epoch total loss 1.29167008\n",
      "Trained batch 1028 batch loss 1.30767989 epoch total loss 1.2916857\n",
      "Trained batch 1029 batch loss 1.32030487 epoch total loss 1.29171348\n",
      "Trained batch 1030 batch loss 1.25565398 epoch total loss 1.29167843\n",
      "Trained batch 1031 batch loss 1.2619791 epoch total loss 1.2916497\n",
      "Trained batch 1032 batch loss 1.17268395 epoch total loss 1.29153442\n",
      "Trained batch 1033 batch loss 1.30758393 epoch total loss 1.29155\n",
      "Trained batch 1034 batch loss 1.2508136 epoch total loss 1.29151058\n",
      "Trained batch 1035 batch loss 1.17731452 epoch total loss 1.29140031\n",
      "Trained batch 1036 batch loss 1.25917125 epoch total loss 1.2913692\n",
      "Trained batch 1037 batch loss 1.29874015 epoch total loss 1.29137635\n",
      "Trained batch 1038 batch loss 1.30321348 epoch total loss 1.29138768\n",
      "Trained batch 1039 batch loss 1.24542236 epoch total loss 1.29134345\n",
      "Trained batch 1040 batch loss 1.17904532 epoch total loss 1.29123545\n",
      "Trained batch 1041 batch loss 1.21644568 epoch total loss 1.29116356\n",
      "Trained batch 1042 batch loss 1.36120236 epoch total loss 1.2912308\n",
      "Trained batch 1043 batch loss 1.36495161 epoch total loss 1.29130149\n",
      "Trained batch 1044 batch loss 1.19814134 epoch total loss 1.29121232\n",
      "Trained batch 1045 batch loss 1.04932237 epoch total loss 1.29098082\n",
      "Trained batch 1046 batch loss 1.23973167 epoch total loss 1.29093182\n",
      "Trained batch 1047 batch loss 1.1730659 epoch total loss 1.29081929\n",
      "Trained batch 1048 batch loss 1.26162624 epoch total loss 1.29079139\n",
      "Trained batch 1049 batch loss 1.24706602 epoch total loss 1.29074967\n",
      "Trained batch 1050 batch loss 1.31980395 epoch total loss 1.29077744\n",
      "Trained batch 1051 batch loss 1.19522381 epoch total loss 1.29068649\n",
      "Trained batch 1052 batch loss 1.33326769 epoch total loss 1.2907269\n",
      "Trained batch 1053 batch loss 1.34323549 epoch total loss 1.29077685\n",
      "Trained batch 1054 batch loss 1.45277882 epoch total loss 1.29093051\n",
      "Trained batch 1055 batch loss 1.44637728 epoch total loss 1.29107785\n",
      "Trained batch 1056 batch loss 1.51668525 epoch total loss 1.29129159\n",
      "Trained batch 1057 batch loss 1.38980961 epoch total loss 1.2913847\n",
      "Trained batch 1058 batch loss 1.2370038 epoch total loss 1.29133332\n",
      "Trained batch 1059 batch loss 1.30904543 epoch total loss 1.29135013\n",
      "Trained batch 1060 batch loss 1.41967535 epoch total loss 1.29147124\n",
      "Trained batch 1061 batch loss 1.47470021 epoch total loss 1.29164398\n",
      "Trained batch 1062 batch loss 1.34093404 epoch total loss 1.29169035\n",
      "Trained batch 1063 batch loss 1.31326866 epoch total loss 1.29171062\n",
      "Trained batch 1064 batch loss 1.35308826 epoch total loss 1.29176819\n",
      "Trained batch 1065 batch loss 1.34848559 epoch total loss 1.29182148\n",
      "Trained batch 1066 batch loss 1.43150985 epoch total loss 1.29195261\n",
      "Trained batch 1067 batch loss 1.34866619 epoch total loss 1.29200566\n",
      "Trained batch 1068 batch loss 1.28924537 epoch total loss 1.29200304\n",
      "Trained batch 1069 batch loss 1.33704484 epoch total loss 1.29204512\n",
      "Trained batch 1070 batch loss 1.31649935 epoch total loss 1.292068\n",
      "Trained batch 1071 batch loss 1.31992769 epoch total loss 1.29209411\n",
      "Trained batch 1072 batch loss 1.30713749 epoch total loss 1.29210806\n",
      "Trained batch 1073 batch loss 1.40013933 epoch total loss 1.29220879\n",
      "Trained batch 1074 batch loss 1.41625965 epoch total loss 1.2923243\n",
      "Trained batch 1075 batch loss 1.37006736 epoch total loss 1.29239666\n",
      "Trained batch 1076 batch loss 1.29134476 epoch total loss 1.29239571\n",
      "Trained batch 1077 batch loss 1.40610349 epoch total loss 1.29250133\n",
      "Trained batch 1078 batch loss 1.21302712 epoch total loss 1.29242754\n",
      "Trained batch 1079 batch loss 1.42877352 epoch total loss 1.29255402\n",
      "Trained batch 1080 batch loss 1.40269935 epoch total loss 1.29265594\n",
      "Trained batch 1081 batch loss 1.41171575 epoch total loss 1.29276621\n",
      "Trained batch 1082 batch loss 1.2497232 epoch total loss 1.2927264\n",
      "Trained batch 1083 batch loss 1.2184999 epoch total loss 1.29265785\n",
      "Trained batch 1084 batch loss 1.10833013 epoch total loss 1.29248774\n",
      "Trained batch 1085 batch loss 1.24900794 epoch total loss 1.29244769\n",
      "Trained batch 1086 batch loss 1.25543761 epoch total loss 1.29241371\n",
      "Trained batch 1087 batch loss 1.41410518 epoch total loss 1.29252565\n",
      "Trained batch 1088 batch loss 1.21500516 epoch total loss 1.29245436\n",
      "Trained batch 1089 batch loss 1.21318865 epoch total loss 1.29238153\n",
      "Trained batch 1090 batch loss 1.31030703 epoch total loss 1.29239798\n",
      "Trained batch 1091 batch loss 1.18565202 epoch total loss 1.29230011\n",
      "Trained batch 1092 batch loss 1.17002571 epoch total loss 1.29218817\n",
      "Trained batch 1093 batch loss 1.25688839 epoch total loss 1.29215586\n",
      "Trained batch 1094 batch loss 1.2783711 epoch total loss 1.29214311\n",
      "Trained batch 1095 batch loss 1.22220087 epoch total loss 1.29207921\n",
      "Trained batch 1096 batch loss 1.31228161 epoch total loss 1.29209769\n",
      "Trained batch 1097 batch loss 1.18431544 epoch total loss 1.29199946\n",
      "Trained batch 1098 batch loss 1.27822423 epoch total loss 1.29198682\n",
      "Trained batch 1099 batch loss 1.22140527 epoch total loss 1.29192269\n",
      "Trained batch 1100 batch loss 1.21689808 epoch total loss 1.2918545\n",
      "Trained batch 1101 batch loss 1.20341325 epoch total loss 1.29177415\n",
      "Trained batch 1102 batch loss 1.16584468 epoch total loss 1.29165983\n",
      "Trained batch 1103 batch loss 1.20010805 epoch total loss 1.29157686\n",
      "Trained batch 1104 batch loss 1.1707375 epoch total loss 1.29146743\n",
      "Trained batch 1105 batch loss 1.3808074 epoch total loss 1.29154837\n",
      "Trained batch 1106 batch loss 1.25611627 epoch total loss 1.2915163\n",
      "Trained batch 1107 batch loss 1.19111037 epoch total loss 1.29142559\n",
      "Trained batch 1108 batch loss 1.15558636 epoch total loss 1.29130304\n",
      "Trained batch 1109 batch loss 1.23454356 epoch total loss 1.2912519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1110 batch loss 1.27236819 epoch total loss 1.29123485\n",
      "Trained batch 1111 batch loss 1.37082601 epoch total loss 1.2913065\n",
      "Trained batch 1112 batch loss 1.30901444 epoch total loss 1.29132235\n",
      "Trained batch 1113 batch loss 1.29949212 epoch total loss 1.29132962\n",
      "Trained batch 1114 batch loss 1.29101598 epoch total loss 1.29132938\n",
      "Trained batch 1115 batch loss 1.29161966 epoch total loss 1.29132962\n",
      "Trained batch 1116 batch loss 1.34154606 epoch total loss 1.29137456\n",
      "Trained batch 1117 batch loss 1.31849122 epoch total loss 1.29139888\n",
      "Trained batch 1118 batch loss 1.22736812 epoch total loss 1.29134166\n",
      "Trained batch 1119 batch loss 1.20282948 epoch total loss 1.29126263\n",
      "Trained batch 1120 batch loss 1.28084636 epoch total loss 1.29125333\n",
      "Trained batch 1121 batch loss 1.19694567 epoch total loss 1.29116917\n",
      "Trained batch 1122 batch loss 1.16797805 epoch total loss 1.29105937\n",
      "Trained batch 1123 batch loss 1.27141726 epoch total loss 1.29104185\n",
      "Trained batch 1124 batch loss 1.20105445 epoch total loss 1.29096174\n",
      "Trained batch 1125 batch loss 1.19447207 epoch total loss 1.29087603\n",
      "Trained batch 1126 batch loss 1.17781591 epoch total loss 1.29077566\n",
      "Trained batch 1127 batch loss 1.27356839 epoch total loss 1.29076028\n",
      "Trained batch 1128 batch loss 1.22477508 epoch total loss 1.29070175\n",
      "Trained batch 1129 batch loss 1.17257023 epoch total loss 1.2905972\n",
      "Trained batch 1130 batch loss 1.47234011 epoch total loss 1.29075801\n",
      "Trained batch 1131 batch loss 1.55434132 epoch total loss 1.29099107\n",
      "Trained batch 1132 batch loss 1.44722199 epoch total loss 1.29112911\n",
      "Trained batch 1133 batch loss 1.40481377 epoch total loss 1.29122937\n",
      "Trained batch 1134 batch loss 1.22725654 epoch total loss 1.29117298\n",
      "Trained batch 1135 batch loss 1.32720399 epoch total loss 1.29120469\n",
      "Trained batch 1136 batch loss 1.19733167 epoch total loss 1.29112208\n",
      "Trained batch 1137 batch loss 1.26746178 epoch total loss 1.29110134\n",
      "Trained batch 1138 batch loss 1.33607543 epoch total loss 1.29114079\n",
      "Trained batch 1139 batch loss 1.1682713 epoch total loss 1.29103291\n",
      "Trained batch 1140 batch loss 1.12643087 epoch total loss 1.29088855\n",
      "Trained batch 1141 batch loss 1.13749325 epoch total loss 1.29075408\n",
      "Trained batch 1142 batch loss 1.13395715 epoch total loss 1.29061675\n",
      "Trained batch 1143 batch loss 1.24598145 epoch total loss 1.29057765\n",
      "Trained batch 1144 batch loss 1.54471481 epoch total loss 1.29079974\n",
      "Trained batch 1145 batch loss 1.53219926 epoch total loss 1.29101062\n",
      "Trained batch 1146 batch loss 1.51758933 epoch total loss 1.29120827\n",
      "Trained batch 1147 batch loss 1.36985445 epoch total loss 1.29127693\n",
      "Trained batch 1148 batch loss 1.41591287 epoch total loss 1.29138541\n",
      "Trained batch 1149 batch loss 1.36677468 epoch total loss 1.2914511\n",
      "Trained batch 1150 batch loss 1.38094306 epoch total loss 1.29152894\n",
      "Trained batch 1151 batch loss 1.35024416 epoch total loss 1.29158\n",
      "Trained batch 1152 batch loss 1.42016852 epoch total loss 1.29169154\n",
      "Trained batch 1153 batch loss 1.29474759 epoch total loss 1.29169428\n",
      "Trained batch 1154 batch loss 1.30908823 epoch total loss 1.2917093\n",
      "Trained batch 1155 batch loss 1.37097442 epoch total loss 1.29177797\n",
      "Trained batch 1156 batch loss 1.29811406 epoch total loss 1.29178345\n",
      "Trained batch 1157 batch loss 1.34084499 epoch total loss 1.29182577\n",
      "Trained batch 1158 batch loss 1.37527108 epoch total loss 1.29189789\n",
      "Trained batch 1159 batch loss 1.36199617 epoch total loss 1.29195833\n",
      "Trained batch 1160 batch loss 1.328246 epoch total loss 1.29198956\n",
      "Trained batch 1161 batch loss 1.37802041 epoch total loss 1.29206371\n",
      "Trained batch 1162 batch loss 1.2730732 epoch total loss 1.29204738\n",
      "Trained batch 1163 batch loss 1.39053571 epoch total loss 1.29213202\n",
      "Trained batch 1164 batch loss 1.28563905 epoch total loss 1.29212642\n",
      "Trained batch 1165 batch loss 1.30888927 epoch total loss 1.29214072\n",
      "Trained batch 1166 batch loss 1.3027333 epoch total loss 1.2921499\n",
      "Trained batch 1167 batch loss 1.35953867 epoch total loss 1.2922076\n",
      "Trained batch 1168 batch loss 1.31971955 epoch total loss 1.29223108\n",
      "Trained batch 1169 batch loss 1.30705261 epoch total loss 1.29224372\n",
      "Trained batch 1170 batch loss 1.26826477 epoch total loss 1.29222333\n",
      "Trained batch 1171 batch loss 1.26260078 epoch total loss 1.29219794\n",
      "Trained batch 1172 batch loss 1.37874269 epoch total loss 1.29227185\n",
      "Trained batch 1173 batch loss 1.23315907 epoch total loss 1.29222143\n",
      "Trained batch 1174 batch loss 1.33961904 epoch total loss 1.29226184\n",
      "Trained batch 1175 batch loss 1.2663331 epoch total loss 1.29223979\n",
      "Trained batch 1176 batch loss 1.30884361 epoch total loss 1.29225385\n",
      "Trained batch 1177 batch loss 1.184093 epoch total loss 1.29216194\n",
      "Trained batch 1178 batch loss 1.18752527 epoch total loss 1.29207313\n",
      "Trained batch 1179 batch loss 1.22669601 epoch total loss 1.2920177\n",
      "Trained batch 1180 batch loss 1.29947472 epoch total loss 1.2920239\n",
      "Trained batch 1181 batch loss 1.39617968 epoch total loss 1.29211223\n",
      "Trained batch 1182 batch loss 1.32995129 epoch total loss 1.29214418\n",
      "Trained batch 1183 batch loss 1.36824048 epoch total loss 1.29220855\n",
      "Trained batch 1184 batch loss 1.36099887 epoch total loss 1.29226661\n",
      "Trained batch 1185 batch loss 1.28748035 epoch total loss 1.29226255\n",
      "Trained batch 1186 batch loss 1.4072057 epoch total loss 1.29235947\n",
      "Trained batch 1187 batch loss 1.29269743 epoch total loss 1.29235983\n",
      "Trained batch 1188 batch loss 1.25836837 epoch total loss 1.29233122\n",
      "Trained batch 1189 batch loss 1.30914581 epoch total loss 1.2923454\n",
      "Trained batch 1190 batch loss 1.21355569 epoch total loss 1.29227924\n",
      "Trained batch 1191 batch loss 1.12099361 epoch total loss 1.29213536\n",
      "Trained batch 1192 batch loss 1.23366499 epoch total loss 1.29208624\n",
      "Trained batch 1193 batch loss 1.1776799 epoch total loss 1.2919904\n",
      "Trained batch 1194 batch loss 1.23244143 epoch total loss 1.29194057\n",
      "Trained batch 1195 batch loss 1.24001825 epoch total loss 1.29189706\n",
      "Trained batch 1196 batch loss 1.19053078 epoch total loss 1.2918123\n",
      "Trained batch 1197 batch loss 1.30227494 epoch total loss 1.29182112\n",
      "Trained batch 1198 batch loss 1.42791247 epoch total loss 1.29193461\n",
      "Trained batch 1199 batch loss 1.44705665 epoch total loss 1.29206395\n",
      "Trained batch 1200 batch loss 1.26459086 epoch total loss 1.29204106\n",
      "Trained batch 1201 batch loss 1.27581406 epoch total loss 1.29202759\n",
      "Trained batch 1202 batch loss 1.26298416 epoch total loss 1.29200339\n",
      "Trained batch 1203 batch loss 1.25711346 epoch total loss 1.29197431\n",
      "Trained batch 1204 batch loss 1.37380254 epoch total loss 1.29204226\n",
      "Trained batch 1205 batch loss 1.29188645 epoch total loss 1.29204214\n",
      "Trained batch 1206 batch loss 1.55110109 epoch total loss 1.29225695\n",
      "Trained batch 1207 batch loss 1.45666802 epoch total loss 1.29239321\n",
      "Trained batch 1208 batch loss 1.40091133 epoch total loss 1.29248297\n",
      "Trained batch 1209 batch loss 1.37059426 epoch total loss 1.29254758\n",
      "Trained batch 1210 batch loss 1.31297171 epoch total loss 1.29256451\n",
      "Trained batch 1211 batch loss 1.3812319 epoch total loss 1.29263771\n",
      "Trained batch 1212 batch loss 1.40875506 epoch total loss 1.29273355\n",
      "Trained batch 1213 batch loss 1.28815556 epoch total loss 1.29272985\n",
      "Trained batch 1214 batch loss 1.23330665 epoch total loss 1.29268086\n",
      "Trained batch 1215 batch loss 1.23330665 epoch total loss 1.29263198\n",
      "Trained batch 1216 batch loss 1.33099651 epoch total loss 1.29266357\n",
      "Trained batch 1217 batch loss 1.32284844 epoch total loss 1.29268837\n",
      "Trained batch 1218 batch loss 1.20197034 epoch total loss 1.29261398\n",
      "Trained batch 1219 batch loss 1.23530579 epoch total loss 1.29256701\n",
      "Trained batch 1220 batch loss 1.24194694 epoch total loss 1.29252553\n",
      "Trained batch 1221 batch loss 1.28312731 epoch total loss 1.29251778\n",
      "Trained batch 1222 batch loss 1.37058544 epoch total loss 1.29258168\n",
      "Trained batch 1223 batch loss 1.24388 epoch total loss 1.29254186\n",
      "Trained batch 1224 batch loss 1.28431523 epoch total loss 1.29253507\n",
      "Trained batch 1225 batch loss 1.3634851 epoch total loss 1.29259312\n",
      "Trained batch 1226 batch loss 1.36828971 epoch total loss 1.29265475\n",
      "Trained batch 1227 batch loss 1.38698149 epoch total loss 1.29273164\n",
      "Trained batch 1228 batch loss 1.34408653 epoch total loss 1.29277349\n",
      "Trained batch 1229 batch loss 1.32713354 epoch total loss 1.2928015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1230 batch loss 1.19227505 epoch total loss 1.29271972\n",
      "Trained batch 1231 batch loss 1.31505704 epoch total loss 1.29273784\n",
      "Trained batch 1232 batch loss 1.23110175 epoch total loss 1.29268789\n",
      "Trained batch 1233 batch loss 1.15927351 epoch total loss 1.29257965\n",
      "Trained batch 1234 batch loss 1.20644367 epoch total loss 1.29250979\n",
      "Trained batch 1235 batch loss 1.13212037 epoch total loss 1.29238\n",
      "Trained batch 1236 batch loss 1.32736623 epoch total loss 1.29240823\n",
      "Trained batch 1237 batch loss 1.41598308 epoch total loss 1.29250824\n",
      "Trained batch 1238 batch loss 1.35003781 epoch total loss 1.29255474\n",
      "Trained batch 1239 batch loss 1.21476531 epoch total loss 1.29249191\n",
      "Trained batch 1240 batch loss 1.34675825 epoch total loss 1.29253566\n",
      "Trained batch 1241 batch loss 1.33459556 epoch total loss 1.29256964\n",
      "Trained batch 1242 batch loss 1.19742811 epoch total loss 1.29249299\n",
      "Trained batch 1243 batch loss 1.15942597 epoch total loss 1.29238594\n",
      "Trained batch 1244 batch loss 1.23615742 epoch total loss 1.29234076\n",
      "Trained batch 1245 batch loss 1.24518239 epoch total loss 1.29230285\n",
      "Trained batch 1246 batch loss 1.21844661 epoch total loss 1.29224372\n",
      "Trained batch 1247 batch loss 1.29319394 epoch total loss 1.29224443\n",
      "Trained batch 1248 batch loss 1.31584191 epoch total loss 1.29226327\n",
      "Trained batch 1249 batch loss 1.21742177 epoch total loss 1.29220343\n",
      "Trained batch 1250 batch loss 1.17638803 epoch total loss 1.2921108\n",
      "Trained batch 1251 batch loss 1.20461059 epoch total loss 1.29204082\n",
      "Trained batch 1252 batch loss 1.33591354 epoch total loss 1.29207587\n",
      "Trained batch 1253 batch loss 1.3636353 epoch total loss 1.29213297\n",
      "Trained batch 1254 batch loss 1.24667382 epoch total loss 1.29209673\n",
      "Trained batch 1255 batch loss 1.19180393 epoch total loss 1.29201674\n",
      "Trained batch 1256 batch loss 1.22300386 epoch total loss 1.29196191\n",
      "Trained batch 1257 batch loss 1.34167504 epoch total loss 1.29200137\n",
      "Trained batch 1258 batch loss 1.24369574 epoch total loss 1.29196298\n",
      "Trained batch 1259 batch loss 1.21901178 epoch total loss 1.29190505\n",
      "Trained batch 1260 batch loss 1.1958251 epoch total loss 1.29182875\n",
      "Trained batch 1261 batch loss 1.27117705 epoch total loss 1.2918123\n",
      "Trained batch 1262 batch loss 1.30806661 epoch total loss 1.29182529\n",
      "Trained batch 1263 batch loss 1.23271441 epoch total loss 1.29177845\n",
      "Trained batch 1264 batch loss 1.20280361 epoch total loss 1.29170799\n",
      "Trained batch 1265 batch loss 1.19684529 epoch total loss 1.29163301\n",
      "Trained batch 1266 batch loss 1.31728029 epoch total loss 1.29165328\n",
      "Trained batch 1267 batch loss 1.20580184 epoch total loss 1.29158556\n",
      "Trained batch 1268 batch loss 1.27584791 epoch total loss 1.29157317\n",
      "Trained batch 1269 batch loss 1.36094761 epoch total loss 1.29162776\n",
      "Trained batch 1270 batch loss 1.25512481 epoch total loss 1.29159904\n",
      "Trained batch 1271 batch loss 1.21859157 epoch total loss 1.2915417\n",
      "Trained batch 1272 batch loss 1.33513391 epoch total loss 1.29157591\n",
      "Trained batch 1273 batch loss 1.31953669 epoch total loss 1.29159784\n",
      "Trained batch 1274 batch loss 1.29089665 epoch total loss 1.29159737\n",
      "Trained batch 1275 batch loss 1.41785622 epoch total loss 1.29169631\n",
      "Trained batch 1276 batch loss 1.45103252 epoch total loss 1.29182124\n",
      "Trained batch 1277 batch loss 1.41864526 epoch total loss 1.29192054\n",
      "Trained batch 1278 batch loss 1.32015014 epoch total loss 1.29194272\n",
      "Trained batch 1279 batch loss 1.17855513 epoch total loss 1.29185402\n",
      "Trained batch 1280 batch loss 1.19341314 epoch total loss 1.29177713\n",
      "Trained batch 1281 batch loss 1.23552263 epoch total loss 1.29173315\n",
      "Trained batch 1282 batch loss 1.34054899 epoch total loss 1.29177129\n",
      "Trained batch 1283 batch loss 1.29633069 epoch total loss 1.29177487\n",
      "Trained batch 1284 batch loss 1.29214 epoch total loss 1.29177511\n",
      "Trained batch 1285 batch loss 1.30195928 epoch total loss 1.29178309\n",
      "Trained batch 1286 batch loss 1.31347811 epoch total loss 1.2918\n",
      "Trained batch 1287 batch loss 1.31835246 epoch total loss 1.29182065\n",
      "Trained batch 1288 batch loss 1.37044787 epoch total loss 1.29188168\n",
      "Trained batch 1289 batch loss 1.31172621 epoch total loss 1.29189706\n",
      "Trained batch 1290 batch loss 1.3544991 epoch total loss 1.29194558\n",
      "Trained batch 1291 batch loss 1.28904867 epoch total loss 1.29194343\n",
      "Trained batch 1292 batch loss 1.34576988 epoch total loss 1.29198515\n",
      "Trained batch 1293 batch loss 1.31245494 epoch total loss 1.29200101\n",
      "Trained batch 1294 batch loss 1.26822543 epoch total loss 1.29198253\n",
      "Trained batch 1295 batch loss 1.26721668 epoch total loss 1.29196346\n",
      "Trained batch 1296 batch loss 1.29021525 epoch total loss 1.29196203\n",
      "Trained batch 1297 batch loss 1.22640777 epoch total loss 1.29191148\n",
      "Trained batch 1298 batch loss 1.32562649 epoch total loss 1.29193759\n",
      "Trained batch 1299 batch loss 1.25972557 epoch total loss 1.29191279\n",
      "Trained batch 1300 batch loss 1.23279691 epoch total loss 1.29186726\n",
      "Trained batch 1301 batch loss 1.17647827 epoch total loss 1.29177868\n",
      "Trained batch 1302 batch loss 1.20521116 epoch total loss 1.29171216\n",
      "Trained batch 1303 batch loss 1.15587759 epoch total loss 1.29160786\n",
      "Trained batch 1304 batch loss 1.21263051 epoch total loss 1.2915473\n",
      "Trained batch 1305 batch loss 1.24620461 epoch total loss 1.29151261\n",
      "Trained batch 1306 batch loss 1.14340484 epoch total loss 1.29139924\n",
      "Trained batch 1307 batch loss 1.25383902 epoch total loss 1.29137039\n",
      "Trained batch 1308 batch loss 1.29787993 epoch total loss 1.2913754\n",
      "Trained batch 1309 batch loss 1.1737082 epoch total loss 1.29128551\n",
      "Trained batch 1310 batch loss 1.06767821 epoch total loss 1.29111481\n",
      "Trained batch 1311 batch loss 1.04583025 epoch total loss 1.29092765\n",
      "Trained batch 1312 batch loss 1.04434752 epoch total loss 1.29073966\n",
      "Trained batch 1313 batch loss 1.26178074 epoch total loss 1.2907176\n",
      "Trained batch 1314 batch loss 1.32199073 epoch total loss 1.29074144\n",
      "Trained batch 1315 batch loss 1.40662146 epoch total loss 1.29082954\n",
      "Trained batch 1316 batch loss 1.19190705 epoch total loss 1.29075444\n",
      "Trained batch 1317 batch loss 1.28880346 epoch total loss 1.29075289\n",
      "Trained batch 1318 batch loss 1.11561823 epoch total loss 1.29062009\n",
      "Trained batch 1319 batch loss 1.14876151 epoch total loss 1.29051256\n",
      "Trained batch 1320 batch loss 1.20720482 epoch total loss 1.29044938\n",
      "Trained batch 1321 batch loss 1.29814851 epoch total loss 1.29045522\n",
      "Trained batch 1322 batch loss 1.25003815 epoch total loss 1.29042459\n",
      "Trained batch 1323 batch loss 1.11127734 epoch total loss 1.29028916\n",
      "Trained batch 1324 batch loss 0.97514534 epoch total loss 1.2900511\n",
      "Trained batch 1325 batch loss 1.16893125 epoch total loss 1.28995979\n",
      "Trained batch 1326 batch loss 1.32923591 epoch total loss 1.28998935\n",
      "Trained batch 1327 batch loss 1.35756683 epoch total loss 1.29004025\n",
      "Trained batch 1328 batch loss 1.2806679 epoch total loss 1.29003322\n",
      "Trained batch 1329 batch loss 1.40883446 epoch total loss 1.29012251\n",
      "Trained batch 1330 batch loss 1.43391573 epoch total loss 1.29023075\n",
      "Trained batch 1331 batch loss 1.28784478 epoch total loss 1.29022896\n",
      "Trained batch 1332 batch loss 1.27770352 epoch total loss 1.29021955\n",
      "Trained batch 1333 batch loss 1.39188361 epoch total loss 1.29029572\n",
      "Trained batch 1334 batch loss 1.43010688 epoch total loss 1.29040051\n",
      "Trained batch 1335 batch loss 1.24869955 epoch total loss 1.29036927\n",
      "Trained batch 1336 batch loss 1.16677833 epoch total loss 1.29027677\n",
      "Trained batch 1337 batch loss 1.17717302 epoch total loss 1.29019213\n",
      "Trained batch 1338 batch loss 1.09378886 epoch total loss 1.29004526\n",
      "Trained batch 1339 batch loss 1.19214427 epoch total loss 1.28997219\n",
      "Trained batch 1340 batch loss 1.30027127 epoch total loss 1.28997982\n",
      "Trained batch 1341 batch loss 1.25500035 epoch total loss 1.28995371\n",
      "Trained batch 1342 batch loss 1.36572134 epoch total loss 1.29001021\n",
      "Trained batch 1343 batch loss 1.39089775 epoch total loss 1.29008532\n",
      "Trained batch 1344 batch loss 1.27775335 epoch total loss 1.29007614\n",
      "Trained batch 1345 batch loss 1.13921607 epoch total loss 1.28996396\n",
      "Trained batch 1346 batch loss 1.14999485 epoch total loss 1.28985989\n",
      "Trained batch 1347 batch loss 1.17889261 epoch total loss 1.28977752\n",
      "Trained batch 1348 batch loss 1.37715364 epoch total loss 1.28984237\n",
      "Trained batch 1349 batch loss 1.28982842 epoch total loss 1.28984237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1350 batch loss 1.39150763 epoch total loss 1.28991759\n",
      "Trained batch 1351 batch loss 1.35521364 epoch total loss 1.28996599\n",
      "Trained batch 1352 batch loss 1.28751636 epoch total loss 1.28996408\n",
      "Trained batch 1353 batch loss 1.17354751 epoch total loss 1.28987813\n",
      "Trained batch 1354 batch loss 1.1033144 epoch total loss 1.28974032\n",
      "Trained batch 1355 batch loss 1.32276738 epoch total loss 1.28976464\n",
      "Trained batch 1356 batch loss 1.27704489 epoch total loss 1.28975534\n",
      "Trained batch 1357 batch loss 1.31137645 epoch total loss 1.28977132\n",
      "Trained batch 1358 batch loss 1.40307045 epoch total loss 1.28985465\n",
      "Trained batch 1359 batch loss 1.29220343 epoch total loss 1.28985643\n",
      "Trained batch 1360 batch loss 1.31409943 epoch total loss 1.28987432\n",
      "Trained batch 1361 batch loss 1.33718252 epoch total loss 1.28990901\n",
      "Trained batch 1362 batch loss 1.51512051 epoch total loss 1.29007435\n",
      "Trained batch 1363 batch loss 1.33002186 epoch total loss 1.29010367\n",
      "Trained batch 1364 batch loss 1.43409336 epoch total loss 1.29020929\n",
      "Trained batch 1365 batch loss 1.36935627 epoch total loss 1.29026723\n",
      "Trained batch 1366 batch loss 1.32650793 epoch total loss 1.29029381\n",
      "Trained batch 1367 batch loss 1.23865724 epoch total loss 1.29025602\n",
      "Trained batch 1368 batch loss 1.23764825 epoch total loss 1.29021764\n",
      "Trained batch 1369 batch loss 1.19253576 epoch total loss 1.29014623\n",
      "Trained batch 1370 batch loss 1.22322142 epoch total loss 1.29009748\n",
      "Trained batch 1371 batch loss 1.33545899 epoch total loss 1.2901305\n",
      "Trained batch 1372 batch loss 1.25481474 epoch total loss 1.29010475\n",
      "Trained batch 1373 batch loss 1.62142503 epoch total loss 1.29034603\n",
      "Trained batch 1374 batch loss 1.46923709 epoch total loss 1.2904762\n",
      "Trained batch 1375 batch loss 1.37243927 epoch total loss 1.29053581\n",
      "Trained batch 1376 batch loss 1.2721647 epoch total loss 1.29052258\n",
      "Trained batch 1377 batch loss 1.42891979 epoch total loss 1.29062307\n",
      "Trained batch 1378 batch loss 1.26064181 epoch total loss 1.29060125\n",
      "Trained batch 1379 batch loss 1.35615063 epoch total loss 1.29064894\n",
      "Trained batch 1380 batch loss 1.34204149 epoch total loss 1.29068613\n",
      "Trained batch 1381 batch loss 1.23092031 epoch total loss 1.29064286\n",
      "Trained batch 1382 batch loss 1.23420012 epoch total loss 1.29060209\n",
      "Trained batch 1383 batch loss 1.34456372 epoch total loss 1.29064107\n",
      "Trained batch 1384 batch loss 1.27467608 epoch total loss 1.29062963\n",
      "Trained batch 1385 batch loss 1.21807432 epoch total loss 1.29057717\n",
      "Trained batch 1386 batch loss 1.2032671 epoch total loss 1.29051411\n",
      "Trained batch 1387 batch loss 1.24954653 epoch total loss 1.29048455\n",
      "Trained batch 1388 batch loss 1.27303684 epoch total loss 1.29047203\n",
      "Epoch 3 train loss 1.2904720306396484\n",
      "Validated batch 1 batch loss 1.3577913\n",
      "Validated batch 2 batch loss 1.2316829\n",
      "Validated batch 3 batch loss 1.23545396\n",
      "Validated batch 4 batch loss 1.22081625\n",
      "Validated batch 5 batch loss 1.34490585\n",
      "Validated batch 6 batch loss 1.38224673\n",
      "Validated batch 7 batch loss 1.20195878\n",
      "Validated batch 8 batch loss 1.29950798\n",
      "Validated batch 9 batch loss 1.30508709\n",
      "Validated batch 10 batch loss 1.27116883\n",
      "Validated batch 11 batch loss 1.3051759\n",
      "Validated batch 12 batch loss 1.15327036\n",
      "Validated batch 13 batch loss 1.45470452\n",
      "Validated batch 14 batch loss 1.19692266\n",
      "Validated batch 15 batch loss 1.33488762\n",
      "Validated batch 16 batch loss 1.32297778\n",
      "Validated batch 17 batch loss 1.3263483\n",
      "Validated batch 18 batch loss 1.1077559\n",
      "Validated batch 19 batch loss 1.30744171\n",
      "Validated batch 20 batch loss 1.20497847\n",
      "Validated batch 21 batch loss 1.26835179\n",
      "Validated batch 22 batch loss 1.24135041\n",
      "Validated batch 23 batch loss 1.24063766\n",
      "Validated batch 24 batch loss 1.20614398\n",
      "Validated batch 25 batch loss 1.24627817\n",
      "Validated batch 26 batch loss 1.28076935\n",
      "Validated batch 27 batch loss 1.21561456\n",
      "Validated batch 28 batch loss 1.34313095\n",
      "Validated batch 29 batch loss 1.39057279\n",
      "Validated batch 30 batch loss 1.08904791\n",
      "Validated batch 31 batch loss 1.25205326\n",
      "Validated batch 32 batch loss 1.24355888\n",
      "Validated batch 33 batch loss 1.3546536\n",
      "Validated batch 34 batch loss 1.30115771\n",
      "Validated batch 35 batch loss 1.10544729\n",
      "Validated batch 36 batch loss 1.12253881\n",
      "Validated batch 37 batch loss 1.22202158\n",
      "Validated batch 38 batch loss 1.2079128\n",
      "Validated batch 39 batch loss 1.21753335\n",
      "Validated batch 40 batch loss 1.26664948\n",
      "Validated batch 41 batch loss 1.15402007\n",
      "Validated batch 42 batch loss 1.28644705\n",
      "Validated batch 43 batch loss 1.33345366\n",
      "Validated batch 44 batch loss 1.31907272\n",
      "Validated batch 45 batch loss 1.26164043\n",
      "Validated batch 46 batch loss 1.14160895\n",
      "Validated batch 47 batch loss 1.23448384\n",
      "Validated batch 48 batch loss 1.16539335\n",
      "Validated batch 49 batch loss 1.22989511\n",
      "Validated batch 50 batch loss 1.18949604\n",
      "Validated batch 51 batch loss 1.26147532\n",
      "Validated batch 52 batch loss 1.27745879\n",
      "Validated batch 53 batch loss 1.27162552\n",
      "Validated batch 54 batch loss 1.22701311\n",
      "Validated batch 55 batch loss 1.29551041\n",
      "Validated batch 56 batch loss 1.27163517\n",
      "Validated batch 57 batch loss 1.27606928\n",
      "Validated batch 58 batch loss 1.31071\n",
      "Validated batch 59 batch loss 1.21514976\n",
      "Validated batch 60 batch loss 1.20233142\n",
      "Validated batch 61 batch loss 1.29594421\n",
      "Validated batch 62 batch loss 1.19668877\n",
      "Validated batch 63 batch loss 1.41585827\n",
      "Validated batch 64 batch loss 1.31303358\n",
      "Validated batch 65 batch loss 1.14434469\n",
      "Validated batch 66 batch loss 1.34887314\n",
      "Validated batch 67 batch loss 1.23548543\n",
      "Validated batch 68 batch loss 1.18526447\n",
      "Validated batch 69 batch loss 1.29036558\n",
      "Validated batch 70 batch loss 1.23301506\n",
      "Validated batch 71 batch loss 1.28833818\n",
      "Validated batch 72 batch loss 1.24951148\n",
      "Validated batch 73 batch loss 1.15727437\n",
      "Validated batch 74 batch loss 1.20514357\n",
      "Validated batch 75 batch loss 1.35547614\n",
      "Validated batch 76 batch loss 1.20445275\n",
      "Validated batch 77 batch loss 1.15308416\n",
      "Validated batch 78 batch loss 1.23443818\n",
      "Validated batch 79 batch loss 1.23468411\n",
      "Validated batch 80 batch loss 1.12961912\n",
      "Validated batch 81 batch loss 1.29228818\n",
      "Validated batch 82 batch loss 1.23829615\n",
      "Validated batch 83 batch loss 1.22336662\n",
      "Validated batch 84 batch loss 1.33734465\n",
      "Validated batch 85 batch loss 1.34645557\n",
      "Validated batch 86 batch loss 1.20510554\n",
      "Validated batch 87 batch loss 1.33520436\n",
      "Validated batch 88 batch loss 1.09599769\n",
      "Validated batch 89 batch loss 1.21760094\n",
      "Validated batch 90 batch loss 1.20866776\n",
      "Validated batch 91 batch loss 1.23439801\n",
      "Validated batch 92 batch loss 1.43368959\n",
      "Validated batch 93 batch loss 1.26072788\n",
      "Validated batch 94 batch loss 1.31536281\n",
      "Validated batch 95 batch loss 1.21224\n",
      "Validated batch 96 batch loss 1.22735071\n",
      "Validated batch 97 batch loss 1.25501132\n",
      "Validated batch 98 batch loss 1.33017802\n",
      "Validated batch 99 batch loss 1.38913047\n",
      "Validated batch 100 batch loss 1.33691156\n",
      "Validated batch 101 batch loss 1.32468235\n",
      "Validated batch 102 batch loss 1.21837616\n",
      "Validated batch 103 batch loss 1.32075894\n",
      "Validated batch 104 batch loss 1.28839302\n",
      "Validated batch 105 batch loss 1.26421857\n",
      "Validated batch 106 batch loss 1.35462236\n",
      "Validated batch 107 batch loss 1.34019184\n",
      "Validated batch 108 batch loss 1.3177464\n",
      "Validated batch 109 batch loss 1.39681208\n",
      "Validated batch 110 batch loss 1.14632332\n",
      "Validated batch 111 batch loss 1.29353666\n",
      "Validated batch 112 batch loss 1.19839513\n",
      "Validated batch 113 batch loss 1.20581269\n",
      "Validated batch 114 batch loss 1.38397646\n",
      "Validated batch 115 batch loss 1.2144413\n",
      "Validated batch 116 batch loss 1.33791041\n",
      "Validated batch 117 batch loss 1.30862045\n",
      "Validated batch 118 batch loss 1.1965344\n",
      "Validated batch 119 batch loss 1.21370566\n",
      "Validated batch 120 batch loss 1.26804221\n",
      "Validated batch 121 batch loss 1.25316274\n",
      "Validated batch 122 batch loss 1.29626918\n",
      "Validated batch 123 batch loss 1.20491886\n",
      "Validated batch 124 batch loss 1.19911146\n",
      "Validated batch 125 batch loss 1.35941553\n",
      "Validated batch 126 batch loss 1.18716633\n",
      "Validated batch 127 batch loss 1.16021299\n",
      "Validated batch 128 batch loss 1.21753347\n",
      "Validated batch 129 batch loss 1.3837167\n",
      "Validated batch 130 batch loss 1.37842822\n",
      "Validated batch 131 batch loss 1.40681863\n",
      "Validated batch 132 batch loss 1.21807814\n",
      "Validated batch 133 batch loss 1.43217063\n",
      "Validated batch 134 batch loss 1.28570151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 135 batch loss 1.33297086\n",
      "Validated batch 136 batch loss 1.33329046\n",
      "Validated batch 137 batch loss 1.03144503\n",
      "Validated batch 138 batch loss 1.23656714\n",
      "Validated batch 139 batch loss 1.25562143\n",
      "Validated batch 140 batch loss 1.20729327\n",
      "Validated batch 141 batch loss 1.22870564\n",
      "Validated batch 142 batch loss 1.20585179\n",
      "Validated batch 143 batch loss 1.17307305\n",
      "Validated batch 144 batch loss 1.32975769\n",
      "Validated batch 145 batch loss 1.23608613\n",
      "Validated batch 146 batch loss 1.34058428\n",
      "Validated batch 147 batch loss 1.28236616\n",
      "Validated batch 148 batch loss 1.27206981\n",
      "Validated batch 149 batch loss 1.26585591\n",
      "Validated batch 150 batch loss 1.42539775\n",
      "Validated batch 151 batch loss 1.31355071\n",
      "Validated batch 152 batch loss 1.3523798\n",
      "Validated batch 153 batch loss 1.32475829\n",
      "Validated batch 154 batch loss 1.38832378\n",
      "Validated batch 155 batch loss 1.29548621\n",
      "Validated batch 156 batch loss 1.19412911\n",
      "Validated batch 157 batch loss 1.26601362\n",
      "Validated batch 158 batch loss 1.32660902\n",
      "Validated batch 159 batch loss 1.33261442\n",
      "Validated batch 160 batch loss 1.24057281\n",
      "Validated batch 161 batch loss 1.27288115\n",
      "Validated batch 162 batch loss 1.32128656\n",
      "Validated batch 163 batch loss 1.2425108\n",
      "Validated batch 164 batch loss 1.27285624\n",
      "Validated batch 165 batch loss 1.25968838\n",
      "Validated batch 166 batch loss 1.17448568\n",
      "Validated batch 167 batch loss 1.29606044\n",
      "Validated batch 168 batch loss 1.28906357\n",
      "Validated batch 169 batch loss 1.20078242\n",
      "Validated batch 170 batch loss 1.17988777\n",
      "Validated batch 171 batch loss 1.28503549\n",
      "Validated batch 172 batch loss 1.2542274\n",
      "Validated batch 173 batch loss 1.34124756\n",
      "Validated batch 174 batch loss 1.2996676\n",
      "Validated batch 175 batch loss 1.14539778\n",
      "Validated batch 176 batch loss 1.25255203\n",
      "Validated batch 177 batch loss 1.27201605\n",
      "Validated batch 178 batch loss 1.23138189\n",
      "Validated batch 179 batch loss 1.29315162\n",
      "Validated batch 180 batch loss 1.29162765\n",
      "Validated batch 181 batch loss 1.41909528\n",
      "Validated batch 182 batch loss 1.43437064\n",
      "Validated batch 183 batch loss 1.28912735\n",
      "Validated batch 184 batch loss 1.17463624\n",
      "Validated batch 185 batch loss 1.08853948\n",
      "Epoch 3 val loss 1.2649729251861572\n",
      "Model /aiffel/aiffel/mpii/trained/model-epoch-3-loss-1.2650.h5 saved.\n",
      "Start epoch 4 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.39204407 epoch total loss 1.39204407\n",
      "Trained batch 2 batch loss 1.2391268 epoch total loss 1.31558537\n",
      "Trained batch 3 batch loss 1.22798574 epoch total loss 1.28638554\n",
      "Trained batch 4 batch loss 1.13920045 epoch total loss 1.2495892\n",
      "Trained batch 5 batch loss 1.35571194 epoch total loss 1.2708137\n",
      "Trained batch 6 batch loss 1.33971357 epoch total loss 1.28229702\n",
      "Trained batch 7 batch loss 1.38337231 epoch total loss 1.29673636\n",
      "Trained batch 8 batch loss 1.3213532 epoch total loss 1.29981339\n",
      "Trained batch 9 batch loss 1.22326589 epoch total loss 1.29130805\n",
      "Trained batch 10 batch loss 1.26588058 epoch total loss 1.28876531\n",
      "Trained batch 11 batch loss 1.25009441 epoch total loss 1.28524983\n",
      "Trained batch 12 batch loss 1.27538538 epoch total loss 1.28442776\n",
      "Trained batch 13 batch loss 1.28963923 epoch total loss 1.28482866\n",
      "Trained batch 14 batch loss 1.19958818 epoch total loss 1.27874\n",
      "Trained batch 15 batch loss 1.34540558 epoch total loss 1.28318441\n",
      "Trained batch 16 batch loss 1.25331175 epoch total loss 1.28131735\n",
      "Trained batch 17 batch loss 1.31996596 epoch total loss 1.28359079\n",
      "Trained batch 18 batch loss 1.26590014 epoch total loss 1.28260791\n",
      "Trained batch 19 batch loss 1.31151581 epoch total loss 1.28412938\n",
      "Trained batch 20 batch loss 1.39515 epoch total loss 1.28968036\n",
      "Trained batch 21 batch loss 1.25129414 epoch total loss 1.28785253\n",
      "Trained batch 22 batch loss 1.15905523 epoch total loss 1.28199816\n",
      "Trained batch 23 batch loss 1.23285186 epoch total loss 1.27986133\n",
      "Trained batch 24 batch loss 1.18655324 epoch total loss 1.27597344\n",
      "Trained batch 25 batch loss 1.15687275 epoch total loss 1.27120936\n",
      "Trained batch 26 batch loss 1.12721264 epoch total loss 1.26567101\n",
      "Trained batch 27 batch loss 1.16491866 epoch total loss 1.26193941\n",
      "Trained batch 28 batch loss 1.09732628 epoch total loss 1.25606048\n",
      "Trained batch 29 batch loss 1.11660814 epoch total loss 1.2512517\n",
      "Trained batch 30 batch loss 1.16058779 epoch total loss 1.24822962\n",
      "Trained batch 31 batch loss 1.14291084 epoch total loss 1.24483216\n",
      "Trained batch 32 batch loss 1.11017287 epoch total loss 1.24062407\n",
      "Trained batch 33 batch loss 1.13033807 epoch total loss 1.23728204\n",
      "Trained batch 34 batch loss 1.20065284 epoch total loss 1.23620474\n",
      "Trained batch 35 batch loss 1.30846286 epoch total loss 1.23826921\n",
      "Trained batch 36 batch loss 1.43216467 epoch total loss 1.2436552\n",
      "Trained batch 37 batch loss 1.304057 epoch total loss 1.24528766\n",
      "Trained batch 38 batch loss 1.32993937 epoch total loss 1.24751544\n",
      "Trained batch 39 batch loss 1.35335541 epoch total loss 1.25022924\n",
      "Trained batch 40 batch loss 1.24650681 epoch total loss 1.25013614\n",
      "Trained batch 41 batch loss 1.50862992 epoch total loss 1.25644088\n",
      "Trained batch 42 batch loss 1.19036579 epoch total loss 1.25486767\n",
      "Trained batch 43 batch loss 1.15564871 epoch total loss 1.25256014\n",
      "Trained batch 44 batch loss 1.22167706 epoch total loss 1.25185823\n",
      "Trained batch 45 batch loss 1.22413826 epoch total loss 1.25124228\n",
      "Trained batch 46 batch loss 1.28486574 epoch total loss 1.25197315\n",
      "Trained batch 47 batch loss 1.42907619 epoch total loss 1.25574136\n",
      "Trained batch 48 batch loss 1.44503236 epoch total loss 1.25968492\n",
      "Trained batch 49 batch loss 1.49031913 epoch total loss 1.26439178\n",
      "Trained batch 50 batch loss 1.48999357 epoch total loss 1.26890385\n",
      "Trained batch 51 batch loss 1.3153913 epoch total loss 1.26981533\n",
      "Trained batch 52 batch loss 1.16046512 epoch total loss 1.26771235\n",
      "Trained batch 53 batch loss 1.23300612 epoch total loss 1.26705766\n",
      "Trained batch 54 batch loss 1.2768892 epoch total loss 1.26723957\n",
      "Trained batch 55 batch loss 1.17939901 epoch total loss 1.26564252\n",
      "Trained batch 56 batch loss 1.13138354 epoch total loss 1.26324499\n",
      "Trained batch 57 batch loss 1.18421173 epoch total loss 1.26185846\n",
      "Trained batch 58 batch loss 1.20128703 epoch total loss 1.26081419\n",
      "Trained batch 59 batch loss 1.06132734 epoch total loss 1.25743294\n",
      "Trained batch 60 batch loss 1.07484841 epoch total loss 1.25439\n",
      "Trained batch 61 batch loss 1.10902 epoch total loss 1.25200677\n",
      "Trained batch 62 batch loss 1.32905579 epoch total loss 1.25324953\n",
      "Trained batch 63 batch loss 1.11671674 epoch total loss 1.2510823\n",
      "Trained batch 64 batch loss 1.11309111 epoch total loss 1.24892616\n",
      "Trained batch 65 batch loss 1.09567761 epoch total loss 1.24656856\n",
      "Trained batch 66 batch loss 1.01867282 epoch total loss 1.24311554\n",
      "Trained batch 67 batch loss 1.0419724 epoch total loss 1.24011338\n",
      "Trained batch 68 batch loss 1.33653951 epoch total loss 1.24153137\n",
      "Trained batch 69 batch loss 1.37951422 epoch total loss 1.24353111\n",
      "Trained batch 70 batch loss 1.32438111 epoch total loss 1.24468613\n",
      "Trained batch 71 batch loss 1.42902875 epoch total loss 1.24728251\n",
      "Trained batch 72 batch loss 1.30274165 epoch total loss 1.24805284\n",
      "Trained batch 73 batch loss 1.3358115 epoch total loss 1.24925494\n",
      "Trained batch 74 batch loss 1.27954674 epoch total loss 1.24966431\n",
      "Trained batch 75 batch loss 1.36970687 epoch total loss 1.25126481\n",
      "Trained batch 76 batch loss 1.32151747 epoch total loss 1.25218928\n",
      "Trained batch 77 batch loss 1.21439123 epoch total loss 1.25169837\n",
      "Trained batch 78 batch loss 1.20727277 epoch total loss 1.25112891\n",
      "Trained batch 79 batch loss 1.08975708 epoch total loss 1.24908626\n",
      "Trained batch 80 batch loss 1.07452154 epoch total loss 1.24690413\n",
      "Trained batch 81 batch loss 1.14075375 epoch total loss 1.24559367\n",
      "Trained batch 82 batch loss 1.30453682 epoch total loss 1.2463125\n",
      "Trained batch 83 batch loss 1.24053442 epoch total loss 1.24624288\n",
      "Trained batch 84 batch loss 1.26211989 epoch total loss 1.24643195\n",
      "Trained batch 85 batch loss 1.33501208 epoch total loss 1.24747407\n",
      "Trained batch 86 batch loss 1.28668296 epoch total loss 1.24792993\n",
      "Trained batch 87 batch loss 1.33026 epoch total loss 1.24887633\n",
      "Trained batch 88 batch loss 1.34541392 epoch total loss 1.2499733\n",
      "Trained batch 89 batch loss 1.22896171 epoch total loss 1.24973714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 90 batch loss 1.20977867 epoch total loss 1.24929321\n",
      "Trained batch 91 batch loss 1.24775112 epoch total loss 1.24927616\n",
      "Trained batch 92 batch loss 1.24237096 epoch total loss 1.24920118\n",
      "Trained batch 93 batch loss 1.28271747 epoch total loss 1.24956155\n",
      "Trained batch 94 batch loss 1.26788592 epoch total loss 1.24975646\n",
      "Trained batch 95 batch loss 1.40988863 epoch total loss 1.25144207\n",
      "Trained batch 96 batch loss 1.39069319 epoch total loss 1.25289261\n",
      "Trained batch 97 batch loss 1.40142477 epoch total loss 1.25442386\n",
      "Trained batch 98 batch loss 1.29705894 epoch total loss 1.25485897\n",
      "Trained batch 99 batch loss 1.34200096 epoch total loss 1.25573921\n",
      "Trained batch 100 batch loss 1.21403837 epoch total loss 1.2553221\n",
      "Trained batch 101 batch loss 1.26291955 epoch total loss 1.25539732\n",
      "Trained batch 102 batch loss 1.21094942 epoch total loss 1.25496149\n",
      "Trained batch 103 batch loss 1.2755872 epoch total loss 1.25516176\n",
      "Trained batch 104 batch loss 1.19499278 epoch total loss 1.25458324\n",
      "Trained batch 105 batch loss 1.26097763 epoch total loss 1.25464404\n",
      "Trained batch 106 batch loss 1.27249098 epoch total loss 1.25481248\n",
      "Trained batch 107 batch loss 1.28142953 epoch total loss 1.25506127\n",
      "Trained batch 108 batch loss 1.29016781 epoch total loss 1.25538623\n",
      "Trained batch 109 batch loss 1.45473206 epoch total loss 1.25721502\n",
      "Trained batch 110 batch loss 1.33017087 epoch total loss 1.2578783\n",
      "Trained batch 111 batch loss 1.36983395 epoch total loss 1.25888681\n",
      "Trained batch 112 batch loss 1.17575502 epoch total loss 1.2581445\n",
      "Trained batch 113 batch loss 1.16343093 epoch total loss 1.25730634\n",
      "Trained batch 114 batch loss 1.15450406 epoch total loss 1.25640464\n",
      "Trained batch 115 batch loss 1.21397042 epoch total loss 1.25603569\n",
      "Trained batch 116 batch loss 1.19607711 epoch total loss 1.25551879\n",
      "Trained batch 117 batch loss 1.30768549 epoch total loss 1.25596464\n",
      "Trained batch 118 batch loss 1.23545158 epoch total loss 1.25579083\n",
      "Trained batch 119 batch loss 1.24574935 epoch total loss 1.25570643\n",
      "Trained batch 120 batch loss 1.18172193 epoch total loss 1.25508988\n",
      "Trained batch 121 batch loss 1.13814378 epoch total loss 1.25412333\n",
      "Trained batch 122 batch loss 1.27228165 epoch total loss 1.2542721\n",
      "Trained batch 123 batch loss 1.20617318 epoch total loss 1.2538811\n",
      "Trained batch 124 batch loss 1.17401755 epoch total loss 1.25323701\n",
      "Trained batch 125 batch loss 1.22166908 epoch total loss 1.2529844\n",
      "Trained batch 126 batch loss 1.24723363 epoch total loss 1.25293875\n",
      "Trained batch 127 batch loss 1.19634414 epoch total loss 1.25249314\n",
      "Trained batch 128 batch loss 1.24905789 epoch total loss 1.25246632\n",
      "Trained batch 129 batch loss 1.14131892 epoch total loss 1.25160468\n",
      "Trained batch 130 batch loss 1.15077424 epoch total loss 1.25082898\n",
      "Trained batch 131 batch loss 1.18231928 epoch total loss 1.25030601\n",
      "Trained batch 132 batch loss 1.30041838 epoch total loss 1.25068557\n",
      "Trained batch 133 batch loss 1.4849633 epoch total loss 1.25244713\n",
      "Trained batch 134 batch loss 1.4691968 epoch total loss 1.25406468\n",
      "Trained batch 135 batch loss 1.23802471 epoch total loss 1.25394583\n",
      "Trained batch 136 batch loss 1.28724 epoch total loss 1.25419068\n",
      "Trained batch 137 batch loss 1.27355695 epoch total loss 1.25433207\n",
      "Trained batch 138 batch loss 1.19670796 epoch total loss 1.25391448\n",
      "Trained batch 139 batch loss 1.23634994 epoch total loss 1.25378799\n",
      "Trained batch 140 batch loss 1.14004529 epoch total loss 1.25297558\n",
      "Trained batch 141 batch loss 1.28417289 epoch total loss 1.25319684\n",
      "Trained batch 142 batch loss 1.16469741 epoch total loss 1.25257373\n",
      "Trained batch 143 batch loss 1.08863652 epoch total loss 1.25142729\n",
      "Trained batch 144 batch loss 1.12697923 epoch total loss 1.25056314\n",
      "Trained batch 145 batch loss 1.13820529 epoch total loss 1.24978817\n",
      "Trained batch 146 batch loss 1.42749262 epoch total loss 1.25100529\n",
      "Trained batch 147 batch loss 1.354357 epoch total loss 1.25170839\n",
      "Trained batch 148 batch loss 1.42638969 epoch total loss 1.25288868\n",
      "Trained batch 149 batch loss 1.43350232 epoch total loss 1.2541008\n",
      "Trained batch 150 batch loss 1.30689299 epoch total loss 1.25445282\n",
      "Trained batch 151 batch loss 1.44332099 epoch total loss 1.25570357\n",
      "Trained batch 152 batch loss 1.48180974 epoch total loss 1.25719106\n",
      "Trained batch 153 batch loss 1.4914639 epoch total loss 1.25872231\n",
      "Trained batch 154 batch loss 1.52475178 epoch total loss 1.26044977\n",
      "Trained batch 155 batch loss 1.19322228 epoch total loss 1.26001608\n",
      "Trained batch 156 batch loss 1.21451902 epoch total loss 1.2597245\n",
      "Trained batch 157 batch loss 1.21733594 epoch total loss 1.25945437\n",
      "Trained batch 158 batch loss 1.28260183 epoch total loss 1.259601\n",
      "Trained batch 159 batch loss 1.34170616 epoch total loss 1.26011729\n",
      "Trained batch 160 batch loss 1.21324015 epoch total loss 1.2598244\n",
      "Trained batch 161 batch loss 1.29770803 epoch total loss 1.26005971\n",
      "Trained batch 162 batch loss 1.36810017 epoch total loss 1.26072669\n",
      "Trained batch 163 batch loss 1.38593411 epoch total loss 1.26149487\n",
      "Trained batch 164 batch loss 1.34663391 epoch total loss 1.26201391\n",
      "Trained batch 165 batch loss 1.26290035 epoch total loss 1.26201928\n",
      "Trained batch 166 batch loss 1.25373459 epoch total loss 1.26196945\n",
      "Trained batch 167 batch loss 1.17231607 epoch total loss 1.26143253\n",
      "Trained batch 168 batch loss 1.4098984 epoch total loss 1.26231635\n",
      "Trained batch 169 batch loss 1.18367565 epoch total loss 1.26185095\n",
      "Trained batch 170 batch loss 1.30646884 epoch total loss 1.26211345\n",
      "Trained batch 171 batch loss 1.18526912 epoch total loss 1.26166403\n",
      "Trained batch 172 batch loss 1.14078295 epoch total loss 1.26096117\n",
      "Trained batch 173 batch loss 1.2380507 epoch total loss 1.26082885\n",
      "Trained batch 174 batch loss 1.17801154 epoch total loss 1.26035285\n",
      "Trained batch 175 batch loss 1.19307148 epoch total loss 1.2599684\n",
      "Trained batch 176 batch loss 1.30345392 epoch total loss 1.2602154\n",
      "Trained batch 177 batch loss 1.20094061 epoch total loss 1.25988054\n",
      "Trained batch 178 batch loss 1.02589762 epoch total loss 1.25856602\n",
      "Trained batch 179 batch loss 1.26322484 epoch total loss 1.25859201\n",
      "Trained batch 180 batch loss 1.18020761 epoch total loss 1.25815654\n",
      "Trained batch 181 batch loss 1.25800908 epoch total loss 1.25815582\n",
      "Trained batch 182 batch loss 1.24513519 epoch total loss 1.25808418\n",
      "Trained batch 183 batch loss 1.26047373 epoch total loss 1.25809729\n",
      "Trained batch 184 batch loss 1.14485979 epoch total loss 1.25748181\n",
      "Trained batch 185 batch loss 1.18707502 epoch total loss 1.2571013\n",
      "Trained batch 186 batch loss 1.36046898 epoch total loss 1.25765705\n",
      "Trained batch 187 batch loss 1.27075374 epoch total loss 1.25772703\n",
      "Trained batch 188 batch loss 1.25703859 epoch total loss 1.25772333\n",
      "Trained batch 189 batch loss 1.27564096 epoch total loss 1.2578181\n",
      "Trained batch 190 batch loss 1.17010224 epoch total loss 1.25735652\n",
      "Trained batch 191 batch loss 1.17530584 epoch total loss 1.25692689\n",
      "Trained batch 192 batch loss 1.1046145 epoch total loss 1.25613368\n",
      "Trained batch 193 batch loss 1.17079186 epoch total loss 1.25569141\n",
      "Trained batch 194 batch loss 1.26168787 epoch total loss 1.2557224\n",
      "Trained batch 195 batch loss 1.28138781 epoch total loss 1.25585401\n",
      "Trained batch 196 batch loss 1.28128731 epoch total loss 1.25598371\n",
      "Trained batch 197 batch loss 1.13553834 epoch total loss 1.25537229\n",
      "Trained batch 198 batch loss 1.1102016 epoch total loss 1.25463915\n",
      "Trained batch 199 batch loss 1.28845346 epoch total loss 1.25480902\n",
      "Trained batch 200 batch loss 1.20481861 epoch total loss 1.25455904\n",
      "Trained batch 201 batch loss 1.17878079 epoch total loss 1.2541821\n",
      "Trained batch 202 batch loss 1.15617514 epoch total loss 1.25369692\n",
      "Trained batch 203 batch loss 1.15428281 epoch total loss 1.25320721\n",
      "Trained batch 204 batch loss 1.22366738 epoch total loss 1.25306237\n",
      "Trained batch 205 batch loss 1.29022765 epoch total loss 1.25324368\n",
      "Trained batch 206 batch loss 1.22770751 epoch total loss 1.25311971\n",
      "Trained batch 207 batch loss 1.10355365 epoch total loss 1.25239718\n",
      "Trained batch 208 batch loss 1.18749166 epoch total loss 1.25208521\n",
      "Trained batch 209 batch loss 1.20852721 epoch total loss 1.25187671\n",
      "Trained batch 210 batch loss 1.29108131 epoch total loss 1.25206339\n",
      "Trained batch 211 batch loss 1.24878156 epoch total loss 1.2520479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 212 batch loss 1.26420975 epoch total loss 1.25210524\n",
      "Trained batch 213 batch loss 1.19696271 epoch total loss 1.25184643\n",
      "Trained batch 214 batch loss 1.09638774 epoch total loss 1.25111985\n",
      "Trained batch 215 batch loss 1.11658478 epoch total loss 1.25049412\n",
      "Trained batch 216 batch loss 1.19635653 epoch total loss 1.25024343\n",
      "Trained batch 217 batch loss 1.11548686 epoch total loss 1.24962234\n",
      "Trained batch 218 batch loss 1.20621288 epoch total loss 1.24942327\n",
      "Trained batch 219 batch loss 1.15542603 epoch total loss 1.24899399\n",
      "Trained batch 220 batch loss 1.16253412 epoch total loss 1.24860108\n",
      "Trained batch 221 batch loss 1.19014847 epoch total loss 1.24833655\n",
      "Trained batch 222 batch loss 1.20917249 epoch total loss 1.24816012\n",
      "Trained batch 223 batch loss 1.34726155 epoch total loss 1.24860454\n",
      "Trained batch 224 batch loss 1.28473377 epoch total loss 1.24876583\n",
      "Trained batch 225 batch loss 1.15502429 epoch total loss 1.24834919\n",
      "Trained batch 226 batch loss 1.06943452 epoch total loss 1.24755752\n",
      "Trained batch 227 batch loss 1.1859355 epoch total loss 1.24728608\n",
      "Trained batch 228 batch loss 1.12807655 epoch total loss 1.24676323\n",
      "Trained batch 229 batch loss 1.22296524 epoch total loss 1.24665928\n",
      "Trained batch 230 batch loss 1.30741692 epoch total loss 1.24692345\n",
      "Trained batch 231 batch loss 1.34634399 epoch total loss 1.24735379\n",
      "Trained batch 232 batch loss 1.27627337 epoch total loss 1.24747849\n",
      "Trained batch 233 batch loss 1.28276014 epoch total loss 1.24762988\n",
      "Trained batch 234 batch loss 1.13877988 epoch total loss 1.24716473\n",
      "Trained batch 235 batch loss 1.1942904 epoch total loss 1.24693978\n",
      "Trained batch 236 batch loss 1.26187086 epoch total loss 1.24700308\n",
      "Trained batch 237 batch loss 1.22988474 epoch total loss 1.24693084\n",
      "Trained batch 238 batch loss 1.24388051 epoch total loss 1.24691796\n",
      "Trained batch 239 batch loss 1.31001079 epoch total loss 1.24718189\n",
      "Trained batch 240 batch loss 1.28864741 epoch total loss 1.24735463\n",
      "Trained batch 241 batch loss 1.22488141 epoch total loss 1.2472614\n",
      "Trained batch 242 batch loss 1.11127424 epoch total loss 1.24669945\n",
      "Trained batch 243 batch loss 1.10903883 epoch total loss 1.24613297\n",
      "Trained batch 244 batch loss 1.10955167 epoch total loss 1.24557316\n",
      "Trained batch 245 batch loss 1.16439199 epoch total loss 1.24524188\n",
      "Trained batch 246 batch loss 1.17951512 epoch total loss 1.24497461\n",
      "Trained batch 247 batch loss 1.15970969 epoch total loss 1.24462938\n",
      "Trained batch 248 batch loss 1.03301907 epoch total loss 1.24377608\n",
      "Trained batch 249 batch loss 1.18071413 epoch total loss 1.24352288\n",
      "Trained batch 250 batch loss 1.17698288 epoch total loss 1.24325669\n",
      "Trained batch 251 batch loss 1.18294823 epoch total loss 1.24301648\n",
      "Trained batch 252 batch loss 1.12978101 epoch total loss 1.24256718\n",
      "Trained batch 253 batch loss 1.32727718 epoch total loss 1.24290192\n",
      "Trained batch 254 batch loss 1.35506368 epoch total loss 1.24334359\n",
      "Trained batch 255 batch loss 1.42141032 epoch total loss 1.24404192\n",
      "Trained batch 256 batch loss 1.16422617 epoch total loss 1.24373007\n",
      "Trained batch 257 batch loss 1.25433922 epoch total loss 1.24377131\n",
      "Trained batch 258 batch loss 1.28957474 epoch total loss 1.24394894\n",
      "Trained batch 259 batch loss 1.23996234 epoch total loss 1.24393344\n",
      "Trained batch 260 batch loss 1.2166934 epoch total loss 1.24382877\n",
      "Trained batch 261 batch loss 1.32566464 epoch total loss 1.24414229\n",
      "Trained batch 262 batch loss 1.29706335 epoch total loss 1.24434423\n",
      "Trained batch 263 batch loss 1.16988134 epoch total loss 1.24406111\n",
      "Trained batch 264 batch loss 1.0181632 epoch total loss 1.24320543\n",
      "Trained batch 265 batch loss 1.30063069 epoch total loss 1.24342215\n",
      "Trained batch 266 batch loss 1.3277477 epoch total loss 1.24373925\n",
      "Trained batch 267 batch loss 1.32368171 epoch total loss 1.24403858\n",
      "Trained batch 268 batch loss 1.34916449 epoch total loss 1.24443078\n",
      "Trained batch 269 batch loss 1.19990909 epoch total loss 1.24426532\n",
      "Trained batch 270 batch loss 1.29607797 epoch total loss 1.24445724\n",
      "Trained batch 271 batch loss 1.24558806 epoch total loss 1.2444613\n",
      "Trained batch 272 batch loss 1.25776911 epoch total loss 1.24451029\n",
      "Trained batch 273 batch loss 1.24592161 epoch total loss 1.24451542\n",
      "Trained batch 274 batch loss 1.27070212 epoch total loss 1.24461102\n",
      "Trained batch 275 batch loss 1.2115587 epoch total loss 1.24449074\n",
      "Trained batch 276 batch loss 1.30565608 epoch total loss 1.24471235\n",
      "Trained batch 277 batch loss 1.23767865 epoch total loss 1.24468696\n",
      "Trained batch 278 batch loss 1.26595783 epoch total loss 1.24476349\n",
      "Trained batch 279 batch loss 1.27447891 epoch total loss 1.24487\n",
      "Trained batch 280 batch loss 1.29091954 epoch total loss 1.24503446\n",
      "Trained batch 281 batch loss 1.22530425 epoch total loss 1.24496424\n",
      "Trained batch 282 batch loss 1.25690889 epoch total loss 1.24500656\n",
      "Trained batch 283 batch loss 1.32514083 epoch total loss 1.24528968\n",
      "Trained batch 284 batch loss 1.27135015 epoch total loss 1.24538147\n",
      "Trained batch 285 batch loss 1.16592014 epoch total loss 1.24510276\n",
      "Trained batch 286 batch loss 1.16117096 epoch total loss 1.24480927\n",
      "Trained batch 287 batch loss 1.12268472 epoch total loss 1.24438369\n",
      "Trained batch 288 batch loss 1.04065847 epoch total loss 1.2436763\n",
      "Trained batch 289 batch loss 1.28498566 epoch total loss 1.24381924\n",
      "Trained batch 290 batch loss 1.3839252 epoch total loss 1.24430227\n",
      "Trained batch 291 batch loss 1.52403951 epoch total loss 1.24526358\n",
      "Trained batch 292 batch loss 1.48429823 epoch total loss 1.24608219\n",
      "Trained batch 293 batch loss 1.26982653 epoch total loss 1.24616325\n",
      "Trained batch 294 batch loss 1.40308952 epoch total loss 1.24669695\n",
      "Trained batch 295 batch loss 1.29516709 epoch total loss 1.24686122\n",
      "Trained batch 296 batch loss 1.09471869 epoch total loss 1.24634731\n",
      "Trained batch 297 batch loss 1.06488276 epoch total loss 1.24573624\n",
      "Trained batch 298 batch loss 1.38770676 epoch total loss 1.2462126\n",
      "Trained batch 299 batch loss 1.2449398 epoch total loss 1.24620831\n",
      "Trained batch 300 batch loss 1.27069092 epoch total loss 1.24629\n",
      "Trained batch 301 batch loss 1.31399286 epoch total loss 1.24651492\n",
      "Trained batch 302 batch loss 1.18129218 epoch total loss 1.24629903\n",
      "Trained batch 303 batch loss 1.29303217 epoch total loss 1.24645317\n",
      "Trained batch 304 batch loss 1.22889256 epoch total loss 1.24639547\n",
      "Trained batch 305 batch loss 1.3511517 epoch total loss 1.24673891\n",
      "Trained batch 306 batch loss 1.24328554 epoch total loss 1.24672759\n",
      "Trained batch 307 batch loss 1.18903399 epoch total loss 1.24653971\n",
      "Trained batch 308 batch loss 1.14186978 epoch total loss 1.24619985\n",
      "Trained batch 309 batch loss 1.21413088 epoch total loss 1.24609613\n",
      "Trained batch 310 batch loss 1.18145514 epoch total loss 1.24588764\n",
      "Trained batch 311 batch loss 1.24957216 epoch total loss 1.24589944\n",
      "Trained batch 312 batch loss 1.26117575 epoch total loss 1.24594843\n",
      "Trained batch 313 batch loss 1.25974369 epoch total loss 1.24599242\n",
      "Trained batch 314 batch loss 1.38387024 epoch total loss 1.24643159\n",
      "Trained batch 315 batch loss 1.34859896 epoch total loss 1.24675596\n",
      "Trained batch 316 batch loss 1.30008662 epoch total loss 1.24692464\n",
      "Trained batch 317 batch loss 1.32932734 epoch total loss 1.24718463\n",
      "Trained batch 318 batch loss 1.32644892 epoch total loss 1.24743378\n",
      "Trained batch 319 batch loss 1.25731778 epoch total loss 1.2474649\n",
      "Trained batch 320 batch loss 1.16297841 epoch total loss 1.24720073\n",
      "Trained batch 321 batch loss 1.16874599 epoch total loss 1.24695635\n",
      "Trained batch 322 batch loss 1.12732697 epoch total loss 1.24658477\n",
      "Trained batch 323 batch loss 1.09549594 epoch total loss 1.246117\n",
      "Trained batch 324 batch loss 1.21065485 epoch total loss 1.24600756\n",
      "Trained batch 325 batch loss 1.18571818 epoch total loss 1.24582207\n",
      "Trained batch 326 batch loss 1.26773477 epoch total loss 1.24588931\n",
      "Trained batch 327 batch loss 1.23681688 epoch total loss 1.24586153\n",
      "Trained batch 328 batch loss 1.13523245 epoch total loss 1.24552429\n",
      "Trained batch 329 batch loss 1.03282583 epoch total loss 1.24487782\n",
      "Trained batch 330 batch loss 1.01573169 epoch total loss 1.2441833\n",
      "Trained batch 331 batch loss 1.23632956 epoch total loss 1.24415958\n",
      "Trained batch 332 batch loss 1.42079544 epoch total loss 1.24469173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 333 batch loss 1.38974476 epoch total loss 1.24512732\n",
      "Trained batch 334 batch loss 1.1978277 epoch total loss 1.24498558\n",
      "Trained batch 335 batch loss 1.17684436 epoch total loss 1.24478221\n",
      "Trained batch 336 batch loss 1.21170485 epoch total loss 1.24468374\n",
      "Trained batch 337 batch loss 1.17628133 epoch total loss 1.24448073\n",
      "Trained batch 338 batch loss 1.14620161 epoch total loss 1.24419\n",
      "Trained batch 339 batch loss 1.28670549 epoch total loss 1.24431551\n",
      "Trained batch 340 batch loss 1.14594483 epoch total loss 1.24402606\n",
      "Trained batch 341 batch loss 1.3428899 epoch total loss 1.24431598\n",
      "Trained batch 342 batch loss 1.22987628 epoch total loss 1.2442739\n",
      "Trained batch 343 batch loss 1.35070658 epoch total loss 1.2445842\n",
      "Trained batch 344 batch loss 1.16529477 epoch total loss 1.24435365\n",
      "Trained batch 345 batch loss 1.18530202 epoch total loss 1.24418247\n",
      "Trained batch 346 batch loss 1.12714911 epoch total loss 1.24384415\n",
      "Trained batch 347 batch loss 1.2410953 epoch total loss 1.24383628\n",
      "Trained batch 348 batch loss 1.25863194 epoch total loss 1.24387872\n",
      "Trained batch 349 batch loss 1.27125239 epoch total loss 1.24395716\n",
      "Trained batch 350 batch loss 1.24686456 epoch total loss 1.24396551\n",
      "Trained batch 351 batch loss 1.28564346 epoch total loss 1.24408424\n",
      "Trained batch 352 batch loss 1.32981193 epoch total loss 1.24432766\n",
      "Trained batch 353 batch loss 1.23733068 epoch total loss 1.24430788\n",
      "Trained batch 354 batch loss 1.11186171 epoch total loss 1.24393368\n",
      "Trained batch 355 batch loss 1.2127471 epoch total loss 1.24384582\n",
      "Trained batch 356 batch loss 1.11572611 epoch total loss 1.24348593\n",
      "Trained batch 357 batch loss 1.07486737 epoch total loss 1.24301362\n",
      "Trained batch 358 batch loss 1.18437982 epoch total loss 1.24284983\n",
      "Trained batch 359 batch loss 1.1480335 epoch total loss 1.24258578\n",
      "Trained batch 360 batch loss 1.22693908 epoch total loss 1.24254227\n",
      "Trained batch 361 batch loss 1.13347638 epoch total loss 1.24224019\n",
      "Trained batch 362 batch loss 1.15803838 epoch total loss 1.24200761\n",
      "Trained batch 363 batch loss 1.21825862 epoch total loss 1.24194217\n",
      "Trained batch 364 batch loss 1.11466932 epoch total loss 1.24159253\n",
      "Trained batch 365 batch loss 1.2692349 epoch total loss 1.24166822\n",
      "Trained batch 366 batch loss 1.3031323 epoch total loss 1.24183619\n",
      "Trained batch 367 batch loss 1.23811388 epoch total loss 1.24182606\n",
      "Trained batch 368 batch loss 1.19185615 epoch total loss 1.24169028\n",
      "Trained batch 369 batch loss 1.13567436 epoch total loss 1.24140298\n",
      "Trained batch 370 batch loss 1.16024387 epoch total loss 1.24118364\n",
      "Trained batch 371 batch loss 1.13923609 epoch total loss 1.24090886\n",
      "Trained batch 372 batch loss 1.12758398 epoch total loss 1.24060416\n",
      "Trained batch 373 batch loss 1.34729254 epoch total loss 1.24089026\n",
      "Trained batch 374 batch loss 1.16367579 epoch total loss 1.24068367\n",
      "Trained batch 375 batch loss 1.32810819 epoch total loss 1.24091685\n",
      "Trained batch 376 batch loss 1.35903227 epoch total loss 1.24123096\n",
      "Trained batch 377 batch loss 1.22922373 epoch total loss 1.24119914\n",
      "Trained batch 378 batch loss 1.24731827 epoch total loss 1.24121535\n",
      "Trained batch 379 batch loss 1.35129988 epoch total loss 1.24150574\n",
      "Trained batch 380 batch loss 1.31065059 epoch total loss 1.24168766\n",
      "Trained batch 381 batch loss 1.1747669 epoch total loss 1.24151206\n",
      "Trained batch 382 batch loss 1.30993211 epoch total loss 1.24169111\n",
      "Trained batch 383 batch loss 1.33025992 epoch total loss 1.24192238\n",
      "Trained batch 384 batch loss 1.30321455 epoch total loss 1.242082\n",
      "Trained batch 385 batch loss 1.21522295 epoch total loss 1.24201226\n",
      "Trained batch 386 batch loss 1.26826048 epoch total loss 1.24208021\n",
      "Trained batch 387 batch loss 1.41177702 epoch total loss 1.24251866\n",
      "Trained batch 388 batch loss 1.46426249 epoch total loss 1.24309015\n",
      "Trained batch 389 batch loss 1.23980641 epoch total loss 1.24308181\n",
      "Trained batch 390 batch loss 1.27430046 epoch total loss 1.2431618\n",
      "Trained batch 391 batch loss 1.28825796 epoch total loss 1.24327719\n",
      "Trained batch 392 batch loss 1.1807518 epoch total loss 1.24311769\n",
      "Trained batch 393 batch loss 1.19608259 epoch total loss 1.242998\n",
      "Trained batch 394 batch loss 1.30097246 epoch total loss 1.24314511\n",
      "Trained batch 395 batch loss 1.20513272 epoch total loss 1.24304891\n",
      "Trained batch 396 batch loss 1.12003529 epoch total loss 1.24273825\n",
      "Trained batch 397 batch loss 1.15852284 epoch total loss 1.24252605\n",
      "Trained batch 398 batch loss 1.16766858 epoch total loss 1.24233794\n",
      "Trained batch 399 batch loss 1.12594163 epoch total loss 1.24204624\n",
      "Trained batch 400 batch loss 1.0191133 epoch total loss 1.24148893\n",
      "Trained batch 401 batch loss 1.07162678 epoch total loss 1.24106526\n",
      "Trained batch 402 batch loss 1.15073645 epoch total loss 1.24084055\n",
      "Trained batch 403 batch loss 1.10337949 epoch total loss 1.2404995\n",
      "Trained batch 404 batch loss 1.14342189 epoch total loss 1.24025917\n",
      "Trained batch 405 batch loss 1.20585716 epoch total loss 1.24017429\n",
      "Trained batch 406 batch loss 1.13773048 epoch total loss 1.23992193\n",
      "Trained batch 407 batch loss 1.05759895 epoch total loss 1.23947394\n",
      "Trained batch 408 batch loss 1.42830598 epoch total loss 1.23993683\n",
      "Trained batch 409 batch loss 1.40411735 epoch total loss 1.24033821\n",
      "Trained batch 410 batch loss 1.25141478 epoch total loss 1.24036527\n",
      "Trained batch 411 batch loss 1.28394198 epoch total loss 1.24047124\n",
      "Trained batch 412 batch loss 1.21540737 epoch total loss 1.24041033\n",
      "Trained batch 413 batch loss 1.14285707 epoch total loss 1.24017417\n",
      "Trained batch 414 batch loss 1.08167195 epoch total loss 1.23979139\n",
      "Trained batch 415 batch loss 1.15333796 epoch total loss 1.23958302\n",
      "Trained batch 416 batch loss 1.18849742 epoch total loss 1.23946011\n",
      "Trained batch 417 batch loss 1.10298693 epoch total loss 1.23913276\n",
      "Trained batch 418 batch loss 1.27446818 epoch total loss 1.2392174\n",
      "Trained batch 419 batch loss 1.32691 epoch total loss 1.23942661\n",
      "Trained batch 420 batch loss 1.32511961 epoch total loss 1.2396307\n",
      "Trained batch 421 batch loss 1.40280581 epoch total loss 1.24001837\n",
      "Trained batch 422 batch loss 1.27020431 epoch total loss 1.24008989\n",
      "Trained batch 423 batch loss 1.21159017 epoch total loss 1.24002254\n",
      "Trained batch 424 batch loss 1.05244315 epoch total loss 1.23958015\n",
      "Trained batch 425 batch loss 1.13014674 epoch total loss 1.23932254\n",
      "Trained batch 426 batch loss 1.20791888 epoch total loss 1.23924887\n",
      "Trained batch 427 batch loss 1.30505311 epoch total loss 1.23940301\n",
      "Trained batch 428 batch loss 1.33237553 epoch total loss 1.23962033\n",
      "Trained batch 429 batch loss 1.32553589 epoch total loss 1.2398206\n",
      "Trained batch 430 batch loss 1.25283408 epoch total loss 1.23985088\n",
      "Trained batch 431 batch loss 1.22888649 epoch total loss 1.23982537\n",
      "Trained batch 432 batch loss 1.28741968 epoch total loss 1.23993552\n",
      "Trained batch 433 batch loss 1.11988556 epoch total loss 1.23965824\n",
      "Trained batch 434 batch loss 1.14959145 epoch total loss 1.23945081\n",
      "Trained batch 435 batch loss 1.10447574 epoch total loss 1.23914051\n",
      "Trained batch 436 batch loss 1.22000897 epoch total loss 1.23909664\n",
      "Trained batch 437 batch loss 1.26210761 epoch total loss 1.23914933\n",
      "Trained batch 438 batch loss 1.13341153 epoch total loss 1.23890793\n",
      "Trained batch 439 batch loss 1.32459617 epoch total loss 1.23910308\n",
      "Trained batch 440 batch loss 1.22795486 epoch total loss 1.23907781\n",
      "Trained batch 441 batch loss 1.20422769 epoch total loss 1.23899877\n",
      "Trained batch 442 batch loss 1.27904356 epoch total loss 1.23908937\n",
      "Trained batch 443 batch loss 1.19873345 epoch total loss 1.23899829\n",
      "Trained batch 444 batch loss 1.2597338 epoch total loss 1.2390449\n",
      "Trained batch 445 batch loss 1.32231057 epoch total loss 1.23923206\n",
      "Trained batch 446 batch loss 1.23844779 epoch total loss 1.23923028\n",
      "Trained batch 447 batch loss 1.25458133 epoch total loss 1.23926461\n",
      "Trained batch 448 batch loss 1.1711055 epoch total loss 1.2391125\n",
      "Trained batch 449 batch loss 1.25684929 epoch total loss 1.23915195\n",
      "Trained batch 450 batch loss 1.37402225 epoch total loss 1.23945165\n",
      "Trained batch 451 batch loss 1.29927254 epoch total loss 1.23958421\n",
      "Trained batch 452 batch loss 1.2732172 epoch total loss 1.23965859\n",
      "Trained batch 453 batch loss 1.12469852 epoch total loss 1.2394048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 454 batch loss 1.12745214 epoch total loss 1.23915815\n",
      "Trained batch 455 batch loss 1.21699893 epoch total loss 1.2391094\n",
      "Trained batch 456 batch loss 1.27313757 epoch total loss 1.23918402\n",
      "Trained batch 457 batch loss 1.2713294 epoch total loss 1.23925436\n",
      "Trained batch 458 batch loss 1.33105934 epoch total loss 1.23945475\n",
      "Trained batch 459 batch loss 1.16362333 epoch total loss 1.23928964\n",
      "Trained batch 460 batch loss 1.35630536 epoch total loss 1.23954403\n",
      "Trained batch 461 batch loss 1.25222051 epoch total loss 1.23957145\n",
      "Trained batch 462 batch loss 1.24960434 epoch total loss 1.23959327\n",
      "Trained batch 463 batch loss 1.27490771 epoch total loss 1.23966956\n",
      "Trained batch 464 batch loss 1.40184617 epoch total loss 1.24001908\n",
      "Trained batch 465 batch loss 1.443434 epoch total loss 1.24045646\n",
      "Trained batch 466 batch loss 1.25401902 epoch total loss 1.24048555\n",
      "Trained batch 467 batch loss 1.21770382 epoch total loss 1.24043679\n",
      "Trained batch 468 batch loss 1.32111228 epoch total loss 1.24060917\n",
      "Trained batch 469 batch loss 1.30861568 epoch total loss 1.24075413\n",
      "Trained batch 470 batch loss 1.31344795 epoch total loss 1.24090886\n",
      "Trained batch 471 batch loss 1.25983596 epoch total loss 1.24094903\n",
      "Trained batch 472 batch loss 1.2213645 epoch total loss 1.24090755\n",
      "Trained batch 473 batch loss 1.30899596 epoch total loss 1.24105155\n",
      "Trained batch 474 batch loss 1.11740136 epoch total loss 1.24079072\n",
      "Trained batch 475 batch loss 1.39588964 epoch total loss 1.24111724\n",
      "Trained batch 476 batch loss 1.25929201 epoch total loss 1.24115539\n",
      "Trained batch 477 batch loss 1.15681374 epoch total loss 1.2409786\n",
      "Trained batch 478 batch loss 1.14853597 epoch total loss 1.24078524\n",
      "Trained batch 479 batch loss 1.22868085 epoch total loss 1.24076\n",
      "Trained batch 480 batch loss 1.15907717 epoch total loss 1.24058974\n",
      "Trained batch 481 batch loss 1.17970061 epoch total loss 1.24046314\n",
      "Trained batch 482 batch loss 1.28021526 epoch total loss 1.24054563\n",
      "Trained batch 483 batch loss 1.28025782 epoch total loss 1.24062788\n",
      "Trained batch 484 batch loss 1.28478336 epoch total loss 1.24071908\n",
      "Trained batch 485 batch loss 1.34066343 epoch total loss 1.24092519\n",
      "Trained batch 486 batch loss 1.29166436 epoch total loss 1.24102962\n",
      "Trained batch 487 batch loss 1.18357801 epoch total loss 1.2409116\n",
      "Trained batch 488 batch loss 1.32524729 epoch total loss 1.24108446\n",
      "Trained batch 489 batch loss 1.24409544 epoch total loss 1.24109066\n",
      "Trained batch 490 batch loss 1.19879675 epoch total loss 1.24100435\n",
      "Trained batch 491 batch loss 1.11769211 epoch total loss 1.24075305\n",
      "Trained batch 492 batch loss 1.27985489 epoch total loss 1.24083257\n",
      "Trained batch 493 batch loss 1.26926756 epoch total loss 1.24089026\n",
      "Trained batch 494 batch loss 1.27035892 epoch total loss 1.24095\n",
      "Trained batch 495 batch loss 1.40465641 epoch total loss 1.24128067\n",
      "Trained batch 496 batch loss 1.37139356 epoch total loss 1.24154305\n",
      "Trained batch 497 batch loss 1.08834743 epoch total loss 1.24123478\n",
      "Trained batch 498 batch loss 1.23354089 epoch total loss 1.24121928\n",
      "Trained batch 499 batch loss 1.24228442 epoch total loss 1.24122143\n",
      "Trained batch 500 batch loss 1.25287008 epoch total loss 1.24124479\n",
      "Trained batch 501 batch loss 1.28455222 epoch total loss 1.24133122\n",
      "Trained batch 502 batch loss 1.25212204 epoch total loss 1.24135268\n",
      "Trained batch 503 batch loss 1.1409359 epoch total loss 1.241153\n",
      "Trained batch 504 batch loss 1.2756176 epoch total loss 1.24122143\n",
      "Trained batch 505 batch loss 1.19784904 epoch total loss 1.2411356\n",
      "Trained batch 506 batch loss 1.17349112 epoch total loss 1.24100184\n",
      "Trained batch 507 batch loss 1.11577916 epoch total loss 1.24075496\n",
      "Trained batch 508 batch loss 1.38243139 epoch total loss 1.24103379\n",
      "Trained batch 509 batch loss 1.2776494 epoch total loss 1.24110579\n",
      "Trained batch 510 batch loss 1.39033473 epoch total loss 1.24139833\n",
      "Trained batch 511 batch loss 1.36775315 epoch total loss 1.24164557\n",
      "Trained batch 512 batch loss 1.22682607 epoch total loss 1.24161661\n",
      "Trained batch 513 batch loss 1.25256228 epoch total loss 1.24163795\n",
      "Trained batch 514 batch loss 1.32337666 epoch total loss 1.24179697\n",
      "Trained batch 515 batch loss 1.20924282 epoch total loss 1.24173367\n",
      "Trained batch 516 batch loss 1.32697678 epoch total loss 1.24189889\n",
      "Trained batch 517 batch loss 1.20831323 epoch total loss 1.24183393\n",
      "Trained batch 518 batch loss 1.15820312 epoch total loss 1.24167252\n",
      "Trained batch 519 batch loss 1.11015463 epoch total loss 1.24141908\n",
      "Trained batch 520 batch loss 1.04326069 epoch total loss 1.24103808\n",
      "Trained batch 521 batch loss 1.11521876 epoch total loss 1.24079657\n",
      "Trained batch 522 batch loss 0.932412863 epoch total loss 1.24020588\n",
      "Trained batch 523 batch loss 1.16064847 epoch total loss 1.24005377\n",
      "Trained batch 524 batch loss 1.19096696 epoch total loss 1.23996007\n",
      "Trained batch 525 batch loss 1.19894326 epoch total loss 1.23988187\n",
      "Trained batch 526 batch loss 1.20944965 epoch total loss 1.23982406\n",
      "Trained batch 527 batch loss 1.07207322 epoch total loss 1.23950577\n",
      "Trained batch 528 batch loss 1.20024478 epoch total loss 1.23943138\n",
      "Trained batch 529 batch loss 1.24766624 epoch total loss 1.239447\n",
      "Trained batch 530 batch loss 1.10365558 epoch total loss 1.23919082\n",
      "Trained batch 531 batch loss 1.21877134 epoch total loss 1.23915231\n",
      "Trained batch 532 batch loss 1.14341319 epoch total loss 1.23897243\n",
      "Trained batch 533 batch loss 1.10396218 epoch total loss 1.23871899\n",
      "Trained batch 534 batch loss 1.16362214 epoch total loss 1.23857844\n",
      "Trained batch 535 batch loss 1.26960742 epoch total loss 1.23863637\n",
      "Trained batch 536 batch loss 1.21110857 epoch total loss 1.23858511\n",
      "Trained batch 537 batch loss 1.36474872 epoch total loss 1.23882\n",
      "Trained batch 538 batch loss 1.45653141 epoch total loss 1.23922467\n",
      "Trained batch 539 batch loss 1.37057471 epoch total loss 1.23946834\n",
      "Trained batch 540 batch loss 1.1806494 epoch total loss 1.23935938\n",
      "Trained batch 541 batch loss 1.17259347 epoch total loss 1.239236\n",
      "Trained batch 542 batch loss 1.20208514 epoch total loss 1.23916745\n",
      "Trained batch 543 batch loss 1.26327491 epoch total loss 1.2392118\n",
      "Trained batch 544 batch loss 1.28408706 epoch total loss 1.23929429\n",
      "Trained batch 545 batch loss 1.09930527 epoch total loss 1.23903739\n",
      "Trained batch 546 batch loss 1.07155848 epoch total loss 1.23873067\n",
      "Trained batch 547 batch loss 1.07087564 epoch total loss 1.23842371\n",
      "Trained batch 548 batch loss 1.1774056 epoch total loss 1.23831248\n",
      "Trained batch 549 batch loss 1.18039942 epoch total loss 1.23820698\n",
      "Trained batch 550 batch loss 1.17197919 epoch total loss 1.23808658\n",
      "Trained batch 551 batch loss 1.15938175 epoch total loss 1.23794377\n",
      "Trained batch 552 batch loss 1.34851408 epoch total loss 1.23814404\n",
      "Trained batch 553 batch loss 1.23240197 epoch total loss 1.23813367\n",
      "Trained batch 554 batch loss 1.39721894 epoch total loss 1.23842084\n",
      "Trained batch 555 batch loss 1.39475238 epoch total loss 1.23870254\n",
      "Trained batch 556 batch loss 1.07167602 epoch total loss 1.23840213\n",
      "Trained batch 557 batch loss 1.15219355 epoch total loss 1.23824739\n",
      "Trained batch 558 batch loss 1.23548388 epoch total loss 1.23824239\n",
      "Trained batch 559 batch loss 1.24717045 epoch total loss 1.23825848\n",
      "Trained batch 560 batch loss 1.37882149 epoch total loss 1.23850954\n",
      "Trained batch 561 batch loss 1.31607151 epoch total loss 1.23864782\n",
      "Trained batch 562 batch loss 1.28135192 epoch total loss 1.23872375\n",
      "Trained batch 563 batch loss 1.17104053 epoch total loss 1.23860359\n",
      "Trained batch 564 batch loss 1.21521282 epoch total loss 1.23856211\n",
      "Trained batch 565 batch loss 1.17066944 epoch total loss 1.23844194\n",
      "Trained batch 566 batch loss 1.32281649 epoch total loss 1.23859096\n",
      "Trained batch 567 batch loss 1.34772086 epoch total loss 1.23878336\n",
      "Trained batch 568 batch loss 1.17455566 epoch total loss 1.23867035\n",
      "Trained batch 569 batch loss 1.17948782 epoch total loss 1.2385664\n",
      "Trained batch 570 batch loss 1.18800199 epoch total loss 1.23847759\n",
      "Trained batch 571 batch loss 1.17147124 epoch total loss 1.23836029\n",
      "Trained batch 572 batch loss 1.24389911 epoch total loss 1.23837\n",
      "Trained batch 573 batch loss 1.24593115 epoch total loss 1.23838305\n",
      "Trained batch 574 batch loss 1.21269321 epoch total loss 1.23833835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 575 batch loss 1.20575309 epoch total loss 1.23828173\n",
      "Trained batch 576 batch loss 1.2096827 epoch total loss 1.23823202\n",
      "Trained batch 577 batch loss 1.1225636 epoch total loss 1.23803151\n",
      "Trained batch 578 batch loss 1.14920962 epoch total loss 1.23787785\n",
      "Trained batch 579 batch loss 1.16774368 epoch total loss 1.23775673\n",
      "Trained batch 580 batch loss 1.21448112 epoch total loss 1.23771656\n",
      "Trained batch 581 batch loss 1.13876188 epoch total loss 1.23754621\n",
      "Trained batch 582 batch loss 1.23994875 epoch total loss 1.23755026\n",
      "Trained batch 583 batch loss 1.2420001 epoch total loss 1.23755789\n",
      "Trained batch 584 batch loss 1.16978502 epoch total loss 1.2374419\n",
      "Trained batch 585 batch loss 1.35313869 epoch total loss 1.23763967\n",
      "Trained batch 586 batch loss 1.38828874 epoch total loss 1.2378968\n",
      "Trained batch 587 batch loss 1.26933861 epoch total loss 1.23795044\n",
      "Trained batch 588 batch loss 1.16851521 epoch total loss 1.23783231\n",
      "Trained batch 589 batch loss 1.26973152 epoch total loss 1.23788643\n",
      "Trained batch 590 batch loss 1.33008599 epoch total loss 1.23804271\n",
      "Trained batch 591 batch loss 1.27016354 epoch total loss 1.23809695\n",
      "Trained batch 592 batch loss 1.2397908 epoch total loss 1.23809993\n",
      "Trained batch 593 batch loss 1.29127598 epoch total loss 1.23818958\n",
      "Trained batch 594 batch loss 1.39603031 epoch total loss 1.2384553\n",
      "Trained batch 595 batch loss 1.25272703 epoch total loss 1.23847938\n",
      "Trained batch 596 batch loss 1.18788922 epoch total loss 1.23839438\n",
      "Trained batch 597 batch loss 1.25330567 epoch total loss 1.23841941\n",
      "Trained batch 598 batch loss 1.35454738 epoch total loss 1.23861361\n",
      "Trained batch 599 batch loss 1.30446911 epoch total loss 1.23872352\n",
      "Trained batch 600 batch loss 1.16901147 epoch total loss 1.23860729\n",
      "Trained batch 601 batch loss 1.05918324 epoch total loss 1.23830879\n",
      "Trained batch 602 batch loss 1.14708221 epoch total loss 1.23815727\n",
      "Trained batch 603 batch loss 1.12838423 epoch total loss 1.23797512\n",
      "Trained batch 604 batch loss 1.13669741 epoch total loss 1.23780751\n",
      "Trained batch 605 batch loss 1.15827298 epoch total loss 1.23767602\n",
      "Trained batch 606 batch loss 1.06948566 epoch total loss 1.23739851\n",
      "Trained batch 607 batch loss 1.1523068 epoch total loss 1.2372582\n",
      "Trained batch 608 batch loss 1.14273369 epoch total loss 1.23710287\n",
      "Trained batch 609 batch loss 1.36628962 epoch total loss 1.23731494\n",
      "Trained batch 610 batch loss 1.20579576 epoch total loss 1.23726332\n",
      "Trained batch 611 batch loss 1.2126019 epoch total loss 1.23722291\n",
      "Trained batch 612 batch loss 1.37082517 epoch total loss 1.23744118\n",
      "Trained batch 613 batch loss 1.18686068 epoch total loss 1.23735881\n",
      "Trained batch 614 batch loss 1.12004566 epoch total loss 1.23716772\n",
      "Trained batch 615 batch loss 1.32152355 epoch total loss 1.23730493\n",
      "Trained batch 616 batch loss 1.21827602 epoch total loss 1.23727393\n",
      "Trained batch 617 batch loss 1.14395499 epoch total loss 1.23712277\n",
      "Trained batch 618 batch loss 1.15057266 epoch total loss 1.2369827\n",
      "Trained batch 619 batch loss 1.14676857 epoch total loss 1.23683703\n",
      "Trained batch 620 batch loss 1.31809795 epoch total loss 1.23696816\n",
      "Trained batch 621 batch loss 1.4159441 epoch total loss 1.23725629\n",
      "Trained batch 622 batch loss 1.44314098 epoch total loss 1.23758733\n",
      "Trained batch 623 batch loss 1.28499508 epoch total loss 1.23766339\n",
      "Trained batch 624 batch loss 1.33878613 epoch total loss 1.23782539\n",
      "Trained batch 625 batch loss 1.19423676 epoch total loss 1.23775566\n",
      "Trained batch 626 batch loss 1.16987872 epoch total loss 1.23764718\n",
      "Trained batch 627 batch loss 0.949177146 epoch total loss 1.23718715\n",
      "Trained batch 628 batch loss 1.02758265 epoch total loss 1.23685336\n",
      "Trained batch 629 batch loss 1.10658789 epoch total loss 1.23664618\n",
      "Trained batch 630 batch loss 1.19076025 epoch total loss 1.23657334\n",
      "Trained batch 631 batch loss 1.23096442 epoch total loss 1.2365644\n",
      "Trained batch 632 batch loss 1.23706055 epoch total loss 1.23656523\n",
      "Trained batch 633 batch loss 1.33648658 epoch total loss 1.23672307\n",
      "Trained batch 634 batch loss 1.3235724 epoch total loss 1.23686\n",
      "Trained batch 635 batch loss 1.28691626 epoch total loss 1.23693883\n",
      "Trained batch 636 batch loss 1.24981451 epoch total loss 1.2369591\n",
      "Trained batch 637 batch loss 1.17557561 epoch total loss 1.23686278\n",
      "Trained batch 638 batch loss 1.17036855 epoch total loss 1.23675847\n",
      "Trained batch 639 batch loss 1.20629478 epoch total loss 1.23671091\n",
      "Trained batch 640 batch loss 1.27061892 epoch total loss 1.23676383\n",
      "Trained batch 641 batch loss 1.33427739 epoch total loss 1.23691595\n",
      "Trained batch 642 batch loss 1.25983322 epoch total loss 1.23695171\n",
      "Trained batch 643 batch loss 1.26292849 epoch total loss 1.23699212\n",
      "Trained batch 644 batch loss 1.26444721 epoch total loss 1.2370348\n",
      "Trained batch 645 batch loss 1.21285605 epoch total loss 1.23699725\n",
      "Trained batch 646 batch loss 1.20072865 epoch total loss 1.2369411\n",
      "Trained batch 647 batch loss 1.12240028 epoch total loss 1.23676407\n",
      "Trained batch 648 batch loss 1.27157009 epoch total loss 1.23681772\n",
      "Trained batch 649 batch loss 1.29079437 epoch total loss 1.23690081\n",
      "Trained batch 650 batch loss 1.30277145 epoch total loss 1.23700225\n",
      "Trained batch 651 batch loss 1.14694309 epoch total loss 1.23686397\n",
      "Trained batch 652 batch loss 1.31700504 epoch total loss 1.23698688\n",
      "Trained batch 653 batch loss 1.34322238 epoch total loss 1.23714948\n",
      "Trained batch 654 batch loss 1.43483841 epoch total loss 1.23745179\n",
      "Trained batch 655 batch loss 1.39843988 epoch total loss 1.2376976\n",
      "Trained batch 656 batch loss 1.37762952 epoch total loss 1.23791087\n",
      "Trained batch 657 batch loss 1.29682755 epoch total loss 1.23800051\n",
      "Trained batch 658 batch loss 1.3552351 epoch total loss 1.23817861\n",
      "Trained batch 659 batch loss 1.32380295 epoch total loss 1.23830855\n",
      "Trained batch 660 batch loss 1.07160795 epoch total loss 1.23805594\n",
      "Trained batch 661 batch loss 1.22269678 epoch total loss 1.23803282\n",
      "Trained batch 662 batch loss 1.18592942 epoch total loss 1.23795402\n",
      "Trained batch 663 batch loss 1.38191843 epoch total loss 1.2381711\n",
      "Trained batch 664 batch loss 1.26057553 epoch total loss 1.23820484\n",
      "Trained batch 665 batch loss 1.19409847 epoch total loss 1.23813856\n",
      "Trained batch 666 batch loss 1.10421693 epoch total loss 1.23793745\n",
      "Trained batch 667 batch loss 1.14291835 epoch total loss 1.237795\n",
      "Trained batch 668 batch loss 1.12744772 epoch total loss 1.23762977\n",
      "Trained batch 669 batch loss 1.22880387 epoch total loss 1.23761666\n",
      "Trained batch 670 batch loss 1.143435 epoch total loss 1.23747599\n",
      "Trained batch 671 batch loss 1.21571422 epoch total loss 1.23744357\n",
      "Trained batch 672 batch loss 1.22497749 epoch total loss 1.23742509\n",
      "Trained batch 673 batch loss 1.39642394 epoch total loss 1.23766124\n",
      "Trained batch 674 batch loss 1.3620255 epoch total loss 1.23784578\n",
      "Trained batch 675 batch loss 1.24240792 epoch total loss 1.23785257\n",
      "Trained batch 676 batch loss 1.37806296 epoch total loss 1.23806\n",
      "Trained batch 677 batch loss 1.40487802 epoch total loss 1.2383064\n",
      "Trained batch 678 batch loss 1.36663389 epoch total loss 1.23849571\n",
      "Trained batch 679 batch loss 1.2473557 epoch total loss 1.23850882\n",
      "Trained batch 680 batch loss 1.1824944 epoch total loss 1.23842645\n",
      "Trained batch 681 batch loss 1.25176334 epoch total loss 1.238446\n",
      "Trained batch 682 batch loss 1.19111431 epoch total loss 1.23837662\n",
      "Trained batch 683 batch loss 1.3378067 epoch total loss 1.23852217\n",
      "Trained batch 684 batch loss 1.35914183 epoch total loss 1.23869848\n",
      "Trained batch 685 batch loss 1.3266902 epoch total loss 1.23882687\n",
      "Trained batch 686 batch loss 1.32784939 epoch total loss 1.23895669\n",
      "Trained batch 687 batch loss 1.26735699 epoch total loss 1.23899794\n",
      "Trained batch 688 batch loss 1.34994435 epoch total loss 1.23915911\n",
      "Trained batch 689 batch loss 1.11164188 epoch total loss 1.23897409\n",
      "Trained batch 690 batch loss 1.21375883 epoch total loss 1.2389375\n",
      "Trained batch 691 batch loss 1.13485265 epoch total loss 1.23878682\n",
      "Trained batch 692 batch loss 1.22255194 epoch total loss 1.23876333\n",
      "Trained batch 693 batch loss 1.3228128 epoch total loss 1.23888469\n",
      "Trained batch 694 batch loss 1.21767187 epoch total loss 1.23885405\n",
      "Trained batch 695 batch loss 1.2612381 epoch total loss 1.23888624\n",
      "Trained batch 696 batch loss 1.20934868 epoch total loss 1.2388438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 697 batch loss 1.27479553 epoch total loss 1.23889542\n",
      "Trained batch 698 batch loss 1.3316083 epoch total loss 1.23902822\n",
      "Trained batch 699 batch loss 1.14139807 epoch total loss 1.2388885\n",
      "Trained batch 700 batch loss 1.04985285 epoch total loss 1.23861849\n",
      "Trained batch 701 batch loss 1.09344411 epoch total loss 1.23841143\n",
      "Trained batch 702 batch loss 1.32475448 epoch total loss 1.23853445\n",
      "Trained batch 703 batch loss 1.31032336 epoch total loss 1.23863649\n",
      "Trained batch 704 batch loss 1.32443094 epoch total loss 1.23875833\n",
      "Trained batch 705 batch loss 1.38545418 epoch total loss 1.23896635\n",
      "Trained batch 706 batch loss 1.37605333 epoch total loss 1.23916054\n",
      "Trained batch 707 batch loss 1.28746855 epoch total loss 1.23922884\n",
      "Trained batch 708 batch loss 1.29914558 epoch total loss 1.23931348\n",
      "Trained batch 709 batch loss 1.24726605 epoch total loss 1.23932469\n",
      "Trained batch 710 batch loss 1.28940141 epoch total loss 1.23939526\n",
      "Trained batch 711 batch loss 1.32146108 epoch total loss 1.23951066\n",
      "Trained batch 712 batch loss 1.34497511 epoch total loss 1.23965883\n",
      "Trained batch 713 batch loss 1.33869815 epoch total loss 1.23979771\n",
      "Trained batch 714 batch loss 1.20610726 epoch total loss 1.2397505\n",
      "Trained batch 715 batch loss 1.27601242 epoch total loss 1.23980129\n",
      "Trained batch 716 batch loss 1.38981414 epoch total loss 1.24001074\n",
      "Trained batch 717 batch loss 1.141469 epoch total loss 1.23987341\n",
      "Trained batch 718 batch loss 1.21443188 epoch total loss 1.23983788\n",
      "Trained batch 719 batch loss 1.30999601 epoch total loss 1.23993552\n",
      "Trained batch 720 batch loss 1.38481939 epoch total loss 1.24013674\n",
      "Trained batch 721 batch loss 1.37718105 epoch total loss 1.24032676\n",
      "Trained batch 722 batch loss 1.41057014 epoch total loss 1.24056268\n",
      "Trained batch 723 batch loss 1.16791153 epoch total loss 1.24046218\n",
      "Trained batch 724 batch loss 1.26157451 epoch total loss 1.24049127\n",
      "Trained batch 725 batch loss 1.24389374 epoch total loss 1.24049604\n",
      "Trained batch 726 batch loss 1.27524686 epoch total loss 1.24054396\n",
      "Trained batch 727 batch loss 1.32818532 epoch total loss 1.24066448\n",
      "Trained batch 728 batch loss 1.28957438 epoch total loss 1.2407316\n",
      "Trained batch 729 batch loss 1.4069922 epoch total loss 1.24095964\n",
      "Trained batch 730 batch loss 1.33630776 epoch total loss 1.2410903\n",
      "Trained batch 731 batch loss 1.42446363 epoch total loss 1.24134111\n",
      "Trained batch 732 batch loss 1.38265204 epoch total loss 1.24153411\n",
      "Trained batch 733 batch loss 1.47645974 epoch total loss 1.24185455\n",
      "Trained batch 734 batch loss 1.40649688 epoch total loss 1.2420789\n",
      "Trained batch 735 batch loss 1.33722663 epoch total loss 1.24220836\n",
      "Trained batch 736 batch loss 1.36270702 epoch total loss 1.24237216\n",
      "Trained batch 737 batch loss 1.31455433 epoch total loss 1.24247\n",
      "Trained batch 738 batch loss 1.28312671 epoch total loss 1.24252522\n",
      "Trained batch 739 batch loss 1.42905593 epoch total loss 1.24277759\n",
      "Trained batch 740 batch loss 1.33680701 epoch total loss 1.24290466\n",
      "Trained batch 741 batch loss 1.34295905 epoch total loss 1.24303973\n",
      "Trained batch 742 batch loss 1.38535929 epoch total loss 1.24323153\n",
      "Trained batch 743 batch loss 1.34890103 epoch total loss 1.24337375\n",
      "Trained batch 744 batch loss 1.30960274 epoch total loss 1.2434628\n",
      "Trained batch 745 batch loss 1.24698281 epoch total loss 1.24346757\n",
      "Trained batch 746 batch loss 1.2946521 epoch total loss 1.24353611\n",
      "Trained batch 747 batch loss 1.26008284 epoch total loss 1.24355829\n",
      "Trained batch 748 batch loss 1.2600987 epoch total loss 1.24358034\n",
      "Trained batch 749 batch loss 1.26452303 epoch total loss 1.24360836\n",
      "Trained batch 750 batch loss 1.21758103 epoch total loss 1.24357367\n",
      "Trained batch 751 batch loss 1.21708906 epoch total loss 1.24353838\n",
      "Trained batch 752 batch loss 1.14283228 epoch total loss 1.24340451\n",
      "Trained batch 753 batch loss 1.09760702 epoch total loss 1.24321079\n",
      "Trained batch 754 batch loss 1.02367663 epoch total loss 1.24291968\n",
      "Trained batch 755 batch loss 1.15418863 epoch total loss 1.24280214\n",
      "Trained batch 756 batch loss 1.30977714 epoch total loss 1.24289072\n",
      "Trained batch 757 batch loss 1.33876789 epoch total loss 1.24301732\n",
      "Trained batch 758 batch loss 1.37848377 epoch total loss 1.24319601\n",
      "Trained batch 759 batch loss 1.37322927 epoch total loss 1.24336731\n",
      "Trained batch 760 batch loss 1.18954611 epoch total loss 1.24329662\n",
      "Trained batch 761 batch loss 1.25310874 epoch total loss 1.2433095\n",
      "Trained batch 762 batch loss 1.43090546 epoch total loss 1.24355567\n",
      "Trained batch 763 batch loss 1.19105601 epoch total loss 1.24348688\n",
      "Trained batch 764 batch loss 1.14175642 epoch total loss 1.24335372\n",
      "Trained batch 765 batch loss 1.07229042 epoch total loss 1.24313009\n",
      "Trained batch 766 batch loss 1.16413152 epoch total loss 1.24302697\n",
      "Trained batch 767 batch loss 1.26363778 epoch total loss 1.24305379\n",
      "Trained batch 768 batch loss 1.29618 epoch total loss 1.24312294\n",
      "Trained batch 769 batch loss 1.27652884 epoch total loss 1.24316645\n",
      "Trained batch 770 batch loss 1.24466169 epoch total loss 1.24316847\n",
      "Trained batch 771 batch loss 1.25236988 epoch total loss 1.24318039\n",
      "Trained batch 772 batch loss 1.20941663 epoch total loss 1.24313664\n",
      "Trained batch 773 batch loss 1.23246336 epoch total loss 1.24312282\n",
      "Trained batch 774 batch loss 1.30964792 epoch total loss 1.24320877\n",
      "Trained batch 775 batch loss 1.32473516 epoch total loss 1.24331391\n",
      "Trained batch 776 batch loss 1.3453058 epoch total loss 1.24344528\n",
      "Trained batch 777 batch loss 1.1860882 epoch total loss 1.24337149\n",
      "Trained batch 778 batch loss 1.14103413 epoch total loss 1.24324\n",
      "Trained batch 779 batch loss 1.09094048 epoch total loss 1.2430445\n",
      "Trained batch 780 batch loss 1.18410349 epoch total loss 1.24296892\n",
      "Trained batch 781 batch loss 1.28765619 epoch total loss 1.24302614\n",
      "Trained batch 782 batch loss 1.23924589 epoch total loss 1.24302125\n",
      "Trained batch 783 batch loss 1.23265958 epoch total loss 1.24300814\n",
      "Trained batch 784 batch loss 1.28348207 epoch total loss 1.24305975\n",
      "Trained batch 785 batch loss 1.26458132 epoch total loss 1.24308717\n",
      "Trained batch 786 batch loss 1.2566874 epoch total loss 1.24310446\n",
      "Trained batch 787 batch loss 1.27956104 epoch total loss 1.24315083\n",
      "Trained batch 788 batch loss 1.35744929 epoch total loss 1.24329579\n",
      "Trained batch 789 batch loss 1.31593835 epoch total loss 1.24338782\n",
      "Trained batch 790 batch loss 1.29499578 epoch total loss 1.24345315\n",
      "Trained batch 791 batch loss 1.36688769 epoch total loss 1.24360919\n",
      "Trained batch 792 batch loss 1.27599311 epoch total loss 1.24365008\n",
      "Trained batch 793 batch loss 1.28298485 epoch total loss 1.24369967\n",
      "Trained batch 794 batch loss 1.37587869 epoch total loss 1.24386609\n",
      "Trained batch 795 batch loss 1.25567937 epoch total loss 1.24388099\n",
      "Trained batch 796 batch loss 1.17888474 epoch total loss 1.24379933\n",
      "Trained batch 797 batch loss 1.30656314 epoch total loss 1.24387813\n",
      "Trained batch 798 batch loss 1.32535458 epoch total loss 1.24398029\n",
      "Trained batch 799 batch loss 1.22214019 epoch total loss 1.24395299\n",
      "Trained batch 800 batch loss 1.23820114 epoch total loss 1.24394572\n",
      "Trained batch 801 batch loss 1.15150309 epoch total loss 1.24383032\n",
      "Trained batch 802 batch loss 1.13012 epoch total loss 1.24368858\n",
      "Trained batch 803 batch loss 1.11779666 epoch total loss 1.24353182\n",
      "Trained batch 804 batch loss 1.09358919 epoch total loss 1.24334526\n",
      "Trained batch 805 batch loss 1.18100393 epoch total loss 1.24326789\n",
      "Trained batch 806 batch loss 1.12781763 epoch total loss 1.2431246\n",
      "Trained batch 807 batch loss 1.21947694 epoch total loss 1.24309528\n",
      "Trained batch 808 batch loss 1.21802032 epoch total loss 1.24306428\n",
      "Trained batch 809 batch loss 1.22005343 epoch total loss 1.24303579\n",
      "Trained batch 810 batch loss 1.13850236 epoch total loss 1.24290669\n",
      "Trained batch 811 batch loss 1.06946659 epoch total loss 1.24269283\n",
      "Trained batch 812 batch loss 0.995512426 epoch total loss 1.24238837\n",
      "Trained batch 813 batch loss 0.917679489 epoch total loss 1.24198902\n",
      "Trained batch 814 batch loss 1.2271626 epoch total loss 1.24197078\n",
      "Trained batch 815 batch loss 1.34146845 epoch total loss 1.24209297\n",
      "Trained batch 816 batch loss 1.15276062 epoch total loss 1.24198341\n",
      "Trained batch 817 batch loss 1.20342159 epoch total loss 1.24193621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 818 batch loss 1.24350786 epoch total loss 1.24193823\n",
      "Trained batch 819 batch loss 1.08691883 epoch total loss 1.24174893\n",
      "Trained batch 820 batch loss 1.11174667 epoch total loss 1.24159038\n",
      "Trained batch 821 batch loss 1.09734952 epoch total loss 1.24141467\n",
      "Trained batch 822 batch loss 1.18081093 epoch total loss 1.24134099\n",
      "Trained batch 823 batch loss 1.21186173 epoch total loss 1.24130511\n",
      "Trained batch 824 batch loss 1.32157254 epoch total loss 1.24140251\n",
      "Trained batch 825 batch loss 1.2829144 epoch total loss 1.24145281\n",
      "Trained batch 826 batch loss 1.2482785 epoch total loss 1.24146116\n",
      "Trained batch 827 batch loss 1.22155 epoch total loss 1.24143708\n",
      "Trained batch 828 batch loss 1.348441 epoch total loss 1.24156618\n",
      "Trained batch 829 batch loss 1.24489629 epoch total loss 1.24157023\n",
      "Trained batch 830 batch loss 1.34583843 epoch total loss 1.24169588\n",
      "Trained batch 831 batch loss 1.32145441 epoch total loss 1.24179173\n",
      "Trained batch 832 batch loss 1.21957421 epoch total loss 1.24176514\n",
      "Trained batch 833 batch loss 1.195099 epoch total loss 1.24170899\n",
      "Trained batch 834 batch loss 1.13452446 epoch total loss 1.24158049\n",
      "Trained batch 835 batch loss 1.29891181 epoch total loss 1.24164927\n",
      "Trained batch 836 batch loss 1.19992244 epoch total loss 1.24159932\n",
      "Trained batch 837 batch loss 1.22212291 epoch total loss 1.24157608\n",
      "Trained batch 838 batch loss 1.07604241 epoch total loss 1.24137855\n",
      "Trained batch 839 batch loss 1.15706706 epoch total loss 1.24127817\n",
      "Trained batch 840 batch loss 1.09488404 epoch total loss 1.24110389\n",
      "Trained batch 841 batch loss 1.1861279 epoch total loss 1.24103844\n",
      "Trained batch 842 batch loss 1.19509196 epoch total loss 1.24098396\n",
      "Trained batch 843 batch loss 1.25789 epoch total loss 1.24100399\n",
      "Trained batch 844 batch loss 1.26986384 epoch total loss 1.2410382\n",
      "Trained batch 845 batch loss 1.20496106 epoch total loss 1.24099553\n",
      "Trained batch 846 batch loss 1.24494457 epoch total loss 1.24100029\n",
      "Trained batch 847 batch loss 1.36941671 epoch total loss 1.24115181\n",
      "Trained batch 848 batch loss 1.28672814 epoch total loss 1.24120557\n",
      "Trained batch 849 batch loss 1.28896308 epoch total loss 1.24126184\n",
      "Trained batch 850 batch loss 1.29483867 epoch total loss 1.24132478\n",
      "Trained batch 851 batch loss 1.37343323 epoch total loss 1.24148\n",
      "Trained batch 852 batch loss 1.2560339 epoch total loss 1.24149704\n",
      "Trained batch 853 batch loss 1.32590437 epoch total loss 1.24159598\n",
      "Trained batch 854 batch loss 1.24544239 epoch total loss 1.24160063\n",
      "Trained batch 855 batch loss 1.30185747 epoch total loss 1.24167109\n",
      "Trained batch 856 batch loss 1.19782794 epoch total loss 1.24162\n",
      "Trained batch 857 batch loss 1.2931633 epoch total loss 1.24168015\n",
      "Trained batch 858 batch loss 1.36217225 epoch total loss 1.24182057\n",
      "Trained batch 859 batch loss 1.32248878 epoch total loss 1.24191451\n",
      "Trained batch 860 batch loss 1.3014 epoch total loss 1.24198365\n",
      "Trained batch 861 batch loss 1.19108427 epoch total loss 1.24192452\n",
      "Trained batch 862 batch loss 1.06717682 epoch total loss 1.24172175\n",
      "Trained batch 863 batch loss 1.20481408 epoch total loss 1.24167895\n",
      "Trained batch 864 batch loss 1.21311653 epoch total loss 1.24164593\n",
      "Trained batch 865 batch loss 1.12851942 epoch total loss 1.24151516\n",
      "Trained batch 866 batch loss 1.33597088 epoch total loss 1.24162424\n",
      "Trained batch 867 batch loss 1.28400683 epoch total loss 1.24167311\n",
      "Trained batch 868 batch loss 1.26062226 epoch total loss 1.24169505\n",
      "Trained batch 869 batch loss 1.22760081 epoch total loss 1.24167883\n",
      "Trained batch 870 batch loss 1.2154572 epoch total loss 1.24164867\n",
      "Trained batch 871 batch loss 1.11739886 epoch total loss 1.2415061\n",
      "Trained batch 872 batch loss 1.23933792 epoch total loss 1.2415036\n",
      "Trained batch 873 batch loss 1.16684222 epoch total loss 1.24141812\n",
      "Trained batch 874 batch loss 1.25121808 epoch total loss 1.24142933\n",
      "Trained batch 875 batch loss 1.12788868 epoch total loss 1.24129963\n",
      "Trained batch 876 batch loss 1.17495847 epoch total loss 1.24122393\n",
      "Trained batch 877 batch loss 1.1773324 epoch total loss 1.24115109\n",
      "Trained batch 878 batch loss 1.06952095 epoch total loss 1.24095571\n",
      "Trained batch 879 batch loss 1.13241518 epoch total loss 1.24083221\n",
      "Trained batch 880 batch loss 1.14152694 epoch total loss 1.24071932\n",
      "Trained batch 881 batch loss 1.11852515 epoch total loss 1.24058068\n",
      "Trained batch 882 batch loss 1.17110157 epoch total loss 1.24050188\n",
      "Trained batch 883 batch loss 1.19793034 epoch total loss 1.2404536\n",
      "Trained batch 884 batch loss 1.22607875 epoch total loss 1.24043739\n",
      "Trained batch 885 batch loss 1.32538271 epoch total loss 1.24053347\n",
      "Trained batch 886 batch loss 1.2356534 epoch total loss 1.24052787\n",
      "Trained batch 887 batch loss 1.29114246 epoch total loss 1.24058485\n",
      "Trained batch 888 batch loss 1.33405483 epoch total loss 1.24069023\n",
      "Trained batch 889 batch loss 1.21445477 epoch total loss 1.24066079\n",
      "Trained batch 890 batch loss 1.31544089 epoch total loss 1.24074471\n",
      "Trained batch 891 batch loss 1.18935919 epoch total loss 1.24068701\n",
      "Trained batch 892 batch loss 1.34011233 epoch total loss 1.24079847\n",
      "Trained batch 893 batch loss 1.21485889 epoch total loss 1.24076939\n",
      "Trained batch 894 batch loss 1.1760819 epoch total loss 1.24069703\n",
      "Trained batch 895 batch loss 1.07927537 epoch total loss 1.24051654\n",
      "Trained batch 896 batch loss 1.42072606 epoch total loss 1.24071777\n",
      "Trained batch 897 batch loss 1.34545243 epoch total loss 1.24083447\n",
      "Trained batch 898 batch loss 1.2525 epoch total loss 1.24084747\n",
      "Trained batch 899 batch loss 1.17613745 epoch total loss 1.24077547\n",
      "Trained batch 900 batch loss 1.04620337 epoch total loss 1.24055922\n",
      "Trained batch 901 batch loss 1.11238897 epoch total loss 1.240417\n",
      "Trained batch 902 batch loss 1.03970706 epoch total loss 1.24019444\n",
      "Trained batch 903 batch loss 1.08814025 epoch total loss 1.24002612\n",
      "Trained batch 904 batch loss 1.0469048 epoch total loss 1.23981237\n",
      "Trained batch 905 batch loss 1.14753675 epoch total loss 1.23971045\n",
      "Trained batch 906 batch loss 1.10286701 epoch total loss 1.23955953\n",
      "Trained batch 907 batch loss 1.14490986 epoch total loss 1.2394551\n",
      "Trained batch 908 batch loss 1.2723031 epoch total loss 1.23949134\n",
      "Trained batch 909 batch loss 1.29524159 epoch total loss 1.23955274\n",
      "Trained batch 910 batch loss 1.24704063 epoch total loss 1.23956096\n",
      "Trained batch 911 batch loss 1.22565603 epoch total loss 1.23954582\n",
      "Trained batch 912 batch loss 1.23555911 epoch total loss 1.23954141\n",
      "Trained batch 913 batch loss 1.16366959 epoch total loss 1.23945832\n",
      "Trained batch 914 batch loss 1.19969642 epoch total loss 1.23941493\n",
      "Trained batch 915 batch loss 1.24932051 epoch total loss 1.23942566\n",
      "Trained batch 916 batch loss 1.20779991 epoch total loss 1.23939109\n",
      "Trained batch 917 batch loss 1.17637825 epoch total loss 1.23932242\n",
      "Trained batch 918 batch loss 1.19864869 epoch total loss 1.23927808\n",
      "Trained batch 919 batch loss 1.23748183 epoch total loss 1.23927605\n",
      "Trained batch 920 batch loss 1.32594562 epoch total loss 1.23937023\n",
      "Trained batch 921 batch loss 1.23862195 epoch total loss 1.23936939\n",
      "Trained batch 922 batch loss 1.22068882 epoch total loss 1.23934913\n",
      "Trained batch 923 batch loss 1.1467036 epoch total loss 1.23924887\n",
      "Trained batch 924 batch loss 1.15043831 epoch total loss 1.23915267\n",
      "Trained batch 925 batch loss 1.25228441 epoch total loss 1.23916686\n",
      "Trained batch 926 batch loss 1.31956804 epoch total loss 1.23925376\n",
      "Trained batch 927 batch loss 1.34239137 epoch total loss 1.23936498\n",
      "Trained batch 928 batch loss 1.37153411 epoch total loss 1.23950744\n",
      "Trained batch 929 batch loss 1.28977346 epoch total loss 1.23956156\n",
      "Trained batch 930 batch loss 1.33147168 epoch total loss 1.23966038\n",
      "Trained batch 931 batch loss 1.32068944 epoch total loss 1.23974741\n",
      "Trained batch 932 batch loss 1.18467021 epoch total loss 1.23968828\n",
      "Trained batch 933 batch loss 1.25205219 epoch total loss 1.23970163\n",
      "Trained batch 934 batch loss 1.19984317 epoch total loss 1.23965895\n",
      "Trained batch 935 batch loss 1.19490469 epoch total loss 1.23961115\n",
      "Trained batch 936 batch loss 1.28844821 epoch total loss 1.23966324\n",
      "Trained batch 937 batch loss 1.24589729 epoch total loss 1.23966992\n",
      "Trained batch 938 batch loss 1.30597305 epoch total loss 1.23974061\n",
      "Trained batch 939 batch loss 1.19337523 epoch total loss 1.23969126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 940 batch loss 1.21480978 epoch total loss 1.23966479\n",
      "Trained batch 941 batch loss 1.46343791 epoch total loss 1.2399025\n",
      "Trained batch 942 batch loss 1.14251184 epoch total loss 1.23979914\n",
      "Trained batch 943 batch loss 1.02238059 epoch total loss 1.23956847\n",
      "Trained batch 944 batch loss 1.21630549 epoch total loss 1.2395438\n",
      "Trained batch 945 batch loss 1.15374 epoch total loss 1.23945296\n",
      "Trained batch 946 batch loss 1.16997254 epoch total loss 1.23937953\n",
      "Trained batch 947 batch loss 1.12653959 epoch total loss 1.23926044\n",
      "Trained batch 948 batch loss 1.28078926 epoch total loss 1.23930418\n",
      "Trained batch 949 batch loss 1.27740443 epoch total loss 1.23934424\n",
      "Trained batch 950 batch loss 1.26598644 epoch total loss 1.23937225\n",
      "Trained batch 951 batch loss 1.23355913 epoch total loss 1.23936617\n",
      "Trained batch 952 batch loss 1.36522412 epoch total loss 1.23949838\n",
      "Trained batch 953 batch loss 1.40422714 epoch total loss 1.23967111\n",
      "Trained batch 954 batch loss 1.3991214 epoch total loss 1.23983836\n",
      "Trained batch 955 batch loss 1.2803427 epoch total loss 1.2398808\n",
      "Trained batch 956 batch loss 1.26589656 epoch total loss 1.23990798\n",
      "Trained batch 957 batch loss 1.27560484 epoch total loss 1.23994529\n",
      "Trained batch 958 batch loss 1.24940908 epoch total loss 1.23995519\n",
      "Trained batch 959 batch loss 1.43026662 epoch total loss 1.24015367\n",
      "Trained batch 960 batch loss 1.42212975 epoch total loss 1.24034321\n",
      "Trained batch 961 batch loss 1.2681632 epoch total loss 1.24037218\n",
      "Trained batch 962 batch loss 1.24037385 epoch total loss 1.24037218\n",
      "Trained batch 963 batch loss 1.20333052 epoch total loss 1.2403338\n",
      "Trained batch 964 batch loss 1.14424813 epoch total loss 1.24023414\n",
      "Trained batch 965 batch loss 1.2857604 epoch total loss 1.24028134\n",
      "Trained batch 966 batch loss 1.34548426 epoch total loss 1.24039018\n",
      "Trained batch 967 batch loss 1.36830103 epoch total loss 1.2405225\n",
      "Trained batch 968 batch loss 1.5012269 epoch total loss 1.2407918\n",
      "Trained batch 969 batch loss 1.2642467 epoch total loss 1.240816\n",
      "Trained batch 970 batch loss 1.2257309 epoch total loss 1.24080038\n",
      "Trained batch 971 batch loss 1.36768532 epoch total loss 1.24093103\n",
      "Trained batch 972 batch loss 1.30237198 epoch total loss 1.24099433\n",
      "Trained batch 973 batch loss 1.36281514 epoch total loss 1.2411195\n",
      "Trained batch 974 batch loss 1.28761053 epoch total loss 1.24116719\n",
      "Trained batch 975 batch loss 1.0960412 epoch total loss 1.24101841\n",
      "Trained batch 976 batch loss 1.08342171 epoch total loss 1.24085689\n",
      "Trained batch 977 batch loss 1.03695786 epoch total loss 1.24064815\n",
      "Trained batch 978 batch loss 1.25806856 epoch total loss 1.24066603\n",
      "Trained batch 979 batch loss 1.22277737 epoch total loss 1.24064767\n",
      "Trained batch 980 batch loss 1.29744542 epoch total loss 1.24070573\n",
      "Trained batch 981 batch loss 1.25381589 epoch total loss 1.24071908\n",
      "Trained batch 982 batch loss 1.30371892 epoch total loss 1.24078321\n",
      "Trained batch 983 batch loss 1.22130609 epoch total loss 1.24076343\n",
      "Trained batch 984 batch loss 1.08361185 epoch total loss 1.24060369\n",
      "Trained batch 985 batch loss 1.07103109 epoch total loss 1.24043155\n",
      "Trained batch 986 batch loss 1.17534602 epoch total loss 1.24036551\n",
      "Trained batch 987 batch loss 1.19255328 epoch total loss 1.24031699\n",
      "Trained batch 988 batch loss 1.24438143 epoch total loss 1.24032116\n",
      "Trained batch 989 batch loss 1.26921058 epoch total loss 1.24035025\n",
      "Trained batch 990 batch loss 1.32572186 epoch total loss 1.24043643\n",
      "Trained batch 991 batch loss 1.31164718 epoch total loss 1.24050832\n",
      "Trained batch 992 batch loss 1.27576292 epoch total loss 1.24054384\n",
      "Trained batch 993 batch loss 1.2942934 epoch total loss 1.24059796\n",
      "Trained batch 994 batch loss 1.30614471 epoch total loss 1.24066401\n",
      "Trained batch 995 batch loss 1.33216703 epoch total loss 1.24075592\n",
      "Trained batch 996 batch loss 1.38633919 epoch total loss 1.24090207\n",
      "Trained batch 997 batch loss 1.31440902 epoch total loss 1.24097586\n",
      "Trained batch 998 batch loss 1.16794813 epoch total loss 1.24090266\n",
      "Trained batch 999 batch loss 1.11208868 epoch total loss 1.24077368\n",
      "Trained batch 1000 batch loss 1.13810515 epoch total loss 1.24067104\n",
      "Trained batch 1001 batch loss 1.22405553 epoch total loss 1.24065435\n",
      "Trained batch 1002 batch loss 1.23210645 epoch total loss 1.24064577\n",
      "Trained batch 1003 batch loss 1.22799325 epoch total loss 1.24063325\n",
      "Trained batch 1004 batch loss 1.34608936 epoch total loss 1.24073827\n",
      "Trained batch 1005 batch loss 1.33668637 epoch total loss 1.24083364\n",
      "Trained batch 1006 batch loss 1.3644563 epoch total loss 1.24095654\n",
      "Trained batch 1007 batch loss 1.3930645 epoch total loss 1.2411077\n",
      "Trained batch 1008 batch loss 1.37989652 epoch total loss 1.24124539\n",
      "Trained batch 1009 batch loss 1.1774174 epoch total loss 1.24118197\n",
      "Trained batch 1010 batch loss 1.11668289 epoch total loss 1.24105883\n",
      "Trained batch 1011 batch loss 1.34041154 epoch total loss 1.24115705\n",
      "Trained batch 1012 batch loss 1.26328635 epoch total loss 1.24117899\n",
      "Trained batch 1013 batch loss 1.16938496 epoch total loss 1.24110818\n",
      "Trained batch 1014 batch loss 1.16084 epoch total loss 1.24102902\n",
      "Trained batch 1015 batch loss 1.20239425 epoch total loss 1.240991\n",
      "Trained batch 1016 batch loss 1.13184261 epoch total loss 1.24088359\n",
      "Trained batch 1017 batch loss 1.12352824 epoch total loss 1.24076819\n",
      "Trained batch 1018 batch loss 1.27941144 epoch total loss 1.2408061\n",
      "Trained batch 1019 batch loss 1.18496549 epoch total loss 1.24075127\n",
      "Trained batch 1020 batch loss 1.29369402 epoch total loss 1.24080324\n",
      "Trained batch 1021 batch loss 1.26196659 epoch total loss 1.24082386\n",
      "Trained batch 1022 batch loss 1.22291863 epoch total loss 1.24080634\n",
      "Trained batch 1023 batch loss 1.22131991 epoch total loss 1.24078739\n",
      "Trained batch 1024 batch loss 1.30642629 epoch total loss 1.2408514\n",
      "Trained batch 1025 batch loss 1.37096334 epoch total loss 1.24097836\n",
      "Trained batch 1026 batch loss 1.18901825 epoch total loss 1.2409277\n",
      "Trained batch 1027 batch loss 1.37406111 epoch total loss 1.24105728\n",
      "Trained batch 1028 batch loss 1.32776237 epoch total loss 1.24114156\n",
      "Trained batch 1029 batch loss 1.34841156 epoch total loss 1.24124587\n",
      "Trained batch 1030 batch loss 1.20575118 epoch total loss 1.24121141\n",
      "Trained batch 1031 batch loss 1.25926399 epoch total loss 1.24122894\n",
      "Trained batch 1032 batch loss 1.22178936 epoch total loss 1.2412101\n",
      "Trained batch 1033 batch loss 1.15870523 epoch total loss 1.24113023\n",
      "Trained batch 1034 batch loss 1.26945937 epoch total loss 1.24115753\n",
      "Trained batch 1035 batch loss 1.26218081 epoch total loss 1.24117792\n",
      "Trained batch 1036 batch loss 1.28022695 epoch total loss 1.24121571\n",
      "Trained batch 1037 batch loss 1.21733868 epoch total loss 1.24119258\n",
      "Trained batch 1038 batch loss 1.16241336 epoch total loss 1.24111664\n",
      "Trained batch 1039 batch loss 1.21507049 epoch total loss 1.24109161\n",
      "Trained batch 1040 batch loss 1.17647159 epoch total loss 1.2410295\n",
      "Trained batch 1041 batch loss 1.14475024 epoch total loss 1.24093699\n",
      "Trained batch 1042 batch loss 1.00456178 epoch total loss 1.24071014\n",
      "Trained batch 1043 batch loss 0.943397939 epoch total loss 1.24042499\n",
      "Trained batch 1044 batch loss 1.13640749 epoch total loss 1.24032533\n",
      "Trained batch 1045 batch loss 1.39031661 epoch total loss 1.24046886\n",
      "Trained batch 1046 batch loss 1.53861272 epoch total loss 1.24075377\n",
      "Trained batch 1047 batch loss 1.45666361 epoch total loss 1.24096\n",
      "Trained batch 1048 batch loss 1.17913258 epoch total loss 1.24090099\n",
      "Trained batch 1049 batch loss 1.28092921 epoch total loss 1.24093914\n",
      "Trained batch 1050 batch loss 1.25357151 epoch total loss 1.24095106\n",
      "Trained batch 1051 batch loss 1.2707057 epoch total loss 1.24097943\n",
      "Trained batch 1052 batch loss 1.26189768 epoch total loss 1.24099934\n",
      "Trained batch 1053 batch loss 1.21909142 epoch total loss 1.24097848\n",
      "Trained batch 1054 batch loss 1.30501974 epoch total loss 1.24103928\n",
      "Trained batch 1055 batch loss 1.27556992 epoch total loss 1.24107194\n",
      "Trained batch 1056 batch loss 1.23778236 epoch total loss 1.24106884\n",
      "Trained batch 1057 batch loss 1.21110213 epoch total loss 1.24104047\n",
      "Trained batch 1058 batch loss 1.08220446 epoch total loss 1.24089026\n",
      "Trained batch 1059 batch loss 1.23157609 epoch total loss 1.24088144\n",
      "Trained batch 1060 batch loss 1.24286246 epoch total loss 1.24088347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1061 batch loss 1.25211287 epoch total loss 1.24089396\n",
      "Trained batch 1062 batch loss 0.991207123 epoch total loss 1.24065888\n",
      "Trained batch 1063 batch loss 1.21717882 epoch total loss 1.24063671\n",
      "Trained batch 1064 batch loss 1.3031981 epoch total loss 1.2406956\n",
      "Trained batch 1065 batch loss 1.01564336 epoch total loss 1.24048424\n",
      "Trained batch 1066 batch loss 1.04623723 epoch total loss 1.24030209\n",
      "Trained batch 1067 batch loss 0.986260414 epoch total loss 1.24006391\n",
      "Trained batch 1068 batch loss 1.14871454 epoch total loss 1.23997831\n",
      "Trained batch 1069 batch loss 1.33405876 epoch total loss 1.24006641\n",
      "Trained batch 1070 batch loss 1.28930795 epoch total loss 1.24011242\n",
      "Trained batch 1071 batch loss 1.32961822 epoch total loss 1.24019599\n",
      "Trained batch 1072 batch loss 1.22901201 epoch total loss 1.2401855\n",
      "Trained batch 1073 batch loss 1.15954244 epoch total loss 1.2401104\n",
      "Trained batch 1074 batch loss 1.07502973 epoch total loss 1.23995674\n",
      "Trained batch 1075 batch loss 1.31993139 epoch total loss 1.24003112\n",
      "Trained batch 1076 batch loss 1.21989322 epoch total loss 1.24001241\n",
      "Trained batch 1077 batch loss 1.23647523 epoch total loss 1.24000907\n",
      "Trained batch 1078 batch loss 1.27717423 epoch total loss 1.24004352\n",
      "Trained batch 1079 batch loss 1.35256147 epoch total loss 1.24014783\n",
      "Trained batch 1080 batch loss 1.40743351 epoch total loss 1.2403028\n",
      "Trained batch 1081 batch loss 1.26007235 epoch total loss 1.24032116\n",
      "Trained batch 1082 batch loss 1.23134875 epoch total loss 1.24031281\n",
      "Trained batch 1083 batch loss 1.11878181 epoch total loss 1.24020052\n",
      "Trained batch 1084 batch loss 1.18062055 epoch total loss 1.24014568\n",
      "Trained batch 1085 batch loss 1.361449 epoch total loss 1.24025738\n",
      "Trained batch 1086 batch loss 1.32175398 epoch total loss 1.24033248\n",
      "Trained batch 1087 batch loss 1.20028853 epoch total loss 1.24029565\n",
      "Trained batch 1088 batch loss 1.07015395 epoch total loss 1.24013937\n",
      "Trained batch 1089 batch loss 1.17931366 epoch total loss 1.24008346\n",
      "Trained batch 1090 batch loss 1.1810956 epoch total loss 1.24002945\n",
      "Trained batch 1091 batch loss 1.1168921 epoch total loss 1.23991656\n",
      "Trained batch 1092 batch loss 1.16363883 epoch total loss 1.23984683\n",
      "Trained batch 1093 batch loss 1.23886728 epoch total loss 1.23984599\n",
      "Trained batch 1094 batch loss 1.25297403 epoch total loss 1.23985791\n",
      "Trained batch 1095 batch loss 1.24123621 epoch total loss 1.2398591\n",
      "Trained batch 1096 batch loss 1.21731436 epoch total loss 1.23983848\n",
      "Trained batch 1097 batch loss 1.2986238 epoch total loss 1.23989213\n",
      "Trained batch 1098 batch loss 1.36260319 epoch total loss 1.24000382\n",
      "Trained batch 1099 batch loss 1.48190308 epoch total loss 1.24022388\n",
      "Trained batch 1100 batch loss 1.4761889 epoch total loss 1.24043846\n",
      "Trained batch 1101 batch loss 1.23955011 epoch total loss 1.24043763\n",
      "Trained batch 1102 batch loss 1.19383049 epoch total loss 1.24039531\n",
      "Trained batch 1103 batch loss 1.26099944 epoch total loss 1.24041402\n",
      "Trained batch 1104 batch loss 1.3671875 epoch total loss 1.24052882\n",
      "Trained batch 1105 batch loss 1.39862609 epoch total loss 1.24067199\n",
      "Trained batch 1106 batch loss 1.31167722 epoch total loss 1.24073613\n",
      "Trained batch 1107 batch loss 1.21759748 epoch total loss 1.24071527\n",
      "Trained batch 1108 batch loss 1.1557461 epoch total loss 1.24063861\n",
      "Trained batch 1109 batch loss 1.10683012 epoch total loss 1.24051797\n",
      "Trained batch 1110 batch loss 1.27941155 epoch total loss 1.24055302\n",
      "Trained batch 1111 batch loss 1.21115935 epoch total loss 1.24052656\n",
      "Trained batch 1112 batch loss 1.30746758 epoch total loss 1.24058676\n",
      "Trained batch 1113 batch loss 1.26194215 epoch total loss 1.24060595\n",
      "Trained batch 1114 batch loss 1.2483182 epoch total loss 1.24061286\n",
      "Trained batch 1115 batch loss 1.25667667 epoch total loss 1.24062729\n",
      "Trained batch 1116 batch loss 1.27069354 epoch total loss 1.24065435\n",
      "Trained batch 1117 batch loss 1.25904715 epoch total loss 1.2406708\n",
      "Trained batch 1118 batch loss 1.17735171 epoch total loss 1.24061418\n",
      "Trained batch 1119 batch loss 1.18192351 epoch total loss 1.2405616\n",
      "Trained batch 1120 batch loss 1.20073867 epoch total loss 1.24052608\n",
      "Trained batch 1121 batch loss 1.20819819 epoch total loss 1.24049723\n",
      "Trained batch 1122 batch loss 1.0971539 epoch total loss 1.24036956\n",
      "Trained batch 1123 batch loss 1.20735133 epoch total loss 1.24034011\n",
      "Trained batch 1124 batch loss 1.1914916 epoch total loss 1.24029672\n",
      "Trained batch 1125 batch loss 1.15212321 epoch total loss 1.24021828\n",
      "Trained batch 1126 batch loss 1.08345163 epoch total loss 1.24007916\n",
      "Trained batch 1127 batch loss 1.15317059 epoch total loss 1.24000204\n",
      "Trained batch 1128 batch loss 1.36312151 epoch total loss 1.24011123\n",
      "Trained batch 1129 batch loss 1.24864101 epoch total loss 1.24011874\n",
      "Trained batch 1130 batch loss 1.36999226 epoch total loss 1.24023378\n",
      "Trained batch 1131 batch loss 1.24737191 epoch total loss 1.24024\n",
      "Trained batch 1132 batch loss 1.32706261 epoch total loss 1.24031663\n",
      "Trained batch 1133 batch loss 1.35853839 epoch total loss 1.24042094\n",
      "Trained batch 1134 batch loss 1.28765309 epoch total loss 1.24046254\n",
      "Trained batch 1135 batch loss 1.14309502 epoch total loss 1.24037683\n",
      "Trained batch 1136 batch loss 1.15740466 epoch total loss 1.24030364\n",
      "Trained batch 1137 batch loss 1.02511609 epoch total loss 1.24011445\n",
      "Trained batch 1138 batch loss 1.081249 epoch total loss 1.23997486\n",
      "Trained batch 1139 batch loss 1.05547214 epoch total loss 1.23981285\n",
      "Trained batch 1140 batch loss 1.15896094 epoch total loss 1.23974192\n",
      "Trained batch 1141 batch loss 1.02055979 epoch total loss 1.23954976\n",
      "Trained batch 1142 batch loss 1.0055331 epoch total loss 1.23934484\n",
      "Trained batch 1143 batch loss 0.929231048 epoch total loss 1.23907351\n",
      "Trained batch 1144 batch loss 0.979875803 epoch total loss 1.2388469\n",
      "Trained batch 1145 batch loss 1.15060055 epoch total loss 1.23876989\n",
      "Trained batch 1146 batch loss 1.20796716 epoch total loss 1.23874307\n",
      "Trained batch 1147 batch loss 1.13175714 epoch total loss 1.23864973\n",
      "Trained batch 1148 batch loss 1.22431874 epoch total loss 1.23863721\n",
      "Trained batch 1149 batch loss 1.3026526 epoch total loss 1.23869288\n",
      "Trained batch 1150 batch loss 1.38332462 epoch total loss 1.23881865\n",
      "Trained batch 1151 batch loss 1.42210412 epoch total loss 1.23897791\n",
      "Trained batch 1152 batch loss 1.247679 epoch total loss 1.23898554\n",
      "Trained batch 1153 batch loss 1.15016901 epoch total loss 1.23890841\n",
      "Trained batch 1154 batch loss 1.25030351 epoch total loss 1.2389183\n",
      "Trained batch 1155 batch loss 1.20803678 epoch total loss 1.23889148\n",
      "Trained batch 1156 batch loss 1.26862299 epoch total loss 1.23891723\n",
      "Trained batch 1157 batch loss 1.12128258 epoch total loss 1.23881567\n",
      "Trained batch 1158 batch loss 1.01737094 epoch total loss 1.23862433\n",
      "Trained batch 1159 batch loss 1.10484648 epoch total loss 1.23850894\n",
      "Trained batch 1160 batch loss 1.13411784 epoch total loss 1.23841906\n",
      "Trained batch 1161 batch loss 1.11371791 epoch total loss 1.23831165\n",
      "Trained batch 1162 batch loss 1.05896759 epoch total loss 1.23815727\n",
      "Trained batch 1163 batch loss 1.10042381 epoch total loss 1.2380389\n",
      "Trained batch 1164 batch loss 1.23737884 epoch total loss 1.23803842\n",
      "Trained batch 1165 batch loss 1.28095222 epoch total loss 1.23807526\n",
      "Trained batch 1166 batch loss 1.05272245 epoch total loss 1.23791635\n",
      "Trained batch 1167 batch loss 1.20950246 epoch total loss 1.23789191\n",
      "Trained batch 1168 batch loss 1.22643352 epoch total loss 1.23788214\n",
      "Trained batch 1169 batch loss 1.32217467 epoch total loss 1.23795414\n",
      "Trained batch 1170 batch loss 1.24188244 epoch total loss 1.2379576\n",
      "Trained batch 1171 batch loss 1.24218071 epoch total loss 1.23796117\n",
      "Trained batch 1172 batch loss 1.20095193 epoch total loss 1.23792958\n",
      "Trained batch 1173 batch loss 1.19810224 epoch total loss 1.23789573\n",
      "Trained batch 1174 batch loss 1.15452468 epoch total loss 1.23782468\n",
      "Trained batch 1175 batch loss 1.12743115 epoch total loss 1.23773074\n",
      "Trained batch 1176 batch loss 1.30085158 epoch total loss 1.2377845\n",
      "Trained batch 1177 batch loss 1.26265025 epoch total loss 1.2378056\n",
      "Trained batch 1178 batch loss 1.42340899 epoch total loss 1.2379632\n",
      "Trained batch 1179 batch loss 1.44934142 epoch total loss 1.23814249\n",
      "Trained batch 1180 batch loss 1.34027457 epoch total loss 1.23822916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1181 batch loss 1.2516222 epoch total loss 1.23824048\n",
      "Trained batch 1182 batch loss 1.36627352 epoch total loss 1.23834884\n",
      "Trained batch 1183 batch loss 1.28848255 epoch total loss 1.23839116\n",
      "Trained batch 1184 batch loss 1.24751329 epoch total loss 1.23839891\n",
      "Trained batch 1185 batch loss 1.22209454 epoch total loss 1.23838508\n",
      "Trained batch 1186 batch loss 1.22793484 epoch total loss 1.23837626\n",
      "Trained batch 1187 batch loss 1.31746173 epoch total loss 1.2384429\n",
      "Trained batch 1188 batch loss 1.21908653 epoch total loss 1.23842669\n",
      "Trained batch 1189 batch loss 1.25439119 epoch total loss 1.23844\n",
      "Trained batch 1190 batch loss 1.16978836 epoch total loss 1.23838234\n",
      "Trained batch 1191 batch loss 1.16157532 epoch total loss 1.23831797\n",
      "Trained batch 1192 batch loss 1.14373338 epoch total loss 1.23823857\n",
      "Trained batch 1193 batch loss 1.22698951 epoch total loss 1.23822904\n",
      "Trained batch 1194 batch loss 1.22413766 epoch total loss 1.23821723\n",
      "Trained batch 1195 batch loss 1.24281955 epoch total loss 1.23822105\n",
      "Trained batch 1196 batch loss 1.28018641 epoch total loss 1.2382561\n",
      "Trained batch 1197 batch loss 1.26850605 epoch total loss 1.23828149\n",
      "Trained batch 1198 batch loss 1.19566309 epoch total loss 1.23824584\n",
      "Trained batch 1199 batch loss 1.20302451 epoch total loss 1.23821652\n",
      "Trained batch 1200 batch loss 1.06094265 epoch total loss 1.2380687\n",
      "Trained batch 1201 batch loss 1.14682126 epoch total loss 1.23799276\n",
      "Trained batch 1202 batch loss 1.14842856 epoch total loss 1.23791826\n",
      "Trained batch 1203 batch loss 1.25445235 epoch total loss 1.23793197\n",
      "Trained batch 1204 batch loss 1.2099098 epoch total loss 1.23790872\n",
      "Trained batch 1205 batch loss 1.17866719 epoch total loss 1.23785961\n",
      "Trained batch 1206 batch loss 1.07774901 epoch total loss 1.23772693\n",
      "Trained batch 1207 batch loss 1.23991728 epoch total loss 1.2377286\n",
      "Trained batch 1208 batch loss 1.38208151 epoch total loss 1.23784816\n",
      "Trained batch 1209 batch loss 1.15829098 epoch total loss 1.23778236\n",
      "Trained batch 1210 batch loss 1.26636815 epoch total loss 1.23780596\n",
      "Trained batch 1211 batch loss 1.10309303 epoch total loss 1.23769474\n",
      "Trained batch 1212 batch loss 1.01101983 epoch total loss 1.2375077\n",
      "Trained batch 1213 batch loss 0.85423404 epoch total loss 1.2371918\n",
      "Trained batch 1214 batch loss 0.954588413 epoch total loss 1.23695898\n",
      "Trained batch 1215 batch loss 1.30791628 epoch total loss 1.23701739\n",
      "Trained batch 1216 batch loss 1.25369179 epoch total loss 1.23703098\n",
      "Trained batch 1217 batch loss 1.23362553 epoch total loss 1.23702824\n",
      "Trained batch 1218 batch loss 1.22738111 epoch total loss 1.23702037\n",
      "Trained batch 1219 batch loss 1.16944671 epoch total loss 1.23696494\n",
      "Trained batch 1220 batch loss 1.12213326 epoch total loss 1.23687088\n",
      "Trained batch 1221 batch loss 1.19112098 epoch total loss 1.23683345\n",
      "Trained batch 1222 batch loss 1.25690842 epoch total loss 1.2368499\n",
      "Trained batch 1223 batch loss 1.17553651 epoch total loss 1.23679972\n",
      "Trained batch 1224 batch loss 1.17516351 epoch total loss 1.23674941\n",
      "Trained batch 1225 batch loss 1.25768209 epoch total loss 1.23676646\n",
      "Trained batch 1226 batch loss 1.20259452 epoch total loss 1.23673868\n",
      "Trained batch 1227 batch loss 1.19109821 epoch total loss 1.23670137\n",
      "Trained batch 1228 batch loss 1.22479939 epoch total loss 1.23669171\n",
      "Trained batch 1229 batch loss 1.18644035 epoch total loss 1.23665082\n",
      "Trained batch 1230 batch loss 1.30759311 epoch total loss 1.23670852\n",
      "Trained batch 1231 batch loss 1.20516217 epoch total loss 1.23668289\n",
      "Trained batch 1232 batch loss 1.22639954 epoch total loss 1.23667467\n",
      "Trained batch 1233 batch loss 1.21682346 epoch total loss 1.23665845\n",
      "Trained batch 1234 batch loss 1.04063797 epoch total loss 1.23649967\n",
      "Trained batch 1235 batch loss 1.12010741 epoch total loss 1.23640537\n",
      "Trained batch 1236 batch loss 1.34344292 epoch total loss 1.23649192\n",
      "Trained batch 1237 batch loss 1.30525 epoch total loss 1.23654759\n",
      "Trained batch 1238 batch loss 1.31748366 epoch total loss 1.23661304\n",
      "Trained batch 1239 batch loss 1.34936023 epoch total loss 1.23670399\n",
      "Trained batch 1240 batch loss 1.31132388 epoch total loss 1.23676407\n",
      "Trained batch 1241 batch loss 1.21720171 epoch total loss 1.23674834\n",
      "Trained batch 1242 batch loss 1.33262861 epoch total loss 1.23682559\n",
      "Trained batch 1243 batch loss 1.22659063 epoch total loss 1.23681724\n",
      "Trained batch 1244 batch loss 1.12734354 epoch total loss 1.23672926\n",
      "Trained batch 1245 batch loss 1.25966096 epoch total loss 1.23674762\n",
      "Trained batch 1246 batch loss 1.37927 epoch total loss 1.23686206\n",
      "Trained batch 1247 batch loss 1.41558671 epoch total loss 1.23700535\n",
      "Trained batch 1248 batch loss 1.31671524 epoch total loss 1.23706925\n",
      "Trained batch 1249 batch loss 1.17736888 epoch total loss 1.23702145\n",
      "Trained batch 1250 batch loss 1.17093968 epoch total loss 1.23696852\n",
      "Trained batch 1251 batch loss 1.32738316 epoch total loss 1.23704088\n",
      "Trained batch 1252 batch loss 1.20896077 epoch total loss 1.23701847\n",
      "Trained batch 1253 batch loss 1.39335895 epoch total loss 1.23714316\n",
      "Trained batch 1254 batch loss 1.3245405 epoch total loss 1.2372129\n",
      "Trained batch 1255 batch loss 1.25944591 epoch total loss 1.23723054\n",
      "Trained batch 1256 batch loss 1.1719842 epoch total loss 1.23717868\n",
      "Trained batch 1257 batch loss 1.06837952 epoch total loss 1.23704433\n",
      "Trained batch 1258 batch loss 1.10872364 epoch total loss 1.23694241\n",
      "Trained batch 1259 batch loss 1.17873037 epoch total loss 1.23689616\n",
      "Trained batch 1260 batch loss 1.0703094 epoch total loss 1.23676395\n",
      "Trained batch 1261 batch loss 1.05602312 epoch total loss 1.23662055\n",
      "Trained batch 1262 batch loss 1.06540322 epoch total loss 1.23648489\n",
      "Trained batch 1263 batch loss 1.17803645 epoch total loss 1.23643863\n",
      "Trained batch 1264 batch loss 1.28343654 epoch total loss 1.23647583\n",
      "Trained batch 1265 batch loss 1.16957867 epoch total loss 1.2364229\n",
      "Trained batch 1266 batch loss 1.26896 epoch total loss 1.23644853\n",
      "Trained batch 1267 batch loss 1.35414338 epoch total loss 1.23654139\n",
      "Trained batch 1268 batch loss 1.13468218 epoch total loss 1.23646104\n",
      "Trained batch 1269 batch loss 1.21752787 epoch total loss 1.23644614\n",
      "Trained batch 1270 batch loss 1.17688453 epoch total loss 1.23639929\n",
      "Trained batch 1271 batch loss 1.24876487 epoch total loss 1.23640895\n",
      "Trained batch 1272 batch loss 1.20203471 epoch total loss 1.23638201\n",
      "Trained batch 1273 batch loss 1.02038705 epoch total loss 1.23621225\n",
      "Trained batch 1274 batch loss 0.978731215 epoch total loss 1.23601019\n",
      "Trained batch 1275 batch loss 1.0115695 epoch total loss 1.23583424\n",
      "Trained batch 1276 batch loss 1.21286643 epoch total loss 1.23581624\n",
      "Trained batch 1277 batch loss 1.35127211 epoch total loss 1.23590672\n",
      "Trained batch 1278 batch loss 1.40276277 epoch total loss 1.23603714\n",
      "Trained batch 1279 batch loss 1.32110918 epoch total loss 1.23610377\n",
      "Trained batch 1280 batch loss 1.21111345 epoch total loss 1.23608422\n",
      "Trained batch 1281 batch loss 1.20384264 epoch total loss 1.23605907\n",
      "Trained batch 1282 batch loss 1.27814364 epoch total loss 1.23609185\n",
      "Trained batch 1283 batch loss 1.2835021 epoch total loss 1.23612881\n",
      "Trained batch 1284 batch loss 1.35849988 epoch total loss 1.23622417\n",
      "Trained batch 1285 batch loss 1.49508846 epoch total loss 1.23642564\n",
      "Trained batch 1286 batch loss 1.33799875 epoch total loss 1.23650455\n",
      "Trained batch 1287 batch loss 1.34768832 epoch total loss 1.23659098\n",
      "Trained batch 1288 batch loss 1.05398738 epoch total loss 1.23644912\n",
      "Trained batch 1289 batch loss 1.12728512 epoch total loss 1.23636448\n",
      "Trained batch 1290 batch loss 1.02711987 epoch total loss 1.23620224\n",
      "Trained batch 1291 batch loss 1.22616386 epoch total loss 1.23619449\n",
      "Trained batch 1292 batch loss 0.98488009 epoch total loss 1.236\n",
      "Trained batch 1293 batch loss 0.957528234 epoch total loss 1.23578465\n",
      "Trained batch 1294 batch loss 0.977211 epoch total loss 1.23558474\n",
      "Trained batch 1295 batch loss 1.07603312 epoch total loss 1.23546159\n",
      "Trained batch 1296 batch loss 1.07689071 epoch total loss 1.23533916\n",
      "Trained batch 1297 batch loss 1.08278167 epoch total loss 1.23522162\n",
      "Trained batch 1298 batch loss 1.18261242 epoch total loss 1.23518109\n",
      "Trained batch 1299 batch loss 1.25556374 epoch total loss 1.23519683\n",
      "Trained batch 1300 batch loss 1.26849508 epoch total loss 1.23522246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1301 batch loss 1.29728019 epoch total loss 1.23527014\n",
      "Trained batch 1302 batch loss 1.39156055 epoch total loss 1.23539019\n",
      "Trained batch 1303 batch loss 1.30684149 epoch total loss 1.23544502\n",
      "Trained batch 1304 batch loss 1.38333189 epoch total loss 1.23555839\n",
      "Trained batch 1305 batch loss 1.1551975 epoch total loss 1.23549688\n",
      "Trained batch 1306 batch loss 1.28559649 epoch total loss 1.23553526\n",
      "Trained batch 1307 batch loss 1.23379052 epoch total loss 1.23553383\n",
      "Trained batch 1308 batch loss 1.21729541 epoch total loss 1.23551989\n",
      "Trained batch 1309 batch loss 1.19698322 epoch total loss 1.23549056\n",
      "Trained batch 1310 batch loss 1.34378827 epoch total loss 1.23557317\n",
      "Trained batch 1311 batch loss 1.32342982 epoch total loss 1.23564017\n",
      "Trained batch 1312 batch loss 1.27785075 epoch total loss 1.23567235\n",
      "Trained batch 1313 batch loss 1.14744902 epoch total loss 1.23560524\n",
      "Trained batch 1314 batch loss 1.17929053 epoch total loss 1.23556232\n",
      "Trained batch 1315 batch loss 1.17084694 epoch total loss 1.23551321\n",
      "Trained batch 1316 batch loss 1.04993033 epoch total loss 1.23537219\n",
      "Trained batch 1317 batch loss 1.26663673 epoch total loss 1.23539591\n",
      "Trained batch 1318 batch loss 1.17070854 epoch total loss 1.23534679\n",
      "Trained batch 1319 batch loss 1.08861899 epoch total loss 1.23523545\n",
      "Trained batch 1320 batch loss 1.13474286 epoch total loss 1.2351594\n",
      "Trained batch 1321 batch loss 1.0998621 epoch total loss 1.235057\n",
      "Trained batch 1322 batch loss 1.25045812 epoch total loss 1.23506868\n",
      "Trained batch 1323 batch loss 1.22154117 epoch total loss 1.23505843\n",
      "Trained batch 1324 batch loss 1.13191557 epoch total loss 1.23498058\n",
      "Trained batch 1325 batch loss 1.21152484 epoch total loss 1.23496282\n",
      "Trained batch 1326 batch loss 1.31616545 epoch total loss 1.23502409\n",
      "Trained batch 1327 batch loss 1.27389991 epoch total loss 1.23505342\n",
      "Trained batch 1328 batch loss 1.22341323 epoch total loss 1.2350446\n",
      "Trained batch 1329 batch loss 1.22483373 epoch total loss 1.23503697\n",
      "Trained batch 1330 batch loss 1.21787202 epoch total loss 1.23502409\n",
      "Trained batch 1331 batch loss 1.16353333 epoch total loss 1.23497045\n",
      "Trained batch 1332 batch loss 1.20596707 epoch total loss 1.23494864\n",
      "Trained batch 1333 batch loss 1.19246793 epoch total loss 1.23491681\n",
      "Trained batch 1334 batch loss 1.09537077 epoch total loss 1.23481214\n",
      "Trained batch 1335 batch loss 1.19623435 epoch total loss 1.23478329\n",
      "Trained batch 1336 batch loss 1.13645613 epoch total loss 1.23470974\n",
      "Trained batch 1337 batch loss 1.32054067 epoch total loss 1.23477387\n",
      "Trained batch 1338 batch loss 1.15564334 epoch total loss 1.23471475\n",
      "Trained batch 1339 batch loss 1.18478906 epoch total loss 1.23467743\n",
      "Trained batch 1340 batch loss 1.17761528 epoch total loss 1.23463488\n",
      "Trained batch 1341 batch loss 0.983252108 epoch total loss 1.23444748\n",
      "Trained batch 1342 batch loss 1.00119138 epoch total loss 1.23427367\n",
      "Trained batch 1343 batch loss 0.9871 epoch total loss 1.23408961\n",
      "Trained batch 1344 batch loss 1.07607341 epoch total loss 1.23397195\n",
      "Trained batch 1345 batch loss 1.19197941 epoch total loss 1.23394084\n",
      "Trained batch 1346 batch loss 1.21592939 epoch total loss 1.23392749\n",
      "Trained batch 1347 batch loss 1.16114759 epoch total loss 1.23387337\n",
      "Trained batch 1348 batch loss 1.24476302 epoch total loss 1.23388147\n",
      "Trained batch 1349 batch loss 1.29403889 epoch total loss 1.23392606\n",
      "Trained batch 1350 batch loss 1.16989779 epoch total loss 1.23387861\n",
      "Trained batch 1351 batch loss 1.22166741 epoch total loss 1.23386967\n",
      "Trained batch 1352 batch loss 1.24223793 epoch total loss 1.23387575\n",
      "Trained batch 1353 batch loss 1.10754251 epoch total loss 1.23378241\n",
      "Trained batch 1354 batch loss 1.05579758 epoch total loss 1.23365092\n",
      "Trained batch 1355 batch loss 1.18159389 epoch total loss 1.23361254\n",
      "Trained batch 1356 batch loss 1.28198087 epoch total loss 1.2336483\n",
      "Trained batch 1357 batch loss 1.31117082 epoch total loss 1.2337054\n",
      "Trained batch 1358 batch loss 1.18961155 epoch total loss 1.23367286\n",
      "Trained batch 1359 batch loss 1.24722862 epoch total loss 1.23368287\n",
      "Trained batch 1360 batch loss 1.18578196 epoch total loss 1.23364758\n",
      "Trained batch 1361 batch loss 1.23490775 epoch total loss 1.23364854\n",
      "Trained batch 1362 batch loss 1.1914959 epoch total loss 1.23361754\n",
      "Trained batch 1363 batch loss 1.17532587 epoch total loss 1.23357475\n",
      "Trained batch 1364 batch loss 1.24551117 epoch total loss 1.23358357\n",
      "Trained batch 1365 batch loss 1.11334145 epoch total loss 1.23349535\n",
      "Trained batch 1366 batch loss 1.23885846 epoch total loss 1.23349929\n",
      "Trained batch 1367 batch loss 1.32554173 epoch total loss 1.23356664\n",
      "Trained batch 1368 batch loss 1.27882075 epoch total loss 1.23359978\n",
      "Trained batch 1369 batch loss 1.24225712 epoch total loss 1.2336061\n",
      "Trained batch 1370 batch loss 1.15597558 epoch total loss 1.23354948\n",
      "Trained batch 1371 batch loss 1.15539098 epoch total loss 1.23349249\n",
      "Trained batch 1372 batch loss 1.08408642 epoch total loss 1.23338354\n",
      "Trained batch 1373 batch loss 1.12550306 epoch total loss 1.23330498\n",
      "Trained batch 1374 batch loss 1.11850524 epoch total loss 1.23322141\n",
      "Trained batch 1375 batch loss 1.10302663 epoch total loss 1.23312676\n",
      "Trained batch 1376 batch loss 1.10645485 epoch total loss 1.23303473\n",
      "Trained batch 1377 batch loss 1.13268232 epoch total loss 1.23296189\n",
      "Trained batch 1378 batch loss 1.09784806 epoch total loss 1.23286378\n",
      "Trained batch 1379 batch loss 1.14824641 epoch total loss 1.23280239\n",
      "Trained batch 1380 batch loss 1.19463706 epoch total loss 1.23277473\n",
      "Trained batch 1381 batch loss 1.28082359 epoch total loss 1.23280954\n",
      "Validated batch 1 batch loss 1.21912467\n",
      "Validated batch 2 batch loss 1.12662709\n",
      "Validated batch 3 batch loss 1.2220825\n",
      "Validated batch 4 batch loss 1.0996964\n",
      "Validated batch 5 batch loss 1.18584549\n",
      "Validated batch 6 batch loss 1.21997857\n",
      "Validated batch 7 batch loss 1.22606206\n",
      "Validated batch 8 batch loss 1.3242631\n",
      "Validated batch 9 batch loss 1.29892397\n",
      "Validated batch 10 batch loss 1.18542969\n",
      "Validated batch 11 batch loss 1.26345801\n",
      "Validated batch 12 batch loss 1.3289597\n",
      "Validated batch 13 batch loss 1.32063961\n",
      "Validated batch 14 batch loss 1.3202126\n",
      "Validated batch 15 batch loss 1.27185678\n",
      "Validated batch 16 batch loss 1.33244526\n",
      "Validated batch 17 batch loss 1.26840758\n",
      "Validated batch 18 batch loss 1.11621308\n",
      "Validated batch 19 batch loss 1.19995713\n",
      "Validated batch 20 batch loss 1.3297199\n",
      "Validated batch 21 batch loss 1.23892021\n",
      "Validated batch 22 batch loss 1.23181462\n",
      "Validated batch 23 batch loss 1.17840648\n",
      "Validated batch 24 batch loss 1.21099246\n",
      "Validated batch 25 batch loss 1.10108852\n",
      "Validated batch 26 batch loss 1.15188348\n",
      "Validated batch 27 batch loss 1.14043117\n",
      "Validated batch 28 batch loss 1.19022989\n",
      "Validated batch 29 batch loss 1.21986389\n",
      "Validated batch 30 batch loss 1.2354393\n",
      "Validated batch 31 batch loss 1.1728667\n",
      "Validated batch 32 batch loss 1.21248269\n",
      "Validated batch 33 batch loss 1.19241452\n",
      "Validated batch 34 batch loss 1.19877517\n",
      "Validated batch 35 batch loss 1.21376753\n",
      "Validated batch 36 batch loss 1.13613904\n",
      "Validated batch 37 batch loss 1.16594803\n",
      "Validated batch 38 batch loss 1.20094013\n",
      "Validated batch 39 batch loss 1.18244839\n",
      "Validated batch 40 batch loss 1.32182634\n",
      "Validated batch 41 batch loss 1.27652836\n",
      "Validated batch 42 batch loss 1.07752311\n",
      "Validated batch 43 batch loss 1.29260385\n",
      "Validated batch 44 batch loss 1.16617441\n",
      "Validated batch 45 batch loss 1.07234752\n",
      "Validated batch 46 batch loss 1.23558378\n",
      "Validated batch 47 batch loss 1.25024784\n",
      "Validated batch 48 batch loss 1.19834352\n",
      "Validated batch 49 batch loss 1.13767529\n",
      "Validated batch 50 batch loss 1.12592518\n",
      "Validated batch 51 batch loss 1.19859242\n",
      "Validated batch 52 batch loss 1.23553824\n",
      "Validated batch 53 batch loss 1.19716191\n",
      "Validated batch 54 batch loss 1.18822956\n",
      "Validated batch 55 batch loss 1.20503926\n",
      "Validated batch 56 batch loss 1.25627053\n",
      "Validated batch 57 batch loss 1.10740089\n",
      "Validated batch 58 batch loss 1.07950807\n",
      "Validated batch 59 batch loss 1.30594444\n",
      "Validated batch 60 batch loss 1.25257695\n",
      "Validated batch 61 batch loss 1.37128735\n",
      "Validated batch 62 batch loss 1.34855616\n",
      "Validated batch 63 batch loss 1.17689049\n",
      "Validated batch 64 batch loss 1.38349605\n",
      "Validated batch 65 batch loss 1.17382884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 66 batch loss 1.28557396\n",
      "Validated batch 67 batch loss 1.24026763\n",
      "Validated batch 68 batch loss 0.972802401\n",
      "Validated batch 69 batch loss 1.1955142\n",
      "Validated batch 70 batch loss 1.20038414\n",
      "Validated batch 71 batch loss 1.25222945\n",
      "Validated batch 72 batch loss 1.149279\n",
      "Validated batch 73 batch loss 1.16201961\n",
      "Validated batch 74 batch loss 1.20483899\n",
      "Validated batch 75 batch loss 1.2832613\n",
      "Validated batch 76 batch loss 1.31937647\n",
      "Validated batch 77 batch loss 1.3036983\n",
      "Validated batch 78 batch loss 1.19381547\n",
      "Validated batch 79 batch loss 1.15221465\n",
      "Validated batch 80 batch loss 1.229177\n",
      "Validated batch 81 batch loss 1.17408538\n",
      "Validated batch 82 batch loss 1.22181177\n",
      "Validated batch 83 batch loss 1.36593688\n",
      "Validated batch 84 batch loss 1.27171302\n",
      "Validated batch 85 batch loss 1.25053835\n",
      "Validated batch 86 batch loss 1.38079822\n",
      "Validated batch 87 batch loss 1.10614395\n",
      "Validated batch 88 batch loss 1.24603987\n",
      "Validated batch 89 batch loss 1.06853116\n",
      "Validated batch 90 batch loss 1.13939583\n",
      "Validated batch 91 batch loss 1.31824136\n",
      "Validated batch 92 batch loss 1.10953856\n",
      "Validated batch 93 batch loss 1.14289713\n",
      "Validated batch 94 batch loss 1.20529246\n",
      "Validated batch 95 batch loss 1.1796416\n",
      "Validated batch 96 batch loss 1.07972193\n",
      "Validated batch 97 batch loss 1.15265441\n",
      "Validated batch 98 batch loss 1.2972883\n",
      "Validated batch 99 batch loss 1.09005415\n",
      "Validated batch 100 batch loss 1.05749941\n",
      "Validated batch 101 batch loss 1.11549592\n",
      "Validated batch 102 batch loss 1.16915\n",
      "Validated batch 103 batch loss 1.08805346\n",
      "Validated batch 104 batch loss 1.18930614\n",
      "Validated batch 105 batch loss 1.16535354\n",
      "Validated batch 106 batch loss 1.13291728\n",
      "Validated batch 107 batch loss 1.25759\n",
      "Validated batch 108 batch loss 1.36927068\n",
      "Validated batch 109 batch loss 1.08642232\n",
      "Validated batch 110 batch loss 1.29560435\n",
      "Validated batch 111 batch loss 0.995475471\n",
      "Validated batch 112 batch loss 1.13207018\n",
      "Validated batch 113 batch loss 1.15423799\n",
      "Validated batch 114 batch loss 1.14427686\n",
      "Validated batch 115 batch loss 1.33128178\n",
      "Validated batch 116 batch loss 1.15025759\n",
      "Validated batch 117 batch loss 1.17022979\n",
      "Validated batch 118 batch loss 1.27226329\n",
      "Validated batch 119 batch loss 1.15824461\n",
      "Validated batch 120 batch loss 1.26595402\n",
      "Validated batch 121 batch loss 1.40195179\n",
      "Validated batch 122 batch loss 1.11426437\n",
      "Validated batch 123 batch loss 1.19170642\n",
      "Validated batch 124 batch loss 1.20698404\n",
      "Validated batch 125 batch loss 1.25150692\n",
      "Validated batch 126 batch loss 1.26951957\n",
      "Validated batch 127 batch loss 1.1527915\n",
      "Validated batch 128 batch loss 1.00126696\n",
      "Validated batch 129 batch loss 1.1839211\n",
      "Validated batch 130 batch loss 1.11853266\n",
      "Validated batch 131 batch loss 1.18697119\n",
      "Validated batch 132 batch loss 1.25439095\n",
      "Validated batch 133 batch loss 1.06128359\n",
      "Validated batch 134 batch loss 1.22540724\n",
      "Validated batch 135 batch loss 1.31773877\n",
      "Validated batch 136 batch loss 1.26382113\n",
      "Validated batch 137 batch loss 1.23564672\n",
      "Validated batch 138 batch loss 1.11989522\n",
      "Validated batch 139 batch loss 1.30502486\n",
      "Validated batch 140 batch loss 1.21299565\n",
      "Validated batch 141 batch loss 1.1884259\n",
      "Validated batch 142 batch loss 1.19665515\n",
      "Validated batch 143 batch loss 1.14479113\n",
      "Validated batch 144 batch loss 1.25509882\n",
      "Validated batch 145 batch loss 1.25151241\n",
      "Validated batch 146 batch loss 1.16793609\n",
      "Validated batch 147 batch loss 1.15687835\n",
      "Validated batch 148 batch loss 1.26332057\n",
      "Validated batch 149 batch loss 1.19078898\n",
      "Validated batch 150 batch loss 1.29518235\n",
      "Validated batch 151 batch loss 1.28442717\n",
      "Validated batch 152 batch loss 1.11591339\n",
      "Validated batch 153 batch loss 1.16563106\n",
      "Validated batch 154 batch loss 1.24128\n",
      "Validated batch 155 batch loss 1.16173875\n",
      "Validated batch 156 batch loss 1.26905978\n",
      "Validated batch 157 batch loss 1.23544693\n",
      "Validated batch 158 batch loss 1.38607609\n",
      "Validated batch 159 batch loss 1.34383261\n",
      "Validated batch 160 batch loss 1.23248661\n",
      "Validated batch 161 batch loss 1.12904048\n",
      "Validated batch 162 batch loss 1.17154193\n",
      "Validated batch 163 batch loss 1.24082422\n",
      "Validated batch 164 batch loss 1.24386621\n",
      "Validated batch 165 batch loss 1.19960332\n",
      "Validated batch 166 batch loss 1.28286314\n",
      "Validated batch 167 batch loss 1.42433143\n",
      "Validated batch 168 batch loss 1.16443491\n",
      "Validated batch 169 batch loss 1.27930045\n",
      "Validated batch 170 batch loss 1.19687903\n",
      "Validated batch 171 batch loss 1.31572354\n",
      "Validated batch 172 batch loss 1.22113478\n",
      "Validated batch 173 batch loss 1.19829488\n",
      "Validated batch 174 batch loss 1.26208532\n",
      "Validated batch 175 batch loss 1.30200982\n",
      "Validated batch 176 batch loss 1.26729667\n",
      "Validated batch 177 batch loss 1.28991497\n",
      "Validated batch 178 batch loss 1.30534768\n",
      "Validated batch 179 batch loss 1.19972861\n",
      "Validated batch 180 batch loss 1.16457701\n",
      "Validated batch 181 batch loss 1.24507976\n",
      "Validated batch 182 batch loss 1.1907351\n",
      "Validated batch 183 batch loss 1.20493555\n",
      "Validated batch 184 batch loss 1.24171948\n",
      "Validated batch 185 batch loss 1.31143272\n",
      "Epoch 4 val loss 1.2129207849502563\n",
      "Model /aiffel/aiffel/mpii/trained/model-epoch-4-loss-1.2129.h5 saved.\n",
      "Start epoch 5 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.2274344 epoch total loss 1.2274344\n",
      "Trained batch 2 batch loss 1.22066796 epoch total loss 1.22405124\n",
      "Trained batch 3 batch loss 1.28176308 epoch total loss 1.24328852\n",
      "Trained batch 4 batch loss 1.16420078 epoch total loss 1.22351658\n",
      "Trained batch 5 batch loss 1.15840197 epoch total loss 1.21049368\n",
      "Trained batch 6 batch loss 1.12162709 epoch total loss 1.19568253\n",
      "Trained batch 7 batch loss 1.19226789 epoch total loss 1.19519484\n",
      "Trained batch 8 batch loss 1.22887111 epoch total loss 1.19940436\n",
      "Trained batch 9 batch loss 1.18346238 epoch total loss 1.19763303\n",
      "Trained batch 10 batch loss 1.18006313 epoch total loss 1.195876\n",
      "Trained batch 11 batch loss 1.29602814 epoch total loss 1.20498073\n",
      "Trained batch 12 batch loss 1.19293141 epoch total loss 1.20397663\n",
      "Trained batch 13 batch loss 1.10378993 epoch total loss 1.19627\n",
      "Trained batch 14 batch loss 1.12521815 epoch total loss 1.19119477\n",
      "Trained batch 15 batch loss 1.26438379 epoch total loss 1.19607401\n",
      "Trained batch 16 batch loss 1.12852085 epoch total loss 1.19185197\n",
      "Trained batch 17 batch loss 1.23949718 epoch total loss 1.19465458\n",
      "Trained batch 18 batch loss 1.18202257 epoch total loss 1.1939528\n",
      "Trained batch 19 batch loss 1.09078491 epoch total loss 1.18852282\n",
      "Trained batch 20 batch loss 1.04258394 epoch total loss 1.1812259\n",
      "Trained batch 21 batch loss 1.30191278 epoch total loss 1.18697286\n",
      "Trained batch 22 batch loss 1.26393366 epoch total loss 1.19047105\n",
      "Trained batch 23 batch loss 1.20458412 epoch total loss 1.19108462\n",
      "Trained batch 24 batch loss 1.22414279 epoch total loss 1.19246209\n",
      "Trained batch 25 batch loss 1.26615596 epoch total loss 1.19540977\n",
      "Trained batch 26 batch loss 1.28852725 epoch total loss 1.19899118\n",
      "Trained batch 27 batch loss 1.2300688 epoch total loss 1.20014215\n",
      "Trained batch 28 batch loss 1.23345041 epoch total loss 1.20133185\n",
      "Trained batch 29 batch loss 1.31900418 epoch total loss 1.2053895\n",
      "Trained batch 30 batch loss 1.23533511 epoch total loss 1.20638776\n",
      "Trained batch 31 batch loss 1.2401464 epoch total loss 1.20747674\n",
      "Trained batch 32 batch loss 1.25930381 epoch total loss 1.20909631\n",
      "Trained batch 33 batch loss 1.20664573 epoch total loss 1.20902205\n",
      "Trained batch 34 batch loss 1.33154261 epoch total loss 1.21262562\n",
      "Trained batch 35 batch loss 1.264274 epoch total loss 1.21410131\n",
      "Trained batch 36 batch loss 1.21965432 epoch total loss 1.21425557\n",
      "Trained batch 37 batch loss 1.22372508 epoch total loss 1.21451151\n",
      "Trained batch 38 batch loss 1.16798759 epoch total loss 1.21328712\n",
      "Trained batch 39 batch loss 1.21005571 epoch total loss 1.21320426\n",
      "Trained batch 40 batch loss 1.23917377 epoch total loss 1.2138536\n",
      "Trained batch 41 batch loss 1.19381428 epoch total loss 1.21336472\n",
      "Trained batch 42 batch loss 1.13261521 epoch total loss 1.21144211\n",
      "Trained batch 43 batch loss 1.10389078 epoch total loss 1.20894086\n",
      "Trained batch 44 batch loss 1.25178111 epoch total loss 1.20991457\n",
      "Trained batch 45 batch loss 1.21279657 epoch total loss 1.20997858\n",
      "Trained batch 46 batch loss 1.23271036 epoch total loss 1.21047282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 47 batch loss 1.22439563 epoch total loss 1.21076906\n",
      "Trained batch 48 batch loss 1.1469686 epoch total loss 1.20943987\n",
      "Trained batch 49 batch loss 1.22072577 epoch total loss 1.20967019\n",
      "Trained batch 50 batch loss 1.23098111 epoch total loss 1.21009636\n",
      "Trained batch 51 batch loss 1.25338364 epoch total loss 1.21094513\n",
      "Trained batch 52 batch loss 1.35129333 epoch total loss 1.21364415\n",
      "Trained batch 53 batch loss 1.33241057 epoch total loss 1.21588492\n",
      "Trained batch 54 batch loss 1.33304358 epoch total loss 1.21805465\n",
      "Trained batch 55 batch loss 1.27891695 epoch total loss 1.21916115\n",
      "Trained batch 56 batch loss 1.26350284 epoch total loss 1.21995294\n",
      "Trained batch 57 batch loss 1.14810395 epoch total loss 1.21869242\n",
      "Trained batch 58 batch loss 1.17238224 epoch total loss 1.21789396\n",
      "Trained batch 59 batch loss 1.23366666 epoch total loss 1.21816123\n",
      "Trained batch 60 batch loss 1.19333959 epoch total loss 1.21774745\n",
      "Trained batch 61 batch loss 1.16771185 epoch total loss 1.21692717\n",
      "Trained batch 62 batch loss 1.23573053 epoch total loss 1.21723056\n",
      "Trained batch 63 batch loss 1.18785322 epoch total loss 1.21676421\n",
      "Trained batch 64 batch loss 1.27844036 epoch total loss 1.2177279\n",
      "Trained batch 65 batch loss 1.29207659 epoch total loss 1.21887171\n",
      "Trained batch 66 batch loss 1.24417901 epoch total loss 1.21925521\n",
      "Trained batch 67 batch loss 1.28991926 epoch total loss 1.22030985\n",
      "Trained batch 68 batch loss 1.27413595 epoch total loss 1.2211014\n",
      "Trained batch 69 batch loss 1.35927701 epoch total loss 1.223104\n",
      "Trained batch 70 batch loss 1.2289995 epoch total loss 1.22318816\n",
      "Trained batch 71 batch loss 1.2127769 epoch total loss 1.22304153\n",
      "Trained batch 72 batch loss 1.22577453 epoch total loss 1.22307944\n",
      "Trained batch 73 batch loss 1.19495487 epoch total loss 1.22269416\n",
      "Trained batch 74 batch loss 1.18638754 epoch total loss 1.22220349\n",
      "Trained batch 75 batch loss 1.1649121 epoch total loss 1.2214396\n",
      "Trained batch 76 batch loss 1.22675574 epoch total loss 1.22150958\n",
      "Trained batch 77 batch loss 1.2173363 epoch total loss 1.22145534\n",
      "Trained batch 78 batch loss 1.11555684 epoch total loss 1.22009766\n",
      "Trained batch 79 batch loss 1.11855793 epoch total loss 1.21881235\n",
      "Trained batch 80 batch loss 1.05695426 epoch total loss 1.21678913\n",
      "Trained batch 81 batch loss 1.06321549 epoch total loss 1.21489322\n",
      "Trained batch 82 batch loss 1.05173993 epoch total loss 1.2129035\n",
      "Trained batch 83 batch loss 1.19600773 epoch total loss 1.2127\n",
      "Trained batch 84 batch loss 1.23797345 epoch total loss 1.21300089\n",
      "Trained batch 85 batch loss 1.2620697 epoch total loss 1.21357822\n",
      "Trained batch 86 batch loss 1.17222083 epoch total loss 1.21309721\n",
      "Trained batch 87 batch loss 1.15889919 epoch total loss 1.21247423\n",
      "Trained batch 88 batch loss 1.32935762 epoch total loss 1.21380246\n",
      "Trained batch 89 batch loss 1.0966624 epoch total loss 1.21248639\n",
      "Trained batch 90 batch loss 0.950715065 epoch total loss 1.2095778\n",
      "Trained batch 91 batch loss 1.0161252 epoch total loss 1.20745194\n",
      "Trained batch 92 batch loss 1.14497352 epoch total loss 1.2067728\n",
      "Trained batch 93 batch loss 1.23433936 epoch total loss 1.20706928\n",
      "Trained batch 94 batch loss 1.27318335 epoch total loss 1.20777261\n",
      "Trained batch 95 batch loss 1.33089459 epoch total loss 1.20906866\n",
      "Trained batch 96 batch loss 1.11363339 epoch total loss 1.20807445\n",
      "Trained batch 97 batch loss 1.08673811 epoch total loss 1.20682359\n",
      "Trained batch 98 batch loss 1.1486367 epoch total loss 1.20622981\n",
      "Trained batch 99 batch loss 1.28548098 epoch total loss 1.20703042\n",
      "Trained batch 100 batch loss 1.20936847 epoch total loss 1.20705378\n",
      "Trained batch 101 batch loss 1.28666902 epoch total loss 1.20784199\n",
      "Trained batch 102 batch loss 1.22313845 epoch total loss 1.20799196\n",
      "Trained batch 103 batch loss 1.34317195 epoch total loss 1.20930433\n",
      "Trained batch 104 batch loss 1.3796612 epoch total loss 1.21094239\n",
      "Trained batch 105 batch loss 1.23317075 epoch total loss 1.2111541\n",
      "Trained batch 106 batch loss 1.14091325 epoch total loss 1.21049142\n",
      "Trained batch 107 batch loss 1.15540481 epoch total loss 1.20997667\n",
      "Trained batch 108 batch loss 1.11513543 epoch total loss 1.20909858\n",
      "Trained batch 109 batch loss 1.22535539 epoch total loss 1.20924771\n",
      "Trained batch 110 batch loss 1.15440488 epoch total loss 1.20874906\n",
      "Trained batch 111 batch loss 1.1769681 epoch total loss 1.20846283\n",
      "Trained batch 112 batch loss 1.23192906 epoch total loss 1.2086724\n",
      "Trained batch 113 batch loss 1.24474955 epoch total loss 1.20899165\n",
      "Trained batch 114 batch loss 1.15242183 epoch total loss 1.20849538\n",
      "Trained batch 115 batch loss 1.2079134 epoch total loss 1.20849037\n",
      "Trained batch 116 batch loss 1.10348117 epoch total loss 1.20758522\n",
      "Trained batch 117 batch loss 1.12893772 epoch total loss 1.20691299\n",
      "Trained batch 118 batch loss 1.13973093 epoch total loss 1.20634353\n",
      "Trained batch 119 batch loss 1.21947122 epoch total loss 1.2064538\n",
      "Trained batch 120 batch loss 1.29359567 epoch total loss 1.20718\n",
      "Trained batch 121 batch loss 1.1835475 epoch total loss 1.20698476\n",
      "Trained batch 122 batch loss 1.12841249 epoch total loss 1.20634067\n",
      "Trained batch 123 batch loss 1.2872057 epoch total loss 1.20699811\n",
      "Trained batch 124 batch loss 1.1294868 epoch total loss 1.20637298\n",
      "Trained batch 125 batch loss 1.22209668 epoch total loss 1.20649874\n",
      "Trained batch 126 batch loss 1.14347255 epoch total loss 1.20599866\n",
      "Trained batch 127 batch loss 1.12068021 epoch total loss 1.2053268\n",
      "Trained batch 128 batch loss 1.18161058 epoch total loss 1.20514154\n",
      "Trained batch 129 batch loss 1.04183173 epoch total loss 1.20387554\n",
      "Trained batch 130 batch loss 1.02702188 epoch total loss 1.20251513\n",
      "Trained batch 131 batch loss 1.06124139 epoch total loss 1.20143676\n",
      "Trained batch 132 batch loss 1.09539747 epoch total loss 1.20063341\n",
      "Trained batch 133 batch loss 1.21487284 epoch total loss 1.20074046\n",
      "Trained batch 134 batch loss 1.35433984 epoch total loss 1.20188677\n",
      "Trained batch 135 batch loss 1.33846605 epoch total loss 1.2028985\n",
      "Trained batch 136 batch loss 1.30079615 epoch total loss 1.20361829\n",
      "Trained batch 137 batch loss 1.41747689 epoch total loss 1.20517933\n",
      "Trained batch 138 batch loss 1.23100853 epoch total loss 1.20536649\n",
      "Trained batch 139 batch loss 1.21124458 epoch total loss 1.20540881\n",
      "Trained batch 140 batch loss 1.30301213 epoch total loss 1.20610595\n",
      "Trained batch 141 batch loss 1.26423585 epoch total loss 1.20651817\n",
      "Trained batch 142 batch loss 1.26366234 epoch total loss 1.20692062\n",
      "Trained batch 143 batch loss 1.12520063 epoch total loss 1.20634913\n",
      "Trained batch 144 batch loss 1.03065574 epoch total loss 1.20512903\n",
      "Trained batch 145 batch loss 1.02335191 epoch total loss 1.2038753\n",
      "Trained batch 146 batch loss 1.16608667 epoch total loss 1.2036165\n",
      "Trained batch 147 batch loss 1.16525817 epoch total loss 1.20335555\n",
      "Trained batch 148 batch loss 1.23432708 epoch total loss 1.20356488\n",
      "Trained batch 149 batch loss 1.18419576 epoch total loss 1.20343482\n",
      "Trained batch 150 batch loss 1.36037731 epoch total loss 1.20448112\n",
      "Trained batch 151 batch loss 1.22398019 epoch total loss 1.20461023\n",
      "Trained batch 152 batch loss 1.33286452 epoch total loss 1.20545411\n",
      "Trained batch 153 batch loss 1.14463258 epoch total loss 1.20505655\n",
      "Trained batch 154 batch loss 1.17879 epoch total loss 1.20488596\n",
      "Trained batch 155 batch loss 1.22055113 epoch total loss 1.20498705\n",
      "Trained batch 156 batch loss 1.19540358 epoch total loss 1.20492566\n",
      "Trained batch 157 batch loss 1.14474058 epoch total loss 1.20454228\n",
      "Trained batch 158 batch loss 1.1610707 epoch total loss 1.20426714\n",
      "Trained batch 159 batch loss 1.11328113 epoch total loss 1.20369494\n",
      "Trained batch 160 batch loss 1.32677698 epoch total loss 1.2044642\n",
      "Trained batch 161 batch loss 1.31272101 epoch total loss 1.20513666\n",
      "Trained batch 162 batch loss 1.35678887 epoch total loss 1.20607281\n",
      "Trained batch 163 batch loss 1.06566572 epoch total loss 1.20521128\n",
      "Trained batch 164 batch loss 1.2097764 epoch total loss 1.20523918\n",
      "Trained batch 165 batch loss 1.05726278 epoch total loss 1.20434237\n",
      "Trained batch 166 batch loss 1.15759492 epoch total loss 1.20406079\n",
      "Trained batch 167 batch loss 1.28085065 epoch total loss 1.20452058\n",
      "Trained batch 168 batch loss 1.22926664 epoch total loss 1.20466781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 169 batch loss 1.3140732 epoch total loss 1.20531523\n",
      "Trained batch 170 batch loss 1.3295182 epoch total loss 1.20604575\n",
      "Trained batch 171 batch loss 1.43332815 epoch total loss 1.20737493\n",
      "Trained batch 172 batch loss 1.288468 epoch total loss 1.2078464\n",
      "Trained batch 173 batch loss 1.05679679 epoch total loss 1.20697331\n",
      "Trained batch 174 batch loss 1.1056627 epoch total loss 1.2063911\n",
      "Trained batch 175 batch loss 1.0355885 epoch total loss 1.20541501\n",
      "Trained batch 176 batch loss 1.11569381 epoch total loss 1.20490527\n",
      "Trained batch 177 batch loss 1.07687211 epoch total loss 1.20418191\n",
      "Trained batch 178 batch loss 1.15574419 epoch total loss 1.20390975\n",
      "Trained batch 179 batch loss 1.25421655 epoch total loss 1.20419085\n",
      "Trained batch 180 batch loss 1.20997572 epoch total loss 1.20422292\n",
      "Trained batch 181 batch loss 1.17526293 epoch total loss 1.20406294\n",
      "Trained batch 182 batch loss 1.35022986 epoch total loss 1.20486605\n",
      "Trained batch 183 batch loss 1.1822139 epoch total loss 1.20474231\n",
      "Trained batch 184 batch loss 1.29836679 epoch total loss 1.20525122\n",
      "Trained batch 185 batch loss 1.26743889 epoch total loss 1.20558739\n",
      "Trained batch 186 batch loss 1.25519514 epoch total loss 1.20585406\n",
      "Trained batch 187 batch loss 1.04466903 epoch total loss 1.20499206\n",
      "Trained batch 188 batch loss 1.12199628 epoch total loss 1.2045505\n",
      "Trained batch 189 batch loss 1.22538471 epoch total loss 1.20466077\n",
      "Trained batch 190 batch loss 1.41623068 epoch total loss 1.20577431\n",
      "Trained batch 191 batch loss 1.24204159 epoch total loss 1.20596421\n",
      "Trained batch 192 batch loss 1.27736592 epoch total loss 1.20633602\n",
      "Trained batch 193 batch loss 1.13458979 epoch total loss 1.20596421\n",
      "Trained batch 194 batch loss 1.19514608 epoch total loss 1.20590842\n",
      "Trained batch 195 batch loss 1.22187042 epoch total loss 1.20599031\n",
      "Trained batch 196 batch loss 1.15274858 epoch total loss 1.20571876\n",
      "Trained batch 197 batch loss 1.3026346 epoch total loss 1.20621073\n",
      "Trained batch 198 batch loss 1.45403874 epoch total loss 1.20746231\n",
      "Trained batch 199 batch loss 1.1488893 epoch total loss 1.20716798\n",
      "Trained batch 200 batch loss 1.0439 epoch total loss 1.20635176\n",
      "Trained batch 201 batch loss 1.04469359 epoch total loss 1.20554745\n",
      "Trained batch 202 batch loss 1.10046387 epoch total loss 1.20502722\n",
      "Trained batch 203 batch loss 1.21352375 epoch total loss 1.20506907\n",
      "Trained batch 204 batch loss 1.30043125 epoch total loss 1.20553648\n",
      "Trained batch 205 batch loss 1.33937836 epoch total loss 1.20618939\n",
      "Trained batch 206 batch loss 1.24387336 epoch total loss 1.20637226\n",
      "Trained batch 207 batch loss 1.20438266 epoch total loss 1.20636261\n",
      "Trained batch 208 batch loss 1.19477737 epoch total loss 1.20630693\n",
      "Trained batch 209 batch loss 1.16503465 epoch total loss 1.20610952\n",
      "Trained batch 210 batch loss 1.11294603 epoch total loss 1.20566583\n",
      "Trained batch 211 batch loss 1.34001684 epoch total loss 1.20630264\n",
      "Trained batch 212 batch loss 1.20377386 epoch total loss 1.20629072\n",
      "Trained batch 213 batch loss 1.20466495 epoch total loss 1.20628309\n",
      "Trained batch 214 batch loss 1.11998403 epoch total loss 1.20587981\n",
      "Trained batch 215 batch loss 1.1663748 epoch total loss 1.20569611\n",
      "Trained batch 216 batch loss 1.20000803 epoch total loss 1.20566988\n",
      "Trained batch 217 batch loss 1.31063056 epoch total loss 1.20615351\n",
      "Trained batch 218 batch loss 1.42029154 epoch total loss 1.2071358\n",
      "Trained batch 219 batch loss 1.24039376 epoch total loss 1.20728767\n",
      "Trained batch 220 batch loss 1.32175 epoch total loss 1.2078079\n",
      "Trained batch 221 batch loss 1.17142057 epoch total loss 1.20764327\n",
      "Trained batch 222 batch loss 1.20754766 epoch total loss 1.20764279\n",
      "Trained batch 223 batch loss 1.20445251 epoch total loss 1.20762849\n",
      "Trained batch 224 batch loss 1.23298597 epoch total loss 1.20774162\n",
      "Trained batch 225 batch loss 1.11323285 epoch total loss 1.20732152\n",
      "Trained batch 226 batch loss 1.2605778 epoch total loss 1.2075572\n",
      "Trained batch 227 batch loss 1.24601698 epoch total loss 1.2077266\n",
      "Trained batch 228 batch loss 1.21609235 epoch total loss 1.20776331\n",
      "Trained batch 229 batch loss 1.14668703 epoch total loss 1.20749664\n",
      "Trained batch 230 batch loss 1.15045333 epoch total loss 1.20724857\n",
      "Trained batch 231 batch loss 1.02928472 epoch total loss 1.20647824\n",
      "Trained batch 232 batch loss 1.25440443 epoch total loss 1.20668483\n",
      "Trained batch 233 batch loss 1.18263876 epoch total loss 1.20658159\n",
      "Trained batch 234 batch loss 1.12460685 epoch total loss 1.20623124\n",
      "Trained batch 235 batch loss 1.04051042 epoch total loss 1.20552599\n",
      "Trained batch 236 batch loss 1.08320928 epoch total loss 1.20500779\n",
      "Trained batch 237 batch loss 1.08173859 epoch total loss 1.20448756\n",
      "Trained batch 238 batch loss 1.27814221 epoch total loss 1.20479703\n",
      "Trained batch 239 batch loss 1.24976707 epoch total loss 1.20498514\n",
      "Trained batch 240 batch loss 1.22736955 epoch total loss 1.20507836\n",
      "Trained batch 241 batch loss 1.23896837 epoch total loss 1.20521903\n",
      "Trained batch 242 batch loss 1.17746234 epoch total loss 1.20510435\n",
      "Trained batch 243 batch loss 1.14085078 epoch total loss 1.20484\n",
      "Trained batch 244 batch loss 1.13616168 epoch total loss 1.20455849\n",
      "Trained batch 245 batch loss 1.12471187 epoch total loss 1.20423257\n",
      "Trained batch 246 batch loss 1.12575674 epoch total loss 1.20391357\n",
      "Trained batch 247 batch loss 1.23999739 epoch total loss 1.20405972\n",
      "Trained batch 248 batch loss 1.15450978 epoch total loss 1.20385993\n",
      "Trained batch 249 batch loss 1.12018418 epoch total loss 1.20352376\n",
      "Trained batch 250 batch loss 1.1622529 epoch total loss 1.20335877\n",
      "Trained batch 251 batch loss 1.17908216 epoch total loss 1.20326197\n",
      "Trained batch 252 batch loss 1.16132784 epoch total loss 1.20309556\n",
      "Trained batch 253 batch loss 1.09132123 epoch total loss 1.20265377\n",
      "Trained batch 254 batch loss 1.10335553 epoch total loss 1.20226276\n",
      "Trained batch 255 batch loss 1.24119067 epoch total loss 1.20241547\n",
      "Trained batch 256 batch loss 1.04537666 epoch total loss 1.20180202\n",
      "Trained batch 257 batch loss 1.14195299 epoch total loss 1.2015692\n",
      "Trained batch 258 batch loss 1.20024109 epoch total loss 1.20156407\n",
      "Trained batch 259 batch loss 1.23011458 epoch total loss 1.20167434\n",
      "Trained batch 260 batch loss 1.23845565 epoch total loss 1.20181584\n",
      "Trained batch 261 batch loss 1.14802432 epoch total loss 1.20160961\n",
      "Trained batch 262 batch loss 1.25994515 epoch total loss 1.20183229\n",
      "Trained batch 263 batch loss 1.28393257 epoch total loss 1.2021445\n",
      "Trained batch 264 batch loss 1.22117841 epoch total loss 1.20221663\n",
      "Trained batch 265 batch loss 1.32737112 epoch total loss 1.20268893\n",
      "Trained batch 266 batch loss 1.3631829 epoch total loss 1.20329225\n",
      "Trained batch 267 batch loss 1.2475971 epoch total loss 1.20345819\n",
      "Trained batch 268 batch loss 1.29565907 epoch total loss 1.20380223\n",
      "Trained batch 269 batch loss 1.31983328 epoch total loss 1.20423353\n",
      "Trained batch 270 batch loss 1.3126514 epoch total loss 1.20463502\n",
      "Trained batch 271 batch loss 1.22774649 epoch total loss 1.20472038\n",
      "Trained batch 272 batch loss 1.15548205 epoch total loss 1.2045393\n",
      "Trained batch 273 batch loss 1.10040796 epoch total loss 1.20415795\n",
      "Trained batch 274 batch loss 1.20730805 epoch total loss 1.20416939\n",
      "Trained batch 275 batch loss 1.22503078 epoch total loss 1.20424533\n",
      "Trained batch 276 batch loss 1.26240349 epoch total loss 1.20445597\n",
      "Trained batch 277 batch loss 1.25066209 epoch total loss 1.20462275\n",
      "Trained batch 278 batch loss 1.15571666 epoch total loss 1.20444691\n",
      "Trained batch 279 batch loss 1.19298911 epoch total loss 1.2044059\n",
      "Trained batch 280 batch loss 1.13383484 epoch total loss 1.2041539\n",
      "Trained batch 281 batch loss 1.19981599 epoch total loss 1.20413852\n",
      "Trained batch 282 batch loss 1.02323234 epoch total loss 1.20349693\n",
      "Trained batch 283 batch loss 0.984535515 epoch total loss 1.20272315\n",
      "Trained batch 284 batch loss 1.16686296 epoch total loss 1.2025969\n",
      "Trained batch 285 batch loss 1.2127136 epoch total loss 1.20263243\n",
      "Trained batch 286 batch loss 1.24176729 epoch total loss 1.20276928\n",
      "Trained batch 287 batch loss 1.32360923 epoch total loss 1.20319033\n",
      "Trained batch 288 batch loss 1.25849342 epoch total loss 1.20338225\n",
      "Trained batch 289 batch loss 1.16073596 epoch total loss 1.20323467\n",
      "Trained batch 290 batch loss 1.04586387 epoch total loss 1.20269203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 291 batch loss 1.0618012 epoch total loss 1.20220792\n",
      "Trained batch 292 batch loss 1.09420192 epoch total loss 1.20183802\n",
      "Trained batch 293 batch loss 1.15249252 epoch total loss 1.20166969\n",
      "Trained batch 294 batch loss 1.24213421 epoch total loss 1.20180726\n",
      "Trained batch 295 batch loss 1.44802666 epoch total loss 1.20264184\n",
      "Trained batch 296 batch loss 1.2660712 epoch total loss 1.20285618\n",
      "Trained batch 297 batch loss 1.19535553 epoch total loss 1.20283091\n",
      "Trained batch 298 batch loss 1.27740169 epoch total loss 1.20308113\n",
      "Trained batch 299 batch loss 1.26162457 epoch total loss 1.20327699\n",
      "Trained batch 300 batch loss 1.35872078 epoch total loss 1.20379519\n",
      "Trained batch 301 batch loss 1.26794052 epoch total loss 1.20400834\n",
      "Trained batch 302 batch loss 1.31771767 epoch total loss 1.2043848\n",
      "Trained batch 303 batch loss 1.27338254 epoch total loss 1.20461249\n",
      "Trained batch 304 batch loss 1.17628837 epoch total loss 1.20451939\n",
      "Trained batch 305 batch loss 1.2309885 epoch total loss 1.20460618\n",
      "Trained batch 306 batch loss 1.32215214 epoch total loss 1.20499027\n",
      "Trained batch 307 batch loss 1.37381887 epoch total loss 1.20554018\n",
      "Trained batch 308 batch loss 1.29265618 epoch total loss 1.20582306\n",
      "Trained batch 309 batch loss 1.27519774 epoch total loss 1.20604753\n",
      "Trained batch 310 batch loss 1.3196032 epoch total loss 1.20641387\n",
      "Trained batch 311 batch loss 1.33023369 epoch total loss 1.20681202\n",
      "Trained batch 312 batch loss 1.16603196 epoch total loss 1.20668137\n",
      "Trained batch 313 batch loss 1.06281209 epoch total loss 1.2062217\n",
      "Trained batch 314 batch loss 1.20674384 epoch total loss 1.20622337\n",
      "Trained batch 315 batch loss 1.13075602 epoch total loss 1.20598388\n",
      "Trained batch 316 batch loss 1.28690422 epoch total loss 1.20623994\n",
      "Trained batch 317 batch loss 1.28084111 epoch total loss 1.20647526\n",
      "Trained batch 318 batch loss 1.09206903 epoch total loss 1.20611548\n",
      "Trained batch 319 batch loss 1.18114 epoch total loss 1.20603728\n",
      "Trained batch 320 batch loss 1.02891195 epoch total loss 1.20548368\n",
      "Trained batch 321 batch loss 1.10753262 epoch total loss 1.20517862\n",
      "Trained batch 322 batch loss 1.105744 epoch total loss 1.20486987\n",
      "Trained batch 323 batch loss 1.09639859 epoch total loss 1.20453405\n",
      "Trained batch 324 batch loss 1.11460423 epoch total loss 1.20425642\n",
      "Trained batch 325 batch loss 1.18721783 epoch total loss 1.20420396\n",
      "Trained batch 326 batch loss 1.2055645 epoch total loss 1.20420814\n",
      "Trained batch 327 batch loss 1.16093314 epoch total loss 1.20407581\n",
      "Trained batch 328 batch loss 1.21157849 epoch total loss 1.2040987\n",
      "Trained batch 329 batch loss 1.3869102 epoch total loss 1.20465434\n",
      "Trained batch 330 batch loss 1.26581693 epoch total loss 1.20483959\n",
      "Trained batch 331 batch loss 1.14449167 epoch total loss 1.20465732\n",
      "Trained batch 332 batch loss 1.13476074 epoch total loss 1.20444679\n",
      "Trained batch 333 batch loss 1.04668808 epoch total loss 1.20397305\n",
      "Trained batch 334 batch loss 1.11887813 epoch total loss 1.2037183\n",
      "Trained batch 335 batch loss 1.26941562 epoch total loss 1.2039144\n",
      "Trained batch 336 batch loss 1.19371629 epoch total loss 1.20388401\n",
      "Trained batch 337 batch loss 1.18156815 epoch total loss 1.20381784\n",
      "Trained batch 338 batch loss 1.08016253 epoch total loss 1.20345199\n",
      "Trained batch 339 batch loss 1.06809664 epoch total loss 1.20305276\n",
      "Trained batch 340 batch loss 1.16594696 epoch total loss 1.20294356\n",
      "Trained batch 341 batch loss 1.37694025 epoch total loss 1.2034539\n",
      "Trained batch 342 batch loss 1.11795878 epoch total loss 1.20320392\n",
      "Trained batch 343 batch loss 1.18463182 epoch total loss 1.20314968\n",
      "Trained batch 344 batch loss 1.00694025 epoch total loss 1.20257938\n",
      "Trained batch 345 batch loss 0.888893366 epoch total loss 1.20167\n",
      "Trained batch 346 batch loss 0.925381899 epoch total loss 1.20087159\n",
      "Trained batch 347 batch loss 1.02814591 epoch total loss 1.20037377\n",
      "Trained batch 348 batch loss 1.10521138 epoch total loss 1.2001003\n",
      "Trained batch 349 batch loss 1.18431544 epoch total loss 1.20005512\n",
      "Trained batch 350 batch loss 1.27468014 epoch total loss 1.20026839\n",
      "Trained batch 351 batch loss 1.20093083 epoch total loss 1.2002703\n",
      "Trained batch 352 batch loss 1.25505698 epoch total loss 1.20042598\n",
      "Trained batch 353 batch loss 1.2409147 epoch total loss 1.20054054\n",
      "Trained batch 354 batch loss 1.29760814 epoch total loss 1.20081484\n",
      "Trained batch 355 batch loss 1.25941014 epoch total loss 1.20097983\n",
      "Trained batch 356 batch loss 1.30808115 epoch total loss 1.20128071\n",
      "Trained batch 357 batch loss 1.26631117 epoch total loss 1.20146275\n",
      "Trained batch 358 batch loss 1.14948285 epoch total loss 1.20131755\n",
      "Trained batch 359 batch loss 1.22228706 epoch total loss 1.20137596\n",
      "Trained batch 360 batch loss 1.37140632 epoch total loss 1.20184827\n",
      "Trained batch 361 batch loss 1.18651843 epoch total loss 1.20180583\n",
      "Trained batch 362 batch loss 1.23975623 epoch total loss 1.20191061\n",
      "Trained batch 363 batch loss 1.03663278 epoch total loss 1.20145524\n",
      "Trained batch 364 batch loss 0.979979634 epoch total loss 1.20084679\n",
      "Trained batch 365 batch loss 1.14569628 epoch total loss 1.20069575\n",
      "Trained batch 366 batch loss 1.15749288 epoch total loss 1.20057774\n",
      "Trained batch 367 batch loss 1.15771055 epoch total loss 1.20046091\n",
      "Trained batch 368 batch loss 1.22055817 epoch total loss 1.20051551\n",
      "Trained batch 369 batch loss 1.19279885 epoch total loss 1.20049465\n",
      "Trained batch 370 batch loss 1.10479712 epoch total loss 1.20023596\n",
      "Trained batch 371 batch loss 1.17814589 epoch total loss 1.20017636\n",
      "Trained batch 372 batch loss 1.27368975 epoch total loss 1.20037401\n",
      "Trained batch 373 batch loss 1.23739171 epoch total loss 1.20047319\n",
      "Trained batch 374 batch loss 1.25694406 epoch total loss 1.20062423\n",
      "Trained batch 375 batch loss 1.23784494 epoch total loss 1.20072353\n",
      "Trained batch 376 batch loss 1.16139674 epoch total loss 1.20061898\n",
      "Trained batch 377 batch loss 1.13880503 epoch total loss 1.20045495\n",
      "Trained batch 378 batch loss 1.22303402 epoch total loss 1.20051467\n",
      "Trained batch 379 batch loss 1.11185145 epoch total loss 1.20028079\n",
      "Trained batch 380 batch loss 1.02655113 epoch total loss 1.19982362\n",
      "Trained batch 381 batch loss 1.09852695 epoch total loss 1.19955778\n",
      "Trained batch 382 batch loss 1.10437799 epoch total loss 1.19930851\n",
      "Trained batch 383 batch loss 1.20416272 epoch total loss 1.19932127\n",
      "Trained batch 384 batch loss 1.22313297 epoch total loss 1.19938326\n",
      "Trained batch 385 batch loss 1.17924082 epoch total loss 1.19933093\n",
      "Trained batch 386 batch loss 0.993961871 epoch total loss 1.19879889\n",
      "Trained batch 387 batch loss 1.01004696 epoch total loss 1.19831109\n",
      "Trained batch 388 batch loss 0.941670716 epoch total loss 1.19764972\n",
      "Trained batch 389 batch loss 0.945735931 epoch total loss 1.19700205\n",
      "Trained batch 390 batch loss 1.08305097 epoch total loss 1.19670987\n",
      "Trained batch 391 batch loss 1.30162609 epoch total loss 1.19697821\n",
      "Trained batch 392 batch loss 1.18806088 epoch total loss 1.19695544\n",
      "Trained batch 393 batch loss 1.22390532 epoch total loss 1.19702399\n",
      "Trained batch 394 batch loss 1.2845422 epoch total loss 1.19724619\n",
      "Trained batch 395 batch loss 1.25160432 epoch total loss 1.19738388\n",
      "Trained batch 396 batch loss 1.27867317 epoch total loss 1.19758916\n",
      "Trained batch 397 batch loss 1.11310577 epoch total loss 1.19737637\n",
      "Trained batch 398 batch loss 1.21174145 epoch total loss 1.19741237\n",
      "Trained batch 399 batch loss 1.05214345 epoch total loss 1.19704831\n",
      "Trained batch 400 batch loss 1.07170868 epoch total loss 1.19673502\n",
      "Trained batch 401 batch loss 0.97863996 epoch total loss 1.19619107\n",
      "Trained batch 402 batch loss 1.06209564 epoch total loss 1.19585752\n",
      "Trained batch 403 batch loss 1.05234885 epoch total loss 1.19550145\n",
      "Trained batch 404 batch loss 0.910170197 epoch total loss 1.19479513\n",
      "Trained batch 405 batch loss 0.939919055 epoch total loss 1.19416583\n",
      "Trained batch 406 batch loss 0.92457521 epoch total loss 1.19350171\n",
      "Trained batch 407 batch loss 1.00563693 epoch total loss 1.19304013\n",
      "Trained batch 408 batch loss 1.13926697 epoch total loss 1.19290829\n",
      "Trained batch 409 batch loss 1.11107361 epoch total loss 1.19270825\n",
      "Trained batch 410 batch loss 1.20941603 epoch total loss 1.19274902\n",
      "Trained batch 411 batch loss 1.0905875 epoch total loss 1.19250047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 412 batch loss 1.27306581 epoch total loss 1.19269598\n",
      "Trained batch 413 batch loss 1.19283438 epoch total loss 1.19269633\n",
      "Trained batch 414 batch loss 1.26233256 epoch total loss 1.19286454\n",
      "Trained batch 415 batch loss 1.25858831 epoch total loss 1.19302285\n",
      "Trained batch 416 batch loss 1.05092835 epoch total loss 1.19268131\n",
      "Trained batch 417 batch loss 1.0074811 epoch total loss 1.19223714\n",
      "Trained batch 418 batch loss 1.22544158 epoch total loss 1.19231665\n",
      "Trained batch 419 batch loss 1.44852877 epoch total loss 1.19292808\n",
      "Trained batch 420 batch loss 1.26500225 epoch total loss 1.19309974\n",
      "Trained batch 421 batch loss 1.15050197 epoch total loss 1.19299853\n",
      "Trained batch 422 batch loss 1.17554927 epoch total loss 1.19295716\n",
      "Trained batch 423 batch loss 1.17540467 epoch total loss 1.19291568\n",
      "Trained batch 424 batch loss 1.14511323 epoch total loss 1.19280291\n",
      "Trained batch 425 batch loss 1.11975312 epoch total loss 1.19263101\n",
      "Trained batch 426 batch loss 1.21980512 epoch total loss 1.1926949\n",
      "Trained batch 427 batch loss 1.09227157 epoch total loss 1.1924597\n",
      "Trained batch 428 batch loss 1.26986825 epoch total loss 1.19264054\n",
      "Trained batch 429 batch loss 1.26402712 epoch total loss 1.19280696\n",
      "Trained batch 430 batch loss 1.2170192 epoch total loss 1.19286335\n",
      "Trained batch 431 batch loss 1.09877872 epoch total loss 1.19264495\n",
      "Trained batch 432 batch loss 1.17440104 epoch total loss 1.19260263\n",
      "Trained batch 433 batch loss 1.16784048 epoch total loss 1.19254553\n",
      "Trained batch 434 batch loss 1.20530796 epoch total loss 1.19257498\n",
      "Trained batch 435 batch loss 1.17309308 epoch total loss 1.19253016\n",
      "Trained batch 436 batch loss 1.23648882 epoch total loss 1.19263101\n",
      "Trained batch 437 batch loss 1.27806079 epoch total loss 1.19282651\n",
      "Trained batch 438 batch loss 1.22795773 epoch total loss 1.19290674\n",
      "Trained batch 439 batch loss 1.21861529 epoch total loss 1.19296539\n",
      "Trained batch 440 batch loss 1.31331503 epoch total loss 1.19323885\n",
      "Trained batch 441 batch loss 1.22613108 epoch total loss 1.19331348\n",
      "Trained batch 442 batch loss 1.13175762 epoch total loss 1.19317424\n",
      "Trained batch 443 batch loss 1.11759746 epoch total loss 1.19300365\n",
      "Trained batch 444 batch loss 1.1845479 epoch total loss 1.1929847\n",
      "Trained batch 445 batch loss 1.22582126 epoch total loss 1.19305849\n",
      "Trained batch 446 batch loss 1.28601253 epoch total loss 1.19326687\n",
      "Trained batch 447 batch loss 1.33489192 epoch total loss 1.19358373\n",
      "Trained batch 448 batch loss 1.19240546 epoch total loss 1.19358099\n",
      "Trained batch 449 batch loss 1.19723618 epoch total loss 1.19358921\n",
      "Trained batch 450 batch loss 1.23384488 epoch total loss 1.19367862\n",
      "Trained batch 451 batch loss 1.06901848 epoch total loss 1.19340229\n",
      "Trained batch 452 batch loss 1.19749069 epoch total loss 1.19341135\n",
      "Trained batch 453 batch loss 1.29882312 epoch total loss 1.19364405\n",
      "Trained batch 454 batch loss 1.24792576 epoch total loss 1.19376361\n",
      "Trained batch 455 batch loss 1.12143326 epoch total loss 1.19360471\n",
      "Trained batch 456 batch loss 1.20374227 epoch total loss 1.19362688\n",
      "Trained batch 457 batch loss 1.22191644 epoch total loss 1.19368887\n",
      "Trained batch 458 batch loss 1.27866805 epoch total loss 1.19387448\n",
      "Trained batch 459 batch loss 1.19534922 epoch total loss 1.1938777\n",
      "Trained batch 460 batch loss 1.28632247 epoch total loss 1.19407868\n",
      "Trained batch 461 batch loss 1.28455186 epoch total loss 1.1942749\n",
      "Trained batch 462 batch loss 1.2674644 epoch total loss 1.19443333\n",
      "Trained batch 463 batch loss 1.29328108 epoch total loss 1.19464684\n",
      "Trained batch 464 batch loss 1.27203679 epoch total loss 1.19481361\n",
      "Trained batch 465 batch loss 1.3141371 epoch total loss 1.19507015\n",
      "Trained batch 466 batch loss 1.24655831 epoch total loss 1.19518077\n",
      "Trained batch 467 batch loss 1.19865131 epoch total loss 1.19518816\n",
      "Trained batch 468 batch loss 1.28663087 epoch total loss 1.19538355\n",
      "Trained batch 469 batch loss 1.23374939 epoch total loss 1.19546545\n",
      "Trained batch 470 batch loss 1.21284425 epoch total loss 1.1955024\n",
      "Trained batch 471 batch loss 1.24907553 epoch total loss 1.19561613\n",
      "Trained batch 472 batch loss 1.2233032 epoch total loss 1.19567478\n",
      "Trained batch 473 batch loss 1.29816663 epoch total loss 1.1958915\n",
      "Trained batch 474 batch loss 1.26767421 epoch total loss 1.19604301\n",
      "Trained batch 475 batch loss 1.26216745 epoch total loss 1.19618213\n",
      "Trained batch 476 batch loss 1.2170881 epoch total loss 1.19622612\n",
      "Trained batch 477 batch loss 1.10736012 epoch total loss 1.1960398\n",
      "Trained batch 478 batch loss 1.02840292 epoch total loss 1.19568908\n",
      "Trained batch 479 batch loss 1.24731481 epoch total loss 1.19579685\n",
      "Trained batch 480 batch loss 1.37165546 epoch total loss 1.19616318\n",
      "Trained batch 481 batch loss 1.14087319 epoch total loss 1.19604826\n",
      "Trained batch 482 batch loss 1.35154605 epoch total loss 1.19637084\n",
      "Trained batch 483 batch loss 1.34949243 epoch total loss 1.19668782\n",
      "Trained batch 484 batch loss 1.47522736 epoch total loss 1.19726336\n",
      "Trained batch 485 batch loss 1.36531377 epoch total loss 1.19760978\n",
      "Trained batch 486 batch loss 1.05661297 epoch total loss 1.19731975\n",
      "Trained batch 487 batch loss 1.14236498 epoch total loss 1.19720697\n",
      "Trained batch 488 batch loss 1.19395387 epoch total loss 1.1972003\n",
      "Trained batch 489 batch loss 1.14109874 epoch total loss 1.19708562\n",
      "Trained batch 490 batch loss 1.09746432 epoch total loss 1.19688237\n",
      "Trained batch 491 batch loss 1.09387493 epoch total loss 1.19667256\n",
      "Trained batch 492 batch loss 1.06093836 epoch total loss 1.19639659\n",
      "Trained batch 493 batch loss 1.06343615 epoch total loss 1.19612694\n",
      "Trained batch 494 batch loss 1.04453361 epoch total loss 1.19582009\n",
      "Trained batch 495 batch loss 1.10649037 epoch total loss 1.19563961\n",
      "Trained batch 496 batch loss 1.11827755 epoch total loss 1.19548368\n",
      "Trained batch 497 batch loss 1.01072335 epoch total loss 1.19511199\n",
      "Trained batch 498 batch loss 1.05533195 epoch total loss 1.19483137\n",
      "Trained batch 499 batch loss 1.10633957 epoch total loss 1.19465399\n",
      "Trained batch 500 batch loss 1.29415488 epoch total loss 1.19485295\n",
      "Trained batch 501 batch loss 1.19290745 epoch total loss 1.19484901\n",
      "Trained batch 502 batch loss 1.0424546 epoch total loss 1.19454551\n",
      "Trained batch 503 batch loss 0.928978741 epoch total loss 1.19401753\n",
      "Trained batch 504 batch loss 1.07718682 epoch total loss 1.19378579\n",
      "Trained batch 505 batch loss 1.01559961 epoch total loss 1.19343293\n",
      "Trained batch 506 batch loss 1.18042791 epoch total loss 1.1934073\n",
      "Trained batch 507 batch loss 1.19548404 epoch total loss 1.19341135\n",
      "Trained batch 508 batch loss 1.12563205 epoch total loss 1.19327796\n",
      "Trained batch 509 batch loss 1.37231243 epoch total loss 1.19362962\n",
      "Trained batch 510 batch loss 1.30877388 epoch total loss 1.1938554\n",
      "Trained batch 511 batch loss 1.24350452 epoch total loss 1.19395268\n",
      "Trained batch 512 batch loss 1.33168125 epoch total loss 1.19422162\n",
      "Trained batch 513 batch loss 1.10724783 epoch total loss 1.1940521\n",
      "Trained batch 514 batch loss 1.17739892 epoch total loss 1.19401968\n",
      "Trained batch 515 batch loss 1.17993665 epoch total loss 1.19399238\n",
      "Trained batch 516 batch loss 1.3310442 epoch total loss 1.19425797\n",
      "Trained batch 517 batch loss 1.31049931 epoch total loss 1.1944828\n",
      "Trained batch 518 batch loss 1.27890241 epoch total loss 1.19464588\n",
      "Trained batch 519 batch loss 1.19614851 epoch total loss 1.19464874\n",
      "Trained batch 520 batch loss 1.12013245 epoch total loss 1.19450545\n",
      "Trained batch 521 batch loss 1.25985515 epoch total loss 1.19463086\n",
      "Trained batch 522 batch loss 1.17050815 epoch total loss 1.19458461\n",
      "Trained batch 523 batch loss 1.19632697 epoch total loss 1.19458807\n",
      "Trained batch 524 batch loss 1.29523277 epoch total loss 1.19478011\n",
      "Trained batch 525 batch loss 1.19312727 epoch total loss 1.19477689\n",
      "Trained batch 526 batch loss 1.20934987 epoch total loss 1.19480455\n",
      "Trained batch 527 batch loss 1.17575169 epoch total loss 1.19476855\n",
      "Trained batch 528 batch loss 1.24608552 epoch total loss 1.1948657\n",
      "Trained batch 529 batch loss 1.33006728 epoch total loss 1.19512129\n",
      "Trained batch 530 batch loss 1.00703502 epoch total loss 1.1947664\n",
      "Trained batch 531 batch loss 1.11282313 epoch total loss 1.19461203\n",
      "Trained batch 532 batch loss 1.12012959 epoch total loss 1.19447196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 533 batch loss 1.14741409 epoch total loss 1.19438374\n",
      "Trained batch 534 batch loss 1.15742207 epoch total loss 1.19431448\n",
      "Trained batch 535 batch loss 1.11596477 epoch total loss 1.19416797\n",
      "Trained batch 536 batch loss 1.27547383 epoch total loss 1.19431961\n",
      "Trained batch 537 batch loss 1.20622647 epoch total loss 1.1943419\n",
      "Trained batch 538 batch loss 1.2810266 epoch total loss 1.19450295\n",
      "Trained batch 539 batch loss 1.18934417 epoch total loss 1.19449329\n",
      "Trained batch 540 batch loss 1.38455367 epoch total loss 1.19484532\n",
      "Trained batch 541 batch loss 1.36858702 epoch total loss 1.19516647\n",
      "Trained batch 542 batch loss 1.32167172 epoch total loss 1.19539988\n",
      "Trained batch 543 batch loss 1.20718682 epoch total loss 1.1954217\n",
      "Trained batch 544 batch loss 1.18092978 epoch total loss 1.19539499\n",
      "Trained batch 545 batch loss 1.20432401 epoch total loss 1.19541144\n",
      "Trained batch 546 batch loss 1.13510597 epoch total loss 1.19530094\n",
      "Trained batch 547 batch loss 1.15335321 epoch total loss 1.1952244\n",
      "Trained batch 548 batch loss 1.13935518 epoch total loss 1.19512236\n",
      "Trained batch 549 batch loss 1.16604424 epoch total loss 1.19506931\n",
      "Trained batch 550 batch loss 1.13759136 epoch total loss 1.19496477\n",
      "Trained batch 551 batch loss 1.07516849 epoch total loss 1.19474745\n",
      "Trained batch 552 batch loss 1.22331202 epoch total loss 1.19479918\n",
      "Trained batch 553 batch loss 1.15770054 epoch total loss 1.19473219\n",
      "Trained batch 554 batch loss 1.157727 epoch total loss 1.19466531\n",
      "Trained batch 555 batch loss 1.29866195 epoch total loss 1.19485271\n",
      "Trained batch 556 batch loss 1.11938286 epoch total loss 1.19471693\n",
      "Trained batch 557 batch loss 1.2046361 epoch total loss 1.19473481\n",
      "Trained batch 558 batch loss 1.15335298 epoch total loss 1.19466066\n",
      "Trained batch 559 batch loss 1.3280127 epoch total loss 1.1948992\n",
      "Trained batch 560 batch loss 1.15567744 epoch total loss 1.19482923\n",
      "Trained batch 561 batch loss 1.21531165 epoch total loss 1.19486582\n",
      "Trained batch 562 batch loss 1.19401133 epoch total loss 1.19486427\n",
      "Trained batch 563 batch loss 1.12803328 epoch total loss 1.19474566\n",
      "Trained batch 564 batch loss 1.17239296 epoch total loss 1.19470596\n",
      "Trained batch 565 batch loss 1.19951236 epoch total loss 1.19471443\n",
      "Trained batch 566 batch loss 1.07309556 epoch total loss 1.19449961\n",
      "Trained batch 567 batch loss 1.1922828 epoch total loss 1.19449568\n",
      "Trained batch 568 batch loss 1.23133469 epoch total loss 1.19456053\n",
      "Trained batch 569 batch loss 1.19781971 epoch total loss 1.19456625\n",
      "Trained batch 570 batch loss 1.07355011 epoch total loss 1.19435394\n",
      "Trained batch 571 batch loss 1.12142682 epoch total loss 1.19422615\n",
      "Trained batch 572 batch loss 1.12584901 epoch total loss 1.19410658\n",
      "Trained batch 573 batch loss 1.21248484 epoch total loss 1.19413865\n",
      "Trained batch 574 batch loss 1.32034504 epoch total loss 1.19435859\n",
      "Trained batch 575 batch loss 1.12869394 epoch total loss 1.19424438\n",
      "Trained batch 576 batch loss 1.05543137 epoch total loss 1.19400346\n",
      "Trained batch 577 batch loss 1.10310507 epoch total loss 1.19384587\n",
      "Trained batch 578 batch loss 1.21388412 epoch total loss 1.19388044\n",
      "Trained batch 579 batch loss 1.16120243 epoch total loss 1.19382405\n",
      "Trained batch 580 batch loss 1.22616267 epoch total loss 1.19387972\n",
      "Trained batch 581 batch loss 1.25317311 epoch total loss 1.19398177\n",
      "Trained batch 582 batch loss 1.08301377 epoch total loss 1.19379115\n",
      "Trained batch 583 batch loss 1.04535246 epoch total loss 1.19353652\n",
      "Trained batch 584 batch loss 1.16946137 epoch total loss 1.19349527\n",
      "Trained batch 585 batch loss 1.14375877 epoch total loss 1.19341016\n",
      "Trained batch 586 batch loss 1.1047864 epoch total loss 1.193259\n",
      "Trained batch 587 batch loss 1.24628925 epoch total loss 1.19334924\n",
      "Trained batch 588 batch loss 1.1849556 epoch total loss 1.19333494\n",
      "Trained batch 589 batch loss 1.07712102 epoch total loss 1.19313776\n",
      "Trained batch 590 batch loss 1.15357125 epoch total loss 1.19307065\n",
      "Trained batch 591 batch loss 1.17411947 epoch total loss 1.19303858\n",
      "Trained batch 592 batch loss 1.23688924 epoch total loss 1.19311261\n",
      "Trained batch 593 batch loss 1.1379056 epoch total loss 1.19301951\n",
      "Trained batch 594 batch loss 1.26431763 epoch total loss 1.19313955\n",
      "Trained batch 595 batch loss 1.14273643 epoch total loss 1.19305491\n",
      "Trained batch 596 batch loss 1.30246603 epoch total loss 1.1932385\n",
      "Trained batch 597 batch loss 1.30604172 epoch total loss 1.19342744\n",
      "Trained batch 598 batch loss 1.26315379 epoch total loss 1.19354415\n",
      "Trained batch 599 batch loss 1.39844346 epoch total loss 1.19388616\n",
      "Trained batch 600 batch loss 1.35436058 epoch total loss 1.19415367\n",
      "Trained batch 601 batch loss 1.15677047 epoch total loss 1.19409144\n",
      "Trained batch 602 batch loss 1.157758 epoch total loss 1.19403112\n",
      "Trained batch 603 batch loss 1.19809198 epoch total loss 1.19403791\n",
      "Trained batch 604 batch loss 1.06631863 epoch total loss 1.19382656\n",
      "Trained batch 605 batch loss 1.08518159 epoch total loss 1.19364703\n",
      "Trained batch 606 batch loss 1.12842393 epoch total loss 1.19353938\n",
      "Trained batch 607 batch loss 1.11560011 epoch total loss 1.19341099\n",
      "Trained batch 608 batch loss 1.0321002 epoch total loss 1.19314563\n",
      "Trained batch 609 batch loss 0.984213233 epoch total loss 1.19280255\n",
      "Trained batch 610 batch loss 1.08113217 epoch total loss 1.19261944\n",
      "Trained batch 611 batch loss 1.10363984 epoch total loss 1.19247377\n",
      "Trained batch 612 batch loss 1.17436051 epoch total loss 1.19244421\n",
      "Trained batch 613 batch loss 1.09992588 epoch total loss 1.19229329\n",
      "Trained batch 614 batch loss 1.09514534 epoch total loss 1.1921351\n",
      "Trained batch 615 batch loss 1.16576171 epoch total loss 1.19209218\n",
      "Trained batch 616 batch loss 1.15745068 epoch total loss 1.19203603\n",
      "Trained batch 617 batch loss 1.12336135 epoch total loss 1.19192469\n",
      "Trained batch 618 batch loss 1.1469357 epoch total loss 1.19185185\n",
      "Trained batch 619 batch loss 1.04997098 epoch total loss 1.19162273\n",
      "Trained batch 620 batch loss 1.06660223 epoch total loss 1.19142103\n",
      "Trained batch 621 batch loss 1.15796304 epoch total loss 1.19136715\n",
      "Trained batch 622 batch loss 1.20996547 epoch total loss 1.19139707\n",
      "Trained batch 623 batch loss 1.18168092 epoch total loss 1.19138145\n",
      "Trained batch 624 batch loss 1.4328866 epoch total loss 1.19176841\n",
      "Trained batch 625 batch loss 1.36217618 epoch total loss 1.19204116\n",
      "Trained batch 626 batch loss 1.31253314 epoch total loss 1.19223368\n",
      "Trained batch 627 batch loss 1.19449592 epoch total loss 1.19223726\n",
      "Trained batch 628 batch loss 1.15874541 epoch total loss 1.19218397\n",
      "Trained batch 629 batch loss 1.18547869 epoch total loss 1.19217336\n",
      "Trained batch 630 batch loss 1.19375253 epoch total loss 1.19217575\n",
      "Trained batch 631 batch loss 1.21510983 epoch total loss 1.1922121\n",
      "Trained batch 632 batch loss 1.12094831 epoch total loss 1.19209933\n",
      "Trained batch 633 batch loss 1.04121423 epoch total loss 1.19186103\n",
      "Trained batch 634 batch loss 1.06151843 epoch total loss 1.1916554\n",
      "Trained batch 635 batch loss 1.15037799 epoch total loss 1.19159043\n",
      "Trained batch 636 batch loss 1.25248742 epoch total loss 1.19168615\n",
      "Trained batch 637 batch loss 1.34820008 epoch total loss 1.19193184\n",
      "Trained batch 638 batch loss 1.33077216 epoch total loss 1.19214952\n",
      "Trained batch 639 batch loss 1.39936638 epoch total loss 1.19247377\n",
      "Trained batch 640 batch loss 1.28352666 epoch total loss 1.19261599\n",
      "Trained batch 641 batch loss 1.31688714 epoch total loss 1.19280982\n",
      "Trained batch 642 batch loss 1.21709394 epoch total loss 1.19284773\n",
      "Trained batch 643 batch loss 1.09017682 epoch total loss 1.19268799\n",
      "Trained batch 644 batch loss 1.11865354 epoch total loss 1.19257307\n",
      "Trained batch 645 batch loss 1.22542071 epoch total loss 1.19262397\n",
      "Trained batch 646 batch loss 1.17421186 epoch total loss 1.19259536\n",
      "Trained batch 647 batch loss 1.20938718 epoch total loss 1.19262135\n",
      "Trained batch 648 batch loss 1.19181013 epoch total loss 1.19262016\n",
      "Trained batch 649 batch loss 1.19813967 epoch total loss 1.19262862\n",
      "Trained batch 650 batch loss 1.21830845 epoch total loss 1.1926682\n",
      "Trained batch 651 batch loss 1.11740267 epoch total loss 1.19255257\n",
      "Trained batch 652 batch loss 1.39982343 epoch total loss 1.1928705\n",
      "Trained batch 653 batch loss 1.36040318 epoch total loss 1.19312716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 654 batch loss 1.35844302 epoch total loss 1.19337988\n",
      "Trained batch 655 batch loss 1.3917917 epoch total loss 1.19368279\n",
      "Trained batch 656 batch loss 1.30332971 epoch total loss 1.19385\n",
      "Trained batch 657 batch loss 1.13848209 epoch total loss 1.19376576\n",
      "Trained batch 658 batch loss 1.26462746 epoch total loss 1.19387341\n",
      "Trained batch 659 batch loss 1.30643284 epoch total loss 1.19404423\n",
      "Trained batch 660 batch loss 1.25277472 epoch total loss 1.19413328\n",
      "Trained batch 661 batch loss 1.21551573 epoch total loss 1.19416559\n",
      "Trained batch 662 batch loss 1.18922424 epoch total loss 1.19415808\n",
      "Trained batch 663 batch loss 1.22721684 epoch total loss 1.19420803\n",
      "Trained batch 664 batch loss 1.13737309 epoch total loss 1.19412243\n",
      "Trained batch 665 batch loss 1.1389389 epoch total loss 1.19403934\n",
      "Trained batch 666 batch loss 1.02936935 epoch total loss 1.1937921\n",
      "Trained batch 667 batch loss 1.0324924 epoch total loss 1.19355023\n",
      "Trained batch 668 batch loss 1.09164286 epoch total loss 1.19339764\n",
      "Trained batch 669 batch loss 1.21124148 epoch total loss 1.19342434\n",
      "Trained batch 670 batch loss 1.31145096 epoch total loss 1.19360054\n",
      "Trained batch 671 batch loss 1.3238709 epoch total loss 1.19379461\n",
      "Trained batch 672 batch loss 1.41176689 epoch total loss 1.19411898\n",
      "Trained batch 673 batch loss 1.50228977 epoch total loss 1.19457686\n",
      "Trained batch 674 batch loss 1.3887527 epoch total loss 1.19486499\n",
      "Trained batch 675 batch loss 1.20786381 epoch total loss 1.1948843\n",
      "Trained batch 676 batch loss 1.36606383 epoch total loss 1.1951375\n",
      "Trained batch 677 batch loss 1.3013351 epoch total loss 1.19529438\n",
      "Trained batch 678 batch loss 1.27351665 epoch total loss 1.19540977\n",
      "Trained batch 679 batch loss 1.26054013 epoch total loss 1.19550562\n",
      "Trained batch 680 batch loss 1.16906118 epoch total loss 1.19546676\n",
      "Trained batch 681 batch loss 1.29288626 epoch total loss 1.19560993\n",
      "Trained batch 682 batch loss 1.18678 epoch total loss 1.19559693\n",
      "Trained batch 683 batch loss 1.2296977 epoch total loss 1.19564676\n",
      "Trained batch 684 batch loss 1.26974797 epoch total loss 1.19575512\n",
      "Trained batch 685 batch loss 1.26682007 epoch total loss 1.19585896\n",
      "Trained batch 686 batch loss 1.28761172 epoch total loss 1.19599271\n",
      "Trained batch 687 batch loss 1.29166579 epoch total loss 1.19613194\n",
      "Trained batch 688 batch loss 1.21083724 epoch total loss 1.19615328\n",
      "Trained batch 689 batch loss 1.20390868 epoch total loss 1.19616461\n",
      "Trained batch 690 batch loss 1.04253697 epoch total loss 1.19594193\n",
      "Trained batch 691 batch loss 1.1531918 epoch total loss 1.19588\n",
      "Trained batch 692 batch loss 1.22178781 epoch total loss 1.19591761\n",
      "Trained batch 693 batch loss 1.17809439 epoch total loss 1.19589186\n",
      "Trained batch 694 batch loss 1.1924504 epoch total loss 1.19588685\n",
      "Trained batch 695 batch loss 1.27847064 epoch total loss 1.1960057\n",
      "Trained batch 696 batch loss 1.19480133 epoch total loss 1.19600391\n",
      "Trained batch 697 batch loss 1.0822469 epoch total loss 1.19584084\n",
      "Trained batch 698 batch loss 1.25060689 epoch total loss 1.19591928\n",
      "Trained batch 699 batch loss 1.04021168 epoch total loss 1.19569647\n",
      "Trained batch 700 batch loss 1.0441345 epoch total loss 1.19548\n",
      "Trained batch 701 batch loss 1.17461419 epoch total loss 1.19545019\n",
      "Trained batch 702 batch loss 1.01324332 epoch total loss 1.19519067\n",
      "Trained batch 703 batch loss 1.12893605 epoch total loss 1.19509637\n",
      "Trained batch 704 batch loss 1.03379774 epoch total loss 1.19486725\n",
      "Trained batch 705 batch loss 1.09864259 epoch total loss 1.19473076\n",
      "Trained batch 706 batch loss 1.08153903 epoch total loss 1.19457054\n",
      "Trained batch 707 batch loss 1.17747211 epoch total loss 1.19454634\n",
      "Trained batch 708 batch loss 1.29312491 epoch total loss 1.19468558\n",
      "Trained batch 709 batch loss 1.21353209 epoch total loss 1.19471216\n",
      "Trained batch 710 batch loss 1.14091361 epoch total loss 1.19463646\n",
      "Trained batch 711 batch loss 1.12943876 epoch total loss 1.19454479\n",
      "Trained batch 712 batch loss 1.15100336 epoch total loss 1.19448364\n",
      "Trained batch 713 batch loss 1.15573716 epoch total loss 1.19442928\n",
      "Trained batch 714 batch loss 1.0873661 epoch total loss 1.19427931\n",
      "Trained batch 715 batch loss 1.20034742 epoch total loss 1.19428778\n",
      "Trained batch 716 batch loss 1.32201898 epoch total loss 1.19446623\n",
      "Trained batch 717 batch loss 1.20837486 epoch total loss 1.19448555\n",
      "Trained batch 718 batch loss 1.26524937 epoch total loss 1.19458413\n",
      "Trained batch 719 batch loss 1.30449867 epoch total loss 1.19473708\n",
      "Trained batch 720 batch loss 1.17152846 epoch total loss 1.19470477\n",
      "Trained batch 721 batch loss 1.32135725 epoch total loss 1.19488049\n",
      "Trained batch 722 batch loss 1.21614909 epoch total loss 1.19490981\n",
      "Trained batch 723 batch loss 1.09344614 epoch total loss 1.1947695\n",
      "Trained batch 724 batch loss 1.17697883 epoch total loss 1.19474494\n",
      "Trained batch 725 batch loss 1.28161108 epoch total loss 1.19486475\n",
      "Trained batch 726 batch loss 1.31152987 epoch total loss 1.19502544\n",
      "Trained batch 727 batch loss 1.28953946 epoch total loss 1.1951555\n",
      "Trained batch 728 batch loss 1.20908904 epoch total loss 1.19517469\n",
      "Trained batch 729 batch loss 1.13728642 epoch total loss 1.19509518\n",
      "Trained batch 730 batch loss 1.27106261 epoch total loss 1.19519925\n",
      "Trained batch 731 batch loss 1.18862367 epoch total loss 1.19519031\n",
      "Trained batch 732 batch loss 1.23134601 epoch total loss 1.19523966\n",
      "Trained batch 733 batch loss 1.29722297 epoch total loss 1.19537878\n",
      "Trained batch 734 batch loss 1.29818749 epoch total loss 1.19551885\n",
      "Trained batch 735 batch loss 1.25047398 epoch total loss 1.19559371\n",
      "Trained batch 736 batch loss 1.16017032 epoch total loss 1.19554555\n",
      "Trained batch 737 batch loss 1.02514911 epoch total loss 1.19531429\n",
      "Trained batch 738 batch loss 1.09136701 epoch total loss 1.1951735\n",
      "Trained batch 739 batch loss 1.09647703 epoch total loss 1.19504\n",
      "Trained batch 740 batch loss 1.12797403 epoch total loss 1.19494939\n",
      "Trained batch 741 batch loss 1.11326897 epoch total loss 1.19483912\n",
      "Trained batch 742 batch loss 1.10044432 epoch total loss 1.19471192\n",
      "Trained batch 743 batch loss 1.14795971 epoch total loss 1.19464898\n",
      "Trained batch 744 batch loss 1.23627007 epoch total loss 1.19470489\n",
      "Trained batch 745 batch loss 1.29227889 epoch total loss 1.1948359\n",
      "Trained batch 746 batch loss 1.30040598 epoch total loss 1.1949774\n",
      "Trained batch 747 batch loss 1.27988935 epoch total loss 1.19509113\n",
      "Trained batch 748 batch loss 1.18192053 epoch total loss 1.1950736\n",
      "Trained batch 749 batch loss 1.17085505 epoch total loss 1.19504118\n",
      "Trained batch 750 batch loss 1.03564465 epoch total loss 1.19482875\n",
      "Trained batch 751 batch loss 1.18938422 epoch total loss 1.19482148\n",
      "Trained batch 752 batch loss 1.14467812 epoch total loss 1.19475472\n",
      "Trained batch 753 batch loss 1.35392833 epoch total loss 1.1949662\n",
      "Trained batch 754 batch loss 1.23329258 epoch total loss 1.19501698\n",
      "Trained batch 755 batch loss 1.19808948 epoch total loss 1.19502103\n",
      "Trained batch 756 batch loss 1.19137239 epoch total loss 1.19501615\n",
      "Trained batch 757 batch loss 1.1489222 epoch total loss 1.19495523\n",
      "Trained batch 758 batch loss 1.18086672 epoch total loss 1.19493663\n",
      "Trained batch 759 batch loss 1.10014963 epoch total loss 1.1948117\n",
      "Trained batch 760 batch loss 1.13954532 epoch total loss 1.19473898\n",
      "Trained batch 761 batch loss 1.27967525 epoch total loss 1.19485056\n",
      "Trained batch 762 batch loss 1.18458271 epoch total loss 1.19483709\n",
      "Trained batch 763 batch loss 1.24612153 epoch total loss 1.19490433\n",
      "Trained batch 764 batch loss 1.31326282 epoch total loss 1.19505918\n",
      "Trained batch 765 batch loss 1.12659633 epoch total loss 1.19496965\n",
      "Trained batch 766 batch loss 1.20765734 epoch total loss 1.19498622\n",
      "Trained batch 767 batch loss 1.14924192 epoch total loss 1.1949265\n",
      "Trained batch 768 batch loss 1.18832147 epoch total loss 1.19491792\n",
      "Trained batch 769 batch loss 1.05623364 epoch total loss 1.19473755\n",
      "Trained batch 770 batch loss 1.10188103 epoch total loss 1.19461691\n",
      "Trained batch 771 batch loss 1.19365335 epoch total loss 1.19461572\n",
      "Trained batch 772 batch loss 1.3476336 epoch total loss 1.19481397\n",
      "Trained batch 773 batch loss 1.32327652 epoch total loss 1.19498014\n",
      "Trained batch 774 batch loss 1.42754865 epoch total loss 1.19528067\n",
      "Trained batch 775 batch loss 1.38393235 epoch total loss 1.19552398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 776 batch loss 1.25777149 epoch total loss 1.19560421\n",
      "Trained batch 777 batch loss 1.13695312 epoch total loss 1.19552875\n",
      "Trained batch 778 batch loss 1.00036454 epoch total loss 1.19527793\n",
      "Trained batch 779 batch loss 0.963018954 epoch total loss 1.19497979\n",
      "Trained batch 780 batch loss 0.987027109 epoch total loss 1.19471312\n",
      "Trained batch 781 batch loss 1.03166759 epoch total loss 1.19450438\n",
      "Trained batch 782 batch loss 1.14949822 epoch total loss 1.1944468\n",
      "Trained batch 783 batch loss 1.34594166 epoch total loss 1.19464028\n",
      "Trained batch 784 batch loss 1.41988802 epoch total loss 1.19492757\n",
      "Trained batch 785 batch loss 1.27153349 epoch total loss 1.19502509\n",
      "Trained batch 786 batch loss 1.22409606 epoch total loss 1.19506216\n",
      "Trained batch 787 batch loss 1.23819828 epoch total loss 1.195117\n",
      "Trained batch 788 batch loss 1.20720804 epoch total loss 1.19513237\n",
      "Trained batch 789 batch loss 1.080055 epoch total loss 1.19498646\n",
      "Trained batch 790 batch loss 1.24315357 epoch total loss 1.1950475\n",
      "Trained batch 791 batch loss 1.19183338 epoch total loss 1.19504344\n",
      "Trained batch 792 batch loss 1.11748171 epoch total loss 1.19494557\n",
      "Trained batch 793 batch loss 1.11992073 epoch total loss 1.19485092\n",
      "Trained batch 794 batch loss 0.978918374 epoch total loss 1.19457901\n",
      "Trained batch 795 batch loss 1.03897738 epoch total loss 1.19438326\n",
      "Trained batch 796 batch loss 1.13301182 epoch total loss 1.19430614\n",
      "Trained batch 797 batch loss 1.44015884 epoch total loss 1.19461465\n",
      "Trained batch 798 batch loss 1.39222157 epoch total loss 1.19486237\n",
      "Trained batch 799 batch loss 1.43536782 epoch total loss 1.19516337\n",
      "Trained batch 800 batch loss 1.24061859 epoch total loss 1.19522011\n",
      "Trained batch 801 batch loss 1.30527329 epoch total loss 1.19535756\n",
      "Trained batch 802 batch loss 1.43046904 epoch total loss 1.1956507\n",
      "Trained batch 803 batch loss 1.22418082 epoch total loss 1.19568622\n",
      "Trained batch 804 batch loss 1.14409494 epoch total loss 1.19562209\n",
      "Trained batch 805 batch loss 1.15195441 epoch total loss 1.19556785\n",
      "Trained batch 806 batch loss 1.13183594 epoch total loss 1.19548881\n",
      "Trained batch 807 batch loss 1.13033044 epoch total loss 1.19540799\n",
      "Trained batch 808 batch loss 1.13350952 epoch total loss 1.19533134\n",
      "Trained batch 809 batch loss 1.21585178 epoch total loss 1.19535685\n",
      "Trained batch 810 batch loss 1.13162339 epoch total loss 1.19527817\n",
      "Trained batch 811 batch loss 1.10390472 epoch total loss 1.1951654\n",
      "Trained batch 812 batch loss 0.909705 epoch total loss 1.19481397\n",
      "Trained batch 813 batch loss 0.996600866 epoch total loss 1.19457006\n",
      "Trained batch 814 batch loss 1.19451845 epoch total loss 1.19457006\n",
      "Trained batch 815 batch loss 1.08513653 epoch total loss 1.19443572\n",
      "Trained batch 816 batch loss 1.21520638 epoch total loss 1.19446123\n",
      "Trained batch 817 batch loss 1.23221612 epoch total loss 1.19450748\n",
      "Trained batch 818 batch loss 1.20882273 epoch total loss 1.19452488\n",
      "Trained batch 819 batch loss 1.05187047 epoch total loss 1.19435072\n",
      "Trained batch 820 batch loss 1.03870654 epoch total loss 1.19416094\n",
      "Trained batch 821 batch loss 1.1397059 epoch total loss 1.19409466\n",
      "Trained batch 822 batch loss 1.15154445 epoch total loss 1.19404292\n",
      "Trained batch 823 batch loss 1.19269252 epoch total loss 1.19404125\n",
      "Trained batch 824 batch loss 1.07980049 epoch total loss 1.19390249\n",
      "Trained batch 825 batch loss 1.33507919 epoch total loss 1.19407368\n",
      "Trained batch 826 batch loss 1.0613693 epoch total loss 1.19391298\n",
      "Trained batch 827 batch loss 1.23214483 epoch total loss 1.19395912\n",
      "Trained batch 828 batch loss 1.31982899 epoch total loss 1.19411123\n",
      "Trained batch 829 batch loss 1.16837168 epoch total loss 1.19408011\n",
      "Trained batch 830 batch loss 1.04452527 epoch total loss 1.1939\n",
      "Trained batch 831 batch loss 1.28638661 epoch total loss 1.19401133\n",
      "Trained batch 832 batch loss 1.40532017 epoch total loss 1.19426525\n",
      "Trained batch 833 batch loss 1.22597206 epoch total loss 1.19430327\n",
      "Trained batch 834 batch loss 1.04338861 epoch total loss 1.19412243\n",
      "Trained batch 835 batch loss 1.0155673 epoch total loss 1.19390857\n",
      "Trained batch 836 batch loss 1.05797958 epoch total loss 1.19374597\n",
      "Trained batch 837 batch loss 1.01582646 epoch total loss 1.19353342\n",
      "Trained batch 838 batch loss 1.03518534 epoch total loss 1.19334435\n",
      "Trained batch 839 batch loss 1.07714796 epoch total loss 1.19320583\n",
      "Trained batch 840 batch loss 1.10359204 epoch total loss 1.19309914\n",
      "Trained batch 841 batch loss 1.08093321 epoch total loss 1.19296575\n",
      "Trained batch 842 batch loss 1.09773803 epoch total loss 1.19285274\n",
      "Trained batch 843 batch loss 1.16074431 epoch total loss 1.19281459\n",
      "Trained batch 844 batch loss 1.27028143 epoch total loss 1.19290638\n",
      "Trained batch 845 batch loss 1.11427557 epoch total loss 1.19281328\n",
      "Trained batch 846 batch loss 1.25402677 epoch total loss 1.19288564\n",
      "Trained batch 847 batch loss 1.1819911 epoch total loss 1.19287288\n",
      "Trained batch 848 batch loss 1.16178119 epoch total loss 1.19283617\n",
      "Trained batch 849 batch loss 1.02876115 epoch total loss 1.19264293\n",
      "Trained batch 850 batch loss 1.04771829 epoch total loss 1.19247246\n",
      "Trained batch 851 batch loss 1.04034567 epoch total loss 1.19229364\n",
      "Trained batch 852 batch loss 1.05800807 epoch total loss 1.19213605\n",
      "Trained batch 853 batch loss 1.07488871 epoch total loss 1.1919986\n",
      "Trained batch 854 batch loss 1.09419096 epoch total loss 1.19188404\n",
      "Trained batch 855 batch loss 1.04393566 epoch total loss 1.19171095\n",
      "Trained batch 856 batch loss 1.09612215 epoch total loss 1.19159937\n",
      "Trained batch 857 batch loss 1.08388782 epoch total loss 1.1914736\n",
      "Trained batch 858 batch loss 1.03329694 epoch total loss 1.19128931\n",
      "Trained batch 859 batch loss 1.25266743 epoch total loss 1.19136083\n",
      "Trained batch 860 batch loss 1.35639095 epoch total loss 1.19155276\n",
      "Trained batch 861 batch loss 1.12835479 epoch total loss 1.19147921\n",
      "Trained batch 862 batch loss 1.2275492 epoch total loss 1.19152105\n",
      "Trained batch 863 batch loss 1.42415583 epoch total loss 1.1917907\n",
      "Trained batch 864 batch loss 1.38488054 epoch total loss 1.19201422\n",
      "Trained batch 865 batch loss 1.18952298 epoch total loss 1.19201136\n",
      "Trained batch 866 batch loss 1.21590745 epoch total loss 1.19203901\n",
      "Trained batch 867 batch loss 1.06568623 epoch total loss 1.19189322\n",
      "Trained batch 868 batch loss 1.22963321 epoch total loss 1.19193673\n",
      "Trained batch 869 batch loss 1.15398562 epoch total loss 1.19189298\n",
      "Trained batch 870 batch loss 1.0955255 epoch total loss 1.19178224\n",
      "Trained batch 871 batch loss 1.06315613 epoch total loss 1.19163454\n",
      "Trained batch 872 batch loss 1.21721411 epoch total loss 1.19166386\n",
      "Trained batch 873 batch loss 1.21838164 epoch total loss 1.19169438\n",
      "Trained batch 874 batch loss 1.39559746 epoch total loss 1.19192779\n",
      "Trained batch 875 batch loss 1.40683293 epoch total loss 1.19217336\n",
      "Trained batch 876 batch loss 1.27108657 epoch total loss 1.19226348\n",
      "Trained batch 877 batch loss 1.24821854 epoch total loss 1.19232726\n",
      "Trained batch 878 batch loss 1.27865565 epoch total loss 1.19242561\n",
      "Trained batch 879 batch loss 1.20849764 epoch total loss 1.19244397\n",
      "Trained batch 880 batch loss 1.26871479 epoch total loss 1.19253051\n",
      "Trained batch 881 batch loss 1.22035539 epoch total loss 1.1925621\n",
      "Trained batch 882 batch loss 1.19126213 epoch total loss 1.19256067\n",
      "Trained batch 883 batch loss 1.1890614 epoch total loss 1.19255674\n",
      "Trained batch 884 batch loss 1.26065731 epoch total loss 1.19263375\n",
      "Trained batch 885 batch loss 1.22355151 epoch total loss 1.19266856\n",
      "Trained batch 886 batch loss 1.11864507 epoch total loss 1.19258511\n",
      "Trained batch 887 batch loss 1.08942604 epoch total loss 1.19246888\n",
      "Trained batch 888 batch loss 1.15530205 epoch total loss 1.19242692\n",
      "Trained batch 889 batch loss 1.1242404 epoch total loss 1.19235027\n",
      "Trained batch 890 batch loss 1.14838886 epoch total loss 1.19230092\n",
      "Trained batch 891 batch loss 1.02686453 epoch total loss 1.19211519\n",
      "Trained batch 892 batch loss 1.0874474 epoch total loss 1.19199789\n",
      "Trained batch 893 batch loss 1.18990171 epoch total loss 1.1919955\n",
      "Trained batch 894 batch loss 1.28867507 epoch total loss 1.19210374\n",
      "Trained batch 895 batch loss 1.32001674 epoch total loss 1.19224668\n",
      "Trained batch 896 batch loss 1.24633801 epoch total loss 1.19230711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 897 batch loss 1.10310316 epoch total loss 1.19220769\n",
      "Trained batch 898 batch loss 1.0936116 epoch total loss 1.1920979\n",
      "Trained batch 899 batch loss 1.13687658 epoch total loss 1.19203639\n",
      "Trained batch 900 batch loss 1.06885123 epoch total loss 1.19189954\n",
      "Trained batch 901 batch loss 1.19427741 epoch total loss 1.19190228\n",
      "Trained batch 902 batch loss 1.27648926 epoch total loss 1.19199598\n",
      "Trained batch 903 batch loss 1.26211238 epoch total loss 1.1920737\n",
      "Trained batch 904 batch loss 1.21469152 epoch total loss 1.19209874\n",
      "Trained batch 905 batch loss 1.24584758 epoch total loss 1.1921581\n",
      "Trained batch 906 batch loss 1.11339176 epoch total loss 1.1920712\n",
      "Trained batch 907 batch loss 1.20711827 epoch total loss 1.19208777\n",
      "Trained batch 908 batch loss 1.20765066 epoch total loss 1.19210494\n",
      "Trained batch 909 batch loss 1.24958658 epoch total loss 1.19216824\n",
      "Trained batch 910 batch loss 1.19375825 epoch total loss 1.1921699\n",
      "Trained batch 911 batch loss 1.30695784 epoch total loss 1.19229603\n",
      "Trained batch 912 batch loss 1.40450287 epoch total loss 1.19252872\n",
      "Trained batch 913 batch loss 1.28044212 epoch total loss 1.19262493\n",
      "Trained batch 914 batch loss 1.25977325 epoch total loss 1.19269836\n",
      "Trained batch 915 batch loss 1.0409956 epoch total loss 1.19253266\n",
      "Trained batch 916 batch loss 1.28178358 epoch total loss 1.19263\n",
      "Trained batch 917 batch loss 1.20893478 epoch total loss 1.19264781\n",
      "Trained batch 918 batch loss 1.09053969 epoch total loss 1.19253671\n",
      "Trained batch 919 batch loss 1.07482851 epoch total loss 1.19240856\n",
      "Trained batch 920 batch loss 1.01460779 epoch total loss 1.19221532\n",
      "Trained batch 921 batch loss 1.12960029 epoch total loss 1.19214737\n",
      "Trained batch 922 batch loss 1.10388327 epoch total loss 1.19205165\n",
      "Trained batch 923 batch loss 1.00633478 epoch total loss 1.19185054\n",
      "Trained batch 924 batch loss 1.08479989 epoch total loss 1.19173467\n",
      "Trained batch 925 batch loss 1.18441105 epoch total loss 1.1917268\n",
      "Trained batch 926 batch loss 1.18988538 epoch total loss 1.1917249\n",
      "Trained batch 927 batch loss 1.17131591 epoch total loss 1.19170284\n",
      "Trained batch 928 batch loss 1.11476243 epoch total loss 1.19161987\n",
      "Trained batch 929 batch loss 1.14730942 epoch total loss 1.19157219\n",
      "Trained batch 930 batch loss 1.325423 epoch total loss 1.19171619\n",
      "Trained batch 931 batch loss 1.24829352 epoch total loss 1.19177687\n",
      "Trained batch 932 batch loss 1.18058157 epoch total loss 1.19176483\n",
      "Trained batch 933 batch loss 1.02961457 epoch total loss 1.19159114\n",
      "Trained batch 934 batch loss 0.975346088 epoch total loss 1.19135964\n",
      "Trained batch 935 batch loss 1.0110985 epoch total loss 1.19116676\n",
      "Trained batch 936 batch loss 1.15424263 epoch total loss 1.19112742\n",
      "Trained batch 937 batch loss 1.35669768 epoch total loss 1.19130409\n",
      "Trained batch 938 batch loss 1.53441978 epoch total loss 1.19167\n",
      "Trained batch 939 batch loss 1.32434893 epoch total loss 1.1918112\n",
      "Trained batch 940 batch loss 1.14445746 epoch total loss 1.19176078\n",
      "Trained batch 941 batch loss 1.17720783 epoch total loss 1.1917454\n",
      "Trained batch 942 batch loss 1.26307774 epoch total loss 1.1918211\n",
      "Trained batch 943 batch loss 1.19293785 epoch total loss 1.19182229\n",
      "Trained batch 944 batch loss 1.24773669 epoch total loss 1.19188142\n",
      "Trained batch 945 batch loss 1.21944785 epoch total loss 1.19191062\n",
      "Trained batch 946 batch loss 1.31250513 epoch total loss 1.19203818\n",
      "Trained batch 947 batch loss 1.19938731 epoch total loss 1.19204581\n",
      "Trained batch 948 batch loss 1.21450758 epoch total loss 1.19206953\n",
      "Trained batch 949 batch loss 1.09517407 epoch total loss 1.19196749\n",
      "Trained batch 950 batch loss 1.13970399 epoch total loss 1.19191241\n",
      "Trained batch 951 batch loss 1.12694049 epoch total loss 1.19184411\n",
      "Trained batch 952 batch loss 1.16636765 epoch total loss 1.19181728\n",
      "Trained batch 953 batch loss 1.05834293 epoch total loss 1.19167733\n",
      "Trained batch 954 batch loss 1.13802445 epoch total loss 1.19162107\n",
      "Trained batch 955 batch loss 1.13111377 epoch total loss 1.19155777\n",
      "Trained batch 956 batch loss 1.04978156 epoch total loss 1.19140947\n",
      "Trained batch 957 batch loss 1.26201916 epoch total loss 1.19148314\n",
      "Trained batch 958 batch loss 1.26949167 epoch total loss 1.19156468\n",
      "Trained batch 959 batch loss 1.20261943 epoch total loss 1.19157612\n",
      "Trained batch 960 batch loss 1.19187629 epoch total loss 1.19157648\n",
      "Trained batch 961 batch loss 1.11434746 epoch total loss 1.19149613\n",
      "Trained batch 962 batch loss 1.11079371 epoch total loss 1.19141233\n",
      "Trained batch 963 batch loss 1.08215427 epoch total loss 1.19129884\n",
      "Trained batch 964 batch loss 1.08570802 epoch total loss 1.19118929\n",
      "Trained batch 965 batch loss 1.06467283 epoch total loss 1.19105828\n",
      "Trained batch 966 batch loss 1.17211342 epoch total loss 1.19103861\n",
      "Trained batch 967 batch loss 1.11271942 epoch total loss 1.19095767\n",
      "Trained batch 968 batch loss 1.01077497 epoch total loss 1.19077146\n",
      "Trained batch 969 batch loss 1.10242641 epoch total loss 1.19068027\n",
      "Trained batch 970 batch loss 1.08160591 epoch total loss 1.19056785\n",
      "Trained batch 971 batch loss 1.12613082 epoch total loss 1.19050145\n",
      "Trained batch 972 batch loss 1.18154681 epoch total loss 1.19049227\n",
      "Trained batch 973 batch loss 1.20237064 epoch total loss 1.19050443\n",
      "Trained batch 974 batch loss 1.22310305 epoch total loss 1.19053793\n",
      "Trained batch 975 batch loss 1.24878776 epoch total loss 1.19059765\n",
      "Trained batch 976 batch loss 1.16413558 epoch total loss 1.19057059\n",
      "Trained batch 977 batch loss 1.33463228 epoch total loss 1.19071805\n",
      "Trained batch 978 batch loss 1.1820755 epoch total loss 1.19070923\n",
      "Trained batch 979 batch loss 1.19322252 epoch total loss 1.19071186\n",
      "Trained batch 980 batch loss 1.17018354 epoch total loss 1.19069088\n",
      "Trained batch 981 batch loss 1.07230806 epoch total loss 1.19057012\n",
      "Trained batch 982 batch loss 1.02159488 epoch total loss 1.1903981\n",
      "Trained batch 983 batch loss 1.08247733 epoch total loss 1.19028842\n",
      "Trained batch 984 batch loss 1.20809507 epoch total loss 1.19030654\n",
      "Trained batch 985 batch loss 1.24128985 epoch total loss 1.19035828\n",
      "Trained batch 986 batch loss 1.37325764 epoch total loss 1.19054389\n",
      "Trained batch 987 batch loss 1.33462811 epoch total loss 1.1906898\n",
      "Trained batch 988 batch loss 1.25056219 epoch total loss 1.19075048\n",
      "Trained batch 989 batch loss 1.3092463 epoch total loss 1.19087017\n",
      "Trained batch 990 batch loss 1.22055078 epoch total loss 1.19090021\n",
      "Trained batch 991 batch loss 1.20380902 epoch total loss 1.19091332\n",
      "Trained batch 992 batch loss 1.27529049 epoch total loss 1.19099832\n",
      "Trained batch 993 batch loss 1.21698093 epoch total loss 1.19102454\n",
      "Trained batch 994 batch loss 1.26002789 epoch total loss 1.19109392\n",
      "Trained batch 995 batch loss 1.39149785 epoch total loss 1.19129539\n",
      "Trained batch 996 batch loss 1.21155357 epoch total loss 1.19131565\n",
      "Trained batch 997 batch loss 1.27912688 epoch total loss 1.19140375\n",
      "Trained batch 998 batch loss 1.26695216 epoch total loss 1.19147956\n",
      "Trained batch 999 batch loss 1.20241761 epoch total loss 1.19149041\n",
      "Trained batch 1000 batch loss 1.33086145 epoch total loss 1.19162977\n",
      "Trained batch 1001 batch loss 1.16176581 epoch total loss 1.19159985\n",
      "Trained batch 1002 batch loss 1.11794984 epoch total loss 1.19152641\n",
      "Trained batch 1003 batch loss 1.11703742 epoch total loss 1.19145215\n",
      "Trained batch 1004 batch loss 1.09453475 epoch total loss 1.19135559\n",
      "Trained batch 1005 batch loss 1.21851027 epoch total loss 1.19138253\n",
      "Trained batch 1006 batch loss 1.24168861 epoch total loss 1.1914326\n",
      "Trained batch 1007 batch loss 1.10292339 epoch total loss 1.19134462\n",
      "Trained batch 1008 batch loss 1.17606235 epoch total loss 1.19132948\n",
      "Trained batch 1009 batch loss 1.17744446 epoch total loss 1.19131577\n",
      "Trained batch 1010 batch loss 1.04372764 epoch total loss 1.19116962\n",
      "Trained batch 1011 batch loss 1.22163153 epoch total loss 1.19119978\n",
      "Trained batch 1012 batch loss 1.10560274 epoch total loss 1.19111514\n",
      "Trained batch 1013 batch loss 1.03003049 epoch total loss 1.19095612\n",
      "Trained batch 1014 batch loss 1.21748328 epoch total loss 1.19098234\n",
      "Trained batch 1015 batch loss 1.18886197 epoch total loss 1.19098032\n",
      "Trained batch 1016 batch loss 1.20167494 epoch total loss 1.19099081\n",
      "Trained batch 1017 batch loss 1.15538597 epoch total loss 1.19095576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1018 batch loss 1.14569342 epoch total loss 1.19091141\n",
      "Trained batch 1019 batch loss 1.06639743 epoch total loss 1.19078922\n",
      "Trained batch 1020 batch loss 1.23532879 epoch total loss 1.19083285\n",
      "Trained batch 1021 batch loss 1.0987314 epoch total loss 1.19074273\n",
      "Trained batch 1022 batch loss 1.02826321 epoch total loss 1.19058383\n",
      "Trained batch 1023 batch loss 1.08473051 epoch total loss 1.19048023\n",
      "Trained batch 1024 batch loss 1.21140814 epoch total loss 1.19050074\n",
      "Trained batch 1025 batch loss 1.14995074 epoch total loss 1.19046116\n",
      "Trained batch 1026 batch loss 1.19506812 epoch total loss 1.19046557\n",
      "Trained batch 1027 batch loss 1.03124714 epoch total loss 1.1903106\n",
      "Trained batch 1028 batch loss 1.12655663 epoch total loss 1.19024861\n",
      "Trained batch 1029 batch loss 1.10239613 epoch total loss 1.19016325\n",
      "Trained batch 1030 batch loss 1.11120355 epoch total loss 1.1900866\n",
      "Trained batch 1031 batch loss 1.01160979 epoch total loss 1.18991351\n",
      "Trained batch 1032 batch loss 1.00834274 epoch total loss 1.18973744\n",
      "Trained batch 1033 batch loss 1.19340992 epoch total loss 1.18974102\n",
      "Trained batch 1034 batch loss 1.21279597 epoch total loss 1.18976331\n",
      "Trained batch 1035 batch loss 1.19137955 epoch total loss 1.18976486\n",
      "Trained batch 1036 batch loss 1.17148495 epoch total loss 1.18974721\n",
      "Trained batch 1037 batch loss 1.29252064 epoch total loss 1.18984628\n",
      "Trained batch 1038 batch loss 1.3276546 epoch total loss 1.18997908\n",
      "Trained batch 1039 batch loss 1.22596967 epoch total loss 1.19001365\n",
      "Trained batch 1040 batch loss 1.29578197 epoch total loss 1.19011533\n",
      "Trained batch 1041 batch loss 1.12519121 epoch total loss 1.19005299\n",
      "Trained batch 1042 batch loss 1.24202061 epoch total loss 1.19010293\n",
      "Trained batch 1043 batch loss 1.17545986 epoch total loss 1.19008887\n",
      "Trained batch 1044 batch loss 1.30917597 epoch total loss 1.19020295\n",
      "Trained batch 1045 batch loss 1.15040469 epoch total loss 1.19016492\n",
      "Trained batch 1046 batch loss 1.18342841 epoch total loss 1.19015849\n",
      "Trained batch 1047 batch loss 1.17008018 epoch total loss 1.19013929\n",
      "Trained batch 1048 batch loss 1.10724318 epoch total loss 1.19006026\n",
      "Trained batch 1049 batch loss 1.12312949 epoch total loss 1.18999648\n",
      "Trained batch 1050 batch loss 1.25676417 epoch total loss 1.19006\n",
      "Trained batch 1051 batch loss 1.35138333 epoch total loss 1.19021356\n",
      "Trained batch 1052 batch loss 1.28373408 epoch total loss 1.19030237\n",
      "Trained batch 1053 batch loss 1.22792745 epoch total loss 1.19033813\n",
      "Trained batch 1054 batch loss 1.23070717 epoch total loss 1.1903764\n",
      "Trained batch 1055 batch loss 1.32584012 epoch total loss 1.19050479\n",
      "Trained batch 1056 batch loss 1.30473328 epoch total loss 1.19061291\n",
      "Trained batch 1057 batch loss 1.20226026 epoch total loss 1.19062388\n",
      "Trained batch 1058 batch loss 1.14265358 epoch total loss 1.1905787\n",
      "Trained batch 1059 batch loss 1.16794133 epoch total loss 1.19055724\n",
      "Trained batch 1060 batch loss 1.08629119 epoch total loss 1.19045889\n",
      "Trained batch 1061 batch loss 1.13295519 epoch total loss 1.19040477\n",
      "Trained batch 1062 batch loss 1.10726368 epoch total loss 1.19032645\n",
      "Trained batch 1063 batch loss 1.22967827 epoch total loss 1.19036353\n",
      "Trained batch 1064 batch loss 1.21980166 epoch total loss 1.1903913\n",
      "Trained batch 1065 batch loss 1.1651181 epoch total loss 1.19036758\n",
      "Trained batch 1066 batch loss 1.1580801 epoch total loss 1.1903373\n",
      "Trained batch 1067 batch loss 1.14290822 epoch total loss 1.19029284\n",
      "Trained batch 1068 batch loss 1.18423057 epoch total loss 1.19028711\n",
      "Trained batch 1069 batch loss 1.30182791 epoch total loss 1.19039154\n",
      "Trained batch 1070 batch loss 1.37413192 epoch total loss 1.19056332\n",
      "Trained batch 1071 batch loss 1.15838134 epoch total loss 1.19053316\n",
      "Trained batch 1072 batch loss 1.17537081 epoch total loss 1.19051909\n",
      "Trained batch 1073 batch loss 1.15910542 epoch total loss 1.19048977\n",
      "Trained batch 1074 batch loss 1.20029664 epoch total loss 1.19049895\n",
      "Trained batch 1075 batch loss 1.26418066 epoch total loss 1.19056737\n",
      "Trained batch 1076 batch loss 1.14395213 epoch total loss 1.1905241\n",
      "Trained batch 1077 batch loss 1.1864624 epoch total loss 1.19052041\n",
      "Trained batch 1078 batch loss 1.30706275 epoch total loss 1.19062841\n",
      "Trained batch 1079 batch loss 1.31211925 epoch total loss 1.19074106\n",
      "Trained batch 1080 batch loss 1.2368145 epoch total loss 1.19078374\n",
      "Trained batch 1081 batch loss 1.26016378 epoch total loss 1.19084787\n",
      "Trained batch 1082 batch loss 1.2626617 epoch total loss 1.19091427\n",
      "Trained batch 1083 batch loss 1.3319478 epoch total loss 1.19104445\n",
      "Trained batch 1084 batch loss 1.11567044 epoch total loss 1.19097495\n",
      "Trained batch 1085 batch loss 1.18762732 epoch total loss 1.19097185\n",
      "Trained batch 1086 batch loss 1.1732558 epoch total loss 1.19095552\n",
      "Trained batch 1087 batch loss 1.15102 epoch total loss 1.1909188\n",
      "Trained batch 1088 batch loss 1.21262479 epoch total loss 1.19093871\n",
      "Trained batch 1089 batch loss 1.26129436 epoch total loss 1.19100344\n",
      "Trained batch 1090 batch loss 1.35552025 epoch total loss 1.19115424\n",
      "Trained batch 1091 batch loss 1.30067813 epoch total loss 1.19125462\n",
      "Trained batch 1092 batch loss 1.20319629 epoch total loss 1.19126558\n",
      "Trained batch 1093 batch loss 1.17678571 epoch total loss 1.19125235\n",
      "Trained batch 1094 batch loss 1.11662197 epoch total loss 1.19118404\n",
      "Trained batch 1095 batch loss 1.27217054 epoch total loss 1.19125807\n",
      "Trained batch 1096 batch loss 1.26949215 epoch total loss 1.19132948\n",
      "Trained batch 1097 batch loss 1.34500778 epoch total loss 1.19146955\n",
      "Trained batch 1098 batch loss 1.43808496 epoch total loss 1.19169414\n",
      "Trained batch 1099 batch loss 1.24939036 epoch total loss 1.19174671\n",
      "Trained batch 1100 batch loss 1.15053558 epoch total loss 1.19170916\n",
      "Trained batch 1101 batch loss 1.31525075 epoch total loss 1.19182146\n",
      "Trained batch 1102 batch loss 1.29296041 epoch total loss 1.19191325\n",
      "Trained batch 1103 batch loss 1.23896503 epoch total loss 1.19195592\n",
      "Trained batch 1104 batch loss 1.31217635 epoch total loss 1.19206476\n",
      "Trained batch 1105 batch loss 1.0706085 epoch total loss 1.19195485\n",
      "Trained batch 1106 batch loss 1.00281668 epoch total loss 1.19178379\n",
      "Trained batch 1107 batch loss 1.22132444 epoch total loss 1.19181049\n",
      "Trained batch 1108 batch loss 1.24756575 epoch total loss 1.1918608\n",
      "Trained batch 1109 batch loss 1.12775922 epoch total loss 1.1918031\n",
      "Trained batch 1110 batch loss 1.11895823 epoch total loss 1.19173753\n",
      "Trained batch 1111 batch loss 1.07800817 epoch total loss 1.19163513\n",
      "Trained batch 1112 batch loss 1.10870981 epoch total loss 1.19156063\n",
      "Trained batch 1113 batch loss 0.976092219 epoch total loss 1.19136703\n",
      "Trained batch 1114 batch loss 1.07057202 epoch total loss 1.19125855\n",
      "Trained batch 1115 batch loss 1.0780127 epoch total loss 1.19115698\n",
      "Trained batch 1116 batch loss 1.20090175 epoch total loss 1.19116569\n",
      "Trained batch 1117 batch loss 1.12754822 epoch total loss 1.19110882\n",
      "Trained batch 1118 batch loss 1.0447197 epoch total loss 1.19097781\n",
      "Trained batch 1119 batch loss 1.20591056 epoch total loss 1.19099116\n",
      "Trained batch 1120 batch loss 1.21731 epoch total loss 1.19101465\n",
      "Trained batch 1121 batch loss 1.18343866 epoch total loss 1.19100797\n",
      "Trained batch 1122 batch loss 1.30087066 epoch total loss 1.19110584\n",
      "Trained batch 1123 batch loss 1.30797613 epoch total loss 1.19120991\n",
      "Trained batch 1124 batch loss 1.16898346 epoch total loss 1.19119012\n",
      "Trained batch 1125 batch loss 1.09617388 epoch total loss 1.19110572\n",
      "Trained batch 1126 batch loss 1.23310649 epoch total loss 1.19114304\n",
      "Trained batch 1127 batch loss 1.16235566 epoch total loss 1.19111753\n",
      "Trained batch 1128 batch loss 1.3411144 epoch total loss 1.19125044\n",
      "Trained batch 1129 batch loss 1.34436798 epoch total loss 1.19138598\n",
      "Trained batch 1130 batch loss 1.43728209 epoch total loss 1.19160366\n",
      "Trained batch 1131 batch loss 1.20125639 epoch total loss 1.19161212\n",
      "Trained batch 1132 batch loss 1.10094213 epoch total loss 1.19153214\n",
      "Trained batch 1133 batch loss 1.13981736 epoch total loss 1.19148636\n",
      "Trained batch 1134 batch loss 1.23493409 epoch total loss 1.19152474\n",
      "Trained batch 1135 batch loss 1.22499061 epoch total loss 1.19155419\n",
      "Trained batch 1136 batch loss 1.23907399 epoch total loss 1.19159603\n",
      "Trained batch 1137 batch loss 1.24501157 epoch total loss 1.191643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1138 batch loss 1.3256942 epoch total loss 1.19176078\n",
      "Trained batch 1139 batch loss 1.37879515 epoch total loss 1.19192493\n",
      "Trained batch 1140 batch loss 1.31871557 epoch total loss 1.19203615\n",
      "Trained batch 1141 batch loss 1.27409267 epoch total loss 1.19210804\n",
      "Trained batch 1142 batch loss 1.20901811 epoch total loss 1.19212282\n",
      "Trained batch 1143 batch loss 1.07540023 epoch total loss 1.19202077\n",
      "Trained batch 1144 batch loss 1.23012328 epoch total loss 1.19205403\n",
      "Trained batch 1145 batch loss 1.30510235 epoch total loss 1.19215274\n",
      "Trained batch 1146 batch loss 1.093817 epoch total loss 1.19206703\n",
      "Trained batch 1147 batch loss 1.20549846 epoch total loss 1.19207859\n",
      "Trained batch 1148 batch loss 1.11424875 epoch total loss 1.19201088\n",
      "Trained batch 1149 batch loss 1.1819365 epoch total loss 1.19200206\n",
      "Trained batch 1150 batch loss 1.09408045 epoch total loss 1.19191694\n",
      "Trained batch 1151 batch loss 1.04146731 epoch total loss 1.19178629\n",
      "Trained batch 1152 batch loss 1.11467385 epoch total loss 1.19171929\n",
      "Trained batch 1153 batch loss 1.2454021 epoch total loss 1.19176579\n",
      "Trained batch 1154 batch loss 1.17639041 epoch total loss 1.19175243\n",
      "Trained batch 1155 batch loss 1.23508096 epoch total loss 1.19179\n",
      "Trained batch 1156 batch loss 1.17792571 epoch total loss 1.19177806\n",
      "Trained batch 1157 batch loss 1.37795854 epoch total loss 1.191939\n",
      "Trained batch 1158 batch loss 1.28830194 epoch total loss 1.1920222\n",
      "Trained batch 1159 batch loss 1.37062347 epoch total loss 1.19217622\n",
      "Trained batch 1160 batch loss 1.30017805 epoch total loss 1.19226933\n",
      "Trained batch 1161 batch loss 1.19265401 epoch total loss 1.19226968\n",
      "Trained batch 1162 batch loss 1.18220353 epoch total loss 1.1922611\n",
      "Trained batch 1163 batch loss 1.18031919 epoch total loss 1.19225073\n",
      "Trained batch 1164 batch loss 1.27051127 epoch total loss 1.19231796\n",
      "Trained batch 1165 batch loss 1.17634749 epoch total loss 1.19230437\n",
      "Trained batch 1166 batch loss 1.06217456 epoch total loss 1.19219267\n",
      "Trained batch 1167 batch loss 1.17150557 epoch total loss 1.19217491\n",
      "Trained batch 1168 batch loss 1.00137961 epoch total loss 1.19201159\n",
      "Trained batch 1169 batch loss 1.14585316 epoch total loss 1.19197214\n",
      "Trained batch 1170 batch loss 1.09834409 epoch total loss 1.19189215\n",
      "Trained batch 1171 batch loss 1.11730421 epoch total loss 1.19182849\n",
      "Trained batch 1172 batch loss 1.23147142 epoch total loss 1.19186223\n",
      "Trained batch 1173 batch loss 1.18835735 epoch total loss 1.19185925\n",
      "Trained batch 1174 batch loss 1.30163515 epoch total loss 1.19195271\n",
      "Trained batch 1175 batch loss 1.30048323 epoch total loss 1.19204521\n",
      "Trained batch 1176 batch loss 1.29233134 epoch total loss 1.19213045\n",
      "Trained batch 1177 batch loss 1.40036607 epoch total loss 1.19230747\n",
      "Trained batch 1178 batch loss 1.27271295 epoch total loss 1.19237566\n",
      "Trained batch 1179 batch loss 1.26418066 epoch total loss 1.19243658\n",
      "Trained batch 1180 batch loss 1.08141255 epoch total loss 1.19234252\n",
      "Trained batch 1181 batch loss 0.953983068 epoch total loss 1.19214058\n",
      "Trained batch 1182 batch loss 1.1172142 epoch total loss 1.19207728\n",
      "Trained batch 1183 batch loss 1.11294222 epoch total loss 1.19201028\n",
      "Trained batch 1184 batch loss 0.965251505 epoch total loss 1.19181871\n",
      "Trained batch 1185 batch loss 0.887861133 epoch total loss 1.19156218\n",
      "Trained batch 1186 batch loss 0.976493955 epoch total loss 1.19138086\n",
      "Trained batch 1187 batch loss 0.905093074 epoch total loss 1.1911397\n",
      "Trained batch 1188 batch loss 1.10985875 epoch total loss 1.19107127\n",
      "Trained batch 1189 batch loss 1.12170315 epoch total loss 1.19101298\n",
      "Trained batch 1190 batch loss 1.18705928 epoch total loss 1.19100952\n",
      "Trained batch 1191 batch loss 1.22774565 epoch total loss 1.1910404\n",
      "Trained batch 1192 batch loss 1.27088094 epoch total loss 1.19110739\n",
      "Trained batch 1193 batch loss 1.15725625 epoch total loss 1.19107902\n",
      "Trained batch 1194 batch loss 1.09423041 epoch total loss 1.19099796\n",
      "Trained batch 1195 batch loss 1.1469 epoch total loss 1.190961\n",
      "Trained batch 1196 batch loss 1.07048011 epoch total loss 1.19086015\n",
      "Trained batch 1197 batch loss 1.16575873 epoch total loss 1.19083929\n",
      "Trained batch 1198 batch loss 1.26222384 epoch total loss 1.19089878\n",
      "Trained batch 1199 batch loss 1.23273146 epoch total loss 1.1909337\n",
      "Trained batch 1200 batch loss 1.1638093 epoch total loss 1.19091117\n",
      "Trained batch 1201 batch loss 1.22911453 epoch total loss 1.190943\n",
      "Trained batch 1202 batch loss 1.23610759 epoch total loss 1.19098055\n",
      "Trained batch 1203 batch loss 1.29566145 epoch total loss 1.19106758\n",
      "Trained batch 1204 batch loss 1.1464324 epoch total loss 1.1910305\n",
      "Trained batch 1205 batch loss 1.13779092 epoch total loss 1.19098639\n",
      "Trained batch 1206 batch loss 1.10592759 epoch total loss 1.19091582\n",
      "Trained batch 1207 batch loss 1.18712711 epoch total loss 1.19091272\n",
      "Trained batch 1208 batch loss 1.12310767 epoch total loss 1.19085658\n",
      "Trained batch 1209 batch loss 1.06357014 epoch total loss 1.19075131\n",
      "Trained batch 1210 batch loss 1.11828232 epoch total loss 1.19069135\n",
      "Trained batch 1211 batch loss 1.05908465 epoch total loss 1.19058275\n",
      "Trained batch 1212 batch loss 1.09666407 epoch total loss 1.19050527\n",
      "Trained batch 1213 batch loss 1.03535903 epoch total loss 1.19037735\n",
      "Trained batch 1214 batch loss 1.16906178 epoch total loss 1.19035983\n",
      "Trained batch 1215 batch loss 1.12149525 epoch total loss 1.19030309\n",
      "Trained batch 1216 batch loss 0.987135291 epoch total loss 1.19013608\n",
      "Trained batch 1217 batch loss 1.16791987 epoch total loss 1.19011784\n",
      "Trained batch 1218 batch loss 1.03170323 epoch total loss 1.18998778\n",
      "Trained batch 1219 batch loss 1.25510263 epoch total loss 1.1900413\n",
      "Trained batch 1220 batch loss 1.11835814 epoch total loss 1.18998253\n",
      "Trained batch 1221 batch loss 1.23532414 epoch total loss 1.19001973\n",
      "Trained batch 1222 batch loss 1.34838474 epoch total loss 1.19014931\n",
      "Trained batch 1223 batch loss 1.08624864 epoch total loss 1.19006443\n",
      "Trained batch 1224 batch loss 1.44109058 epoch total loss 1.19026947\n",
      "Trained batch 1225 batch loss 1.25139332 epoch total loss 1.1903193\n",
      "Trained batch 1226 batch loss 1.11839426 epoch total loss 1.19026065\n",
      "Trained batch 1227 batch loss 1.18555427 epoch total loss 1.19025683\n",
      "Trained batch 1228 batch loss 1.27354336 epoch total loss 1.19032466\n",
      "Trained batch 1229 batch loss 1.30427289 epoch total loss 1.19041741\n",
      "Trained batch 1230 batch loss 1.31449604 epoch total loss 1.19051826\n",
      "Trained batch 1231 batch loss 1.18162787 epoch total loss 1.19051099\n",
      "Trained batch 1232 batch loss 1.24126256 epoch total loss 1.19055212\n",
      "Trained batch 1233 batch loss 1.39231217 epoch total loss 1.19071579\n",
      "Trained batch 1234 batch loss 1.30084586 epoch total loss 1.19080508\n",
      "Trained batch 1235 batch loss 1.11498022 epoch total loss 1.19074368\n",
      "Trained batch 1236 batch loss 1.23073435 epoch total loss 1.19077599\n",
      "Trained batch 1237 batch loss 1.17677796 epoch total loss 1.19076467\n",
      "Trained batch 1238 batch loss 1.23298562 epoch total loss 1.19079888\n",
      "Trained batch 1239 batch loss 1.08120406 epoch total loss 1.19071043\n",
      "Trained batch 1240 batch loss 1.1094749 epoch total loss 1.19064486\n",
      "Trained batch 1241 batch loss 1.06875706 epoch total loss 1.19054663\n",
      "Trained batch 1242 batch loss 1.19428444 epoch total loss 1.19054973\n",
      "Trained batch 1243 batch loss 1.16428089 epoch total loss 1.19052863\n",
      "Trained batch 1244 batch loss 1.21916091 epoch total loss 1.19055152\n",
      "Trained batch 1245 batch loss 1.22573757 epoch total loss 1.19057977\n",
      "Trained batch 1246 batch loss 1.10555673 epoch total loss 1.19051158\n",
      "Trained batch 1247 batch loss 1.12723708 epoch total loss 1.1904608\n",
      "Trained batch 1248 batch loss 1.26467228 epoch total loss 1.19052029\n",
      "Trained batch 1249 batch loss 1.22426343 epoch total loss 1.19054723\n",
      "Trained batch 1250 batch loss 1.18643475 epoch total loss 1.19054389\n",
      "Trained batch 1251 batch loss 1.25438201 epoch total loss 1.19059503\n",
      "Trained batch 1252 batch loss 1.19312751 epoch total loss 1.19059706\n",
      "Trained batch 1253 batch loss 1.09833443 epoch total loss 1.19052339\n",
      "Trained batch 1254 batch loss 1.08616519 epoch total loss 1.19044018\n",
      "Trained batch 1255 batch loss 1.08842957 epoch total loss 1.19035888\n",
      "Trained batch 1256 batch loss 1.29656398 epoch total loss 1.1904434\n",
      "Trained batch 1257 batch loss 1.22283423 epoch total loss 1.19046915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1258 batch loss 1.22983789 epoch total loss 1.19050038\n",
      "Trained batch 1259 batch loss 1.18885243 epoch total loss 1.19049907\n",
      "Trained batch 1260 batch loss 1.1902554 epoch total loss 1.19049895\n",
      "Trained batch 1261 batch loss 1.27601767 epoch total loss 1.19056678\n",
      "Trained batch 1262 batch loss 1.10787809 epoch total loss 1.19050121\n",
      "Trained batch 1263 batch loss 1.07368898 epoch total loss 1.19040883\n",
      "Trained batch 1264 batch loss 1.16441071 epoch total loss 1.1903882\n",
      "Trained batch 1265 batch loss 1.11348331 epoch total loss 1.19032753\n",
      "Trained batch 1266 batch loss 1.10307753 epoch total loss 1.1902585\n",
      "Trained batch 1267 batch loss 1.0452801 epoch total loss 1.19014406\n",
      "Trained batch 1268 batch loss 0.927826405 epoch total loss 1.18993723\n",
      "Trained batch 1269 batch loss 1.01750088 epoch total loss 1.18980134\n",
      "Trained batch 1270 batch loss 1.14876425 epoch total loss 1.18976903\n",
      "Trained batch 1271 batch loss 1.118994 epoch total loss 1.18971336\n",
      "Trained batch 1272 batch loss 1.01925731 epoch total loss 1.18957937\n",
      "Trained batch 1273 batch loss 1.10281301 epoch total loss 1.18951118\n",
      "Trained batch 1274 batch loss 1.08120394 epoch total loss 1.18942618\n",
      "Trained batch 1275 batch loss 1.23232543 epoch total loss 1.1894598\n",
      "Trained batch 1276 batch loss 1.24153972 epoch total loss 1.18950069\n",
      "Trained batch 1277 batch loss 1.27453685 epoch total loss 1.18956721\n",
      "Trained batch 1278 batch loss 1.20634174 epoch total loss 1.18958032\n",
      "Trained batch 1279 batch loss 1.27192616 epoch total loss 1.18964481\n",
      "Trained batch 1280 batch loss 1.30086911 epoch total loss 1.18973172\n",
      "Trained batch 1281 batch loss 1.19958711 epoch total loss 1.18973935\n",
      "Trained batch 1282 batch loss 1.17676568 epoch total loss 1.18972921\n",
      "Trained batch 1283 batch loss 1.17594838 epoch total loss 1.18971848\n",
      "Trained batch 1284 batch loss 1.2516737 epoch total loss 1.18976676\n",
      "Trained batch 1285 batch loss 1.23964739 epoch total loss 1.18980551\n",
      "Trained batch 1286 batch loss 1.24095333 epoch total loss 1.18984532\n",
      "Trained batch 1287 batch loss 1.21441269 epoch total loss 1.1898644\n",
      "Trained batch 1288 batch loss 1.24207258 epoch total loss 1.18990493\n",
      "Trained batch 1289 batch loss 1.17298198 epoch total loss 1.18989182\n",
      "Trained batch 1290 batch loss 1.22858465 epoch total loss 1.18992186\n",
      "Trained batch 1291 batch loss 1.21835709 epoch total loss 1.18994391\n",
      "Trained batch 1292 batch loss 1.3606708 epoch total loss 1.19007599\n",
      "Trained batch 1293 batch loss 1.21633089 epoch total loss 1.19009638\n",
      "Trained batch 1294 batch loss 1.11400926 epoch total loss 1.19003749\n",
      "Trained batch 1295 batch loss 1.01053762 epoch total loss 1.18989885\n",
      "Trained batch 1296 batch loss 0.969627261 epoch total loss 1.18972886\n",
      "Trained batch 1297 batch loss 1.05005538 epoch total loss 1.18962121\n",
      "Trained batch 1298 batch loss 1.09331346 epoch total loss 1.18954694\n",
      "Trained batch 1299 batch loss 1.11860728 epoch total loss 1.18949246\n",
      "Trained batch 1300 batch loss 1.02137434 epoch total loss 1.18936312\n",
      "Trained batch 1301 batch loss 1.16072011 epoch total loss 1.18934107\n",
      "Trained batch 1302 batch loss 1.0900228 epoch total loss 1.18926477\n",
      "Trained batch 1303 batch loss 1.21863425 epoch total loss 1.1892873\n",
      "Trained batch 1304 batch loss 1.25113952 epoch total loss 1.18933475\n",
      "Trained batch 1305 batch loss 1.19322276 epoch total loss 1.18933773\n",
      "Trained batch 1306 batch loss 1.40004504 epoch total loss 1.18949902\n",
      "Trained batch 1307 batch loss 1.46440589 epoch total loss 1.18970931\n",
      "Trained batch 1308 batch loss 1.31133354 epoch total loss 1.18980229\n",
      "Trained batch 1309 batch loss 1.15954304 epoch total loss 1.18977916\n",
      "Trained batch 1310 batch loss 1.18849778 epoch total loss 1.18977809\n",
      "Trained batch 1311 batch loss 1.17499053 epoch total loss 1.18976688\n",
      "Trained batch 1312 batch loss 1.31258571 epoch total loss 1.18986058\n",
      "Trained batch 1313 batch loss 1.28016222 epoch total loss 1.18992937\n",
      "Trained batch 1314 batch loss 1.21292782 epoch total loss 1.18994677\n",
      "Trained batch 1315 batch loss 1.31743014 epoch total loss 1.19004369\n",
      "Trained batch 1316 batch loss 1.32402658 epoch total loss 1.19014549\n",
      "Trained batch 1317 batch loss 1.42742598 epoch total loss 1.19032562\n",
      "Trained batch 1318 batch loss 1.38837874 epoch total loss 1.19047594\n",
      "Trained batch 1319 batch loss 1.3616755 epoch total loss 1.19060576\n",
      "Trained batch 1320 batch loss 1.30642462 epoch total loss 1.19069338\n",
      "Trained batch 1321 batch loss 1.31066358 epoch total loss 1.19078422\n",
      "Trained batch 1322 batch loss 1.33322167 epoch total loss 1.19089198\n",
      "Trained batch 1323 batch loss 1.36871767 epoch total loss 1.19102645\n",
      "Trained batch 1324 batch loss 1.37072539 epoch total loss 1.19116223\n",
      "Trained batch 1325 batch loss 1.26240897 epoch total loss 1.19121599\n",
      "Trained batch 1326 batch loss 1.02784252 epoch total loss 1.19109273\n",
      "Trained batch 1327 batch loss 1.25861728 epoch total loss 1.19114375\n",
      "Trained batch 1328 batch loss 1.24040961 epoch total loss 1.19118083\n",
      "Trained batch 1329 batch loss 1.20934045 epoch total loss 1.19119442\n",
      "Trained batch 1330 batch loss 1.29403555 epoch total loss 1.19127178\n",
      "Trained batch 1331 batch loss 1.24607229 epoch total loss 1.19131303\n",
      "Trained batch 1332 batch loss 1.2549895 epoch total loss 1.19136083\n",
      "Trained batch 1333 batch loss 1.32415342 epoch total loss 1.19146037\n",
      "Trained batch 1334 batch loss 1.16436076 epoch total loss 1.19144\n",
      "Trained batch 1335 batch loss 1.22446191 epoch total loss 1.19146478\n",
      "Trained batch 1336 batch loss 1.23772335 epoch total loss 1.19149935\n",
      "Trained batch 1337 batch loss 1.15569067 epoch total loss 1.19147253\n",
      "Trained batch 1338 batch loss 1.23201132 epoch total loss 1.19150281\n",
      "Trained batch 1339 batch loss 1.1729691 epoch total loss 1.19148898\n",
      "Trained batch 1340 batch loss 1.19126892 epoch total loss 1.19148886\n",
      "Trained batch 1341 batch loss 1.21550715 epoch total loss 1.19150674\n",
      "Trained batch 1342 batch loss 1.23095417 epoch total loss 1.19153619\n",
      "Trained batch 1343 batch loss 1.30563927 epoch total loss 1.19162107\n",
      "Trained batch 1344 batch loss 1.17362761 epoch total loss 1.19160771\n",
      "Trained batch 1345 batch loss 1.32724428 epoch total loss 1.19170856\n",
      "Trained batch 1346 batch loss 1.37078059 epoch total loss 1.1918416\n",
      "Trained batch 1347 batch loss 1.21531582 epoch total loss 1.19185901\n",
      "Trained batch 1348 batch loss 1.0665772 epoch total loss 1.19176602\n",
      "Trained batch 1349 batch loss 1.05486417 epoch total loss 1.19166446\n",
      "Trained batch 1350 batch loss 1.07450294 epoch total loss 1.19157767\n",
      "Trained batch 1351 batch loss 1.07233751 epoch total loss 1.19148946\n",
      "Trained batch 1352 batch loss 1.06114149 epoch total loss 1.19139302\n",
      "Trained batch 1353 batch loss 1.26342869 epoch total loss 1.1914463\n",
      "Trained batch 1354 batch loss 1.18872118 epoch total loss 1.19144428\n",
      "Trained batch 1355 batch loss 1.11661386 epoch total loss 1.19138908\n",
      "Trained batch 1356 batch loss 1.14499557 epoch total loss 1.19135487\n",
      "Trained batch 1357 batch loss 1.28988683 epoch total loss 1.19142747\n",
      "Trained batch 1358 batch loss 1.27609372 epoch total loss 1.19148982\n",
      "Trained batch 1359 batch loss 1.43521559 epoch total loss 1.19166911\n",
      "Trained batch 1360 batch loss 1.48954511 epoch total loss 1.19188821\n",
      "Trained batch 1361 batch loss 1.26298165 epoch total loss 1.19194031\n",
      "Trained batch 1362 batch loss 1.07721937 epoch total loss 1.19185615\n",
      "Trained batch 1363 batch loss 1.18363881 epoch total loss 1.19185007\n",
      "Trained batch 1364 batch loss 1.30917847 epoch total loss 1.19193614\n",
      "Trained batch 1365 batch loss 1.34268129 epoch total loss 1.19204652\n",
      "Trained batch 1366 batch loss 1.24098897 epoch total loss 1.19208241\n",
      "Trained batch 1367 batch loss 1.21057296 epoch total loss 1.19209588\n",
      "Trained batch 1368 batch loss 1.14655471 epoch total loss 1.19206262\n",
      "Trained batch 1369 batch loss 1.04857957 epoch total loss 1.19195783\n",
      "Trained batch 1370 batch loss 1.01433992 epoch total loss 1.19182813\n",
      "Trained batch 1371 batch loss 1.10554552 epoch total loss 1.19176531\n",
      "Trained batch 1372 batch loss 1.23309827 epoch total loss 1.19179547\n",
      "Trained batch 1373 batch loss 1.15552449 epoch total loss 1.191769\n",
      "Trained batch 1374 batch loss 1.14138925 epoch total loss 1.19173229\n",
      "Trained batch 1375 batch loss 1.26206779 epoch total loss 1.19178343\n",
      "Trained batch 1376 batch loss 1.25255847 epoch total loss 1.19182765\n",
      "Trained batch 1377 batch loss 1.19487727 epoch total loss 1.1918298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1378 batch loss 1.13969982 epoch total loss 1.19179189\n",
      "Trained batch 1379 batch loss 1.18134129 epoch total loss 1.19178438\n",
      "Trained batch 1380 batch loss 1.24604416 epoch total loss 1.19182372\n",
      "Trained batch 1381 batch loss 1.09032917 epoch total loss 1.19175029\n",
      "Trained batch 1382 batch loss 0.91573894 epoch total loss 1.19155061\n",
      "Trained batch 1383 batch loss 0.934948921 epoch total loss 1.191365\n",
      "Trained batch 1384 batch loss 0.998818398 epoch total loss 1.19122589\n",
      "Trained batch 1385 batch loss 1.40211213 epoch total loss 1.19137812\n",
      "Trained batch 1386 batch loss 1.35114813 epoch total loss 1.19149339\n",
      "Trained batch 1387 batch loss 1.28422594 epoch total loss 1.19156027\n",
      "Trained batch 1388 batch loss 1.30476654 epoch total loss 1.19164181\n",
      "Epoch 5 train loss 1.1916418075561523\n",
      "Validated batch 1 batch loss 1.22735023\n",
      "Validated batch 2 batch loss 1.16183698\n",
      "Validated batch 3 batch loss 1.15775895\n",
      "Validated batch 4 batch loss 1.14011014\n",
      "Validated batch 5 batch loss 1.16180778\n",
      "Validated batch 6 batch loss 1.22421479\n",
      "Validated batch 7 batch loss 1.18879056\n",
      "Validated batch 8 batch loss 1.14002097\n",
      "Validated batch 9 batch loss 1.27871263\n",
      "Validated batch 10 batch loss 1.19701803\n",
      "Validated batch 11 batch loss 1.12390876\n",
      "Validated batch 12 batch loss 1.09745848\n",
      "Validated batch 13 batch loss 1.23701715\n",
      "Validated batch 14 batch loss 1.26024365\n",
      "Validated batch 15 batch loss 1.37066424\n",
      "Validated batch 16 batch loss 1.29671967\n",
      "Validated batch 17 batch loss 1.18159735\n",
      "Validated batch 18 batch loss 1.31673408\n",
      "Validated batch 19 batch loss 1.19802308\n",
      "Validated batch 20 batch loss 1.21015477\n",
      "Validated batch 21 batch loss 1.22476339\n",
      "Validated batch 22 batch loss 0.965650439\n",
      "Validated batch 23 batch loss 1.17945957\n",
      "Validated batch 24 batch loss 1.34281242\n",
      "Validated batch 25 batch loss 1.2728616\n",
      "Validated batch 26 batch loss 1.12613404\n",
      "Validated batch 27 batch loss 1.16529226\n",
      "Validated batch 28 batch loss 1.11559725\n",
      "Validated batch 29 batch loss 1.23294461\n",
      "Validated batch 30 batch loss 1.23939896\n",
      "Validated batch 31 batch loss 1.08245587\n",
      "Validated batch 32 batch loss 1.27630699\n",
      "Validated batch 33 batch loss 1.20404959\n",
      "Validated batch 34 batch loss 1.22639298\n",
      "Validated batch 35 batch loss 1.22676396\n",
      "Validated batch 36 batch loss 1.23763525\n",
      "Validated batch 37 batch loss 1.15468514\n",
      "Validated batch 38 batch loss 1.14478576\n",
      "Validated batch 39 batch loss 1.24674618\n",
      "Validated batch 40 batch loss 1.17065263\n",
      "Validated batch 41 batch loss 1.22218847\n",
      "Validated batch 42 batch loss 1.24332392\n",
      "Validated batch 43 batch loss 1.43795156\n",
      "Validated batch 44 batch loss 1.2614392\n",
      "Validated batch 45 batch loss 1.22389686\n",
      "Validated batch 46 batch loss 1.11889648\n",
      "Validated batch 47 batch loss 1.12769711\n",
      "Validated batch 48 batch loss 1.13118231\n",
      "Validated batch 49 batch loss 1.18583965\n",
      "Validated batch 50 batch loss 1.07587838\n",
      "Validated batch 51 batch loss 1.09766817\n",
      "Validated batch 52 batch loss 1.25393355\n",
      "Validated batch 53 batch loss 1.13509059\n",
      "Validated batch 54 batch loss 0.996896565\n",
      "Validated batch 55 batch loss 1.11062288\n",
      "Validated batch 56 batch loss 1.13191748\n",
      "Validated batch 57 batch loss 1.08482492\n",
      "Validated batch 58 batch loss 1.15078282\n",
      "Validated batch 59 batch loss 1.206604\n",
      "Validated batch 60 batch loss 1.17253399\n",
      "Validated batch 61 batch loss 1.26048863\n",
      "Validated batch 62 batch loss 1.24222398\n",
      "Validated batch 63 batch loss 1.13849092\n",
      "Validated batch 64 batch loss 1.29519367\n",
      "Validated batch 65 batch loss 0.967182636\n",
      "Validated batch 66 batch loss 1.13102341\n",
      "Validated batch 67 batch loss 1.06761444\n",
      "Validated batch 68 batch loss 1.17158806\n",
      "Validated batch 69 batch loss 1.32171464\n",
      "Validated batch 70 batch loss 1.24924779\n",
      "Validated batch 71 batch loss 1.23323154\n",
      "Validated batch 72 batch loss 1.20515716\n",
      "Validated batch 73 batch loss 1.16526937\n",
      "Validated batch 74 batch loss 1.24783266\n",
      "Validated batch 75 batch loss 1.39511168\n",
      "Validated batch 76 batch loss 1.15291643\n",
      "Validated batch 77 batch loss 1.29206049\n",
      "Validated batch 78 batch loss 1.17080319\n",
      "Validated batch 79 batch loss 1.23552608\n",
      "Validated batch 80 batch loss 1.23885286\n",
      "Validated batch 81 batch loss 1.11951101\n",
      "Validated batch 82 batch loss 1.34172225\n",
      "Validated batch 83 batch loss 1.20550573\n",
      "Validated batch 84 batch loss 1.28948271\n",
      "Validated batch 85 batch loss 1.2116574\n",
      "Validated batch 86 batch loss 1.25931835\n",
      "Validated batch 87 batch loss 1.12082195\n",
      "Validated batch 88 batch loss 1.19378471\n",
      "Validated batch 89 batch loss 1.1722188\n",
      "Validated batch 90 batch loss 1.18866611\n",
      "Validated batch 91 batch loss 1.23872447\n",
      "Validated batch 92 batch loss 1.20890045\n",
      "Validated batch 93 batch loss 1.27349699\n",
      "Validated batch 94 batch loss 1.16246533\n",
      "Validated batch 95 batch loss 1.17789948\n",
      "Validated batch 96 batch loss 1.11059105\n",
      "Validated batch 97 batch loss 1.1798178\n",
      "Validated batch 98 batch loss 1.27824593\n",
      "Validated batch 99 batch loss 1.23489368\n",
      "Validated batch 100 batch loss 1.2880069\n",
      "Validated batch 101 batch loss 1.27513826\n",
      "Validated batch 102 batch loss 1.23915648\n",
      "Validated batch 103 batch loss 1.24218321\n",
      "Validated batch 104 batch loss 1.37285805\n",
      "Validated batch 105 batch loss 1.2449789\n",
      "Validated batch 106 batch loss 1.26824033\n",
      "Validated batch 107 batch loss 1.26550138\n",
      "Validated batch 108 batch loss 1.30313039\n",
      "Validated batch 109 batch loss 1.28332019\n",
      "Validated batch 110 batch loss 1.12285268\n",
      "Validated batch 111 batch loss 1.19099867\n",
      "Validated batch 112 batch loss 1.25417554\n",
      "Validated batch 113 batch loss 1.26427114\n",
      "Validated batch 114 batch loss 1.12191808\n",
      "Validated batch 115 batch loss 1.18019629\n",
      "Validated batch 116 batch loss 1.1549952\n",
      "Validated batch 117 batch loss 1.25087953\n",
      "Validated batch 118 batch loss 1.13365984\n",
      "Validated batch 119 batch loss 1.13298154\n",
      "Validated batch 120 batch loss 1.17819929\n",
      "Validated batch 121 batch loss 1.23667204\n",
      "Validated batch 122 batch loss 1.28570652\n",
      "Validated batch 123 batch loss 1.30707252\n",
      "Validated batch 124 batch loss 1.26626825\n",
      "Validated batch 125 batch loss 1.14238381\n",
      "Validated batch 126 batch loss 1.21386051\n",
      "Validated batch 127 batch loss 1.18910062\n",
      "Validated batch 128 batch loss 1.18063867\n",
      "Validated batch 129 batch loss 1.34112835\n",
      "Validated batch 130 batch loss 1.25190413\n",
      "Validated batch 131 batch loss 1.25083292\n",
      "Validated batch 132 batch loss 1.32040465\n",
      "Validated batch 133 batch loss 1.13797975\n",
      "Validated batch 134 batch loss 1.16374874\n",
      "Validated batch 135 batch loss 1.14922321\n",
      "Validated batch 136 batch loss 1.09702897\n",
      "Validated batch 137 batch loss 1.27072239\n",
      "Validated batch 138 batch loss 1.12793469\n",
      "Validated batch 139 batch loss 1.14424658\n",
      "Validated batch 140 batch loss 1.17814469\n",
      "Validated batch 141 batch loss 1.23813534\n",
      "Validated batch 142 batch loss 1.15635705\n",
      "Validated batch 143 batch loss 1.26875937\n",
      "Validated batch 144 batch loss 1.36441827\n",
      "Validated batch 145 batch loss 1.10081553\n",
      "Validated batch 146 batch loss 1.18334544\n",
      "Validated batch 147 batch loss 1.20240343\n",
      "Validated batch 148 batch loss 1.24901795\n",
      "Validated batch 149 batch loss 1.27223384\n",
      "Validated batch 150 batch loss 1.1837275\n",
      "Validated batch 151 batch loss 0.993093789\n",
      "Validated batch 152 batch loss 1.2170167\n",
      "Validated batch 153 batch loss 1.16653323\n",
      "Validated batch 154 batch loss 1.22188103\n",
      "Validated batch 155 batch loss 1.28335524\n",
      "Validated batch 156 batch loss 1.07138491\n",
      "Validated batch 157 batch loss 1.23295128\n",
      "Validated batch 158 batch loss 1.29841113\n",
      "Validated batch 159 batch loss 1.24948657\n",
      "Validated batch 160 batch loss 1.17850387\n",
      "Validated batch 161 batch loss 1.13547111\n",
      "Validated batch 162 batch loss 1.19307399\n",
      "Validated batch 163 batch loss 1.0739882\n",
      "Validated batch 164 batch loss 1.16939735\n",
      "Validated batch 165 batch loss 1.1448065\n",
      "Validated batch 166 batch loss 1.12412381\n",
      "Validated batch 167 batch loss 1.22992337\n",
      "Validated batch 168 batch loss 1.11953545\n",
      "Validated batch 169 batch loss 1.21715724\n",
      "Validated batch 170 batch loss 1.31207037\n",
      "Validated batch 171 batch loss 1.11624813\n",
      "Validated batch 172 batch loss 1.24308228\n",
      "Validated batch 173 batch loss 1.1957171\n",
      "Validated batch 174 batch loss 1.08565342\n",
      "Validated batch 175 batch loss 1.20843792\n",
      "Validated batch 176 batch loss 1.20796037\n",
      "Validated batch 177 batch loss 1.15600145\n",
      "Validated batch 178 batch loss 1.30681908\n",
      "Validated batch 179 batch loss 1.21491194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 180 batch loss 1.27874565\n",
      "Validated batch 181 batch loss 1.12658441\n",
      "Validated batch 182 batch loss 1.26064181\n",
      "Validated batch 183 batch loss 1.17215633\n",
      "Validated batch 184 batch loss 1.12040186\n",
      "Validated batch 185 batch loss 1.24266613\n",
      "Epoch 5 val loss 1.2018053531646729\n",
      "Model /aiffel/aiffel/mpii/trained/model-epoch-5-loss-1.2018.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751c140",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 1 val loss 1.4370572566986084\n",
    "Epoch 2 val loss 1.3234355449676514\n",
    "Epoch 3 val loss 1.2649729251861572\n",
    "Epoch 4 val loss 1.2129207849502563\n",
    "Epoch 5 val loss 1.2018053531646729\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50a1ab",
   "metadata": {},
   "source": [
    "### 둠칫둠칫 댄스타임\n",
    "- 예측 엔진 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e18769e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리 학습된 모델 사용\n",
    "# WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'models', 'model-v0.0.1-epoch-2-loss-1.3072.h5')\n",
    "WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'trained', 'model-epoch-5-loss-1.2018.h5')\n",
    "\n",
    "model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "# 이전의 학습하는 코드 블럭을 통해 학습하고 그 모델을 사용할 경우 아래 주석 처리된 코드를 사용하면 됩니다\n",
    "# model.load_weights(best_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59cd7437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypoint 변수 정의\n",
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf1ba013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 heatmap으로 했기 때문에 모델이 추론해 내놓은 결과도 heatmap\n",
    "# heatmap으로부터 좌표를 추출\n",
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb46a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 256x256 이미지에 64x64 heatmap max 값을 표현할 때 quantization 오차가 발생하기 때문에 실제 계산에서는 3x3 필터를 이용해서 근사치를 구함\n",
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aec9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 이미지 경로를 입력하면 이미지와 keypoint를 출력하는 함수\n",
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6560bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92970a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c0419a",
   "metadata": {},
   "source": [
    "### Pretrained 모델이랑 비교\n",
    "- Epoch 1  \n",
    "  ![epoch1](./images/epoch1.png)\n",
    "  - 몸의 keypoint들이 제대로 찍혀있지 않음\n",
    "\n",
    "- Epoch 5\n",
    "  - 그에 반해 위 결과는 팔꿈치 하나랑 발 하나를 제외하고는 꽤 잘 나옴\n",
    "\n",
    "[SimpleBaseline 실험 이동](./0317_simplebaseline.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
